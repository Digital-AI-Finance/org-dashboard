[
  {
    "name": "org-dashboard",
    "full_name": "Digital-AI-Finance/org-dashboard",
    "description": "Automated dashboard for monitoring all repositories in the Digital-AI-Finance organization",
    "url": "https://github.com/Digital-AI-Finance/org-dashboard",
    "clone_url": "https://github.com/Digital-AI-Finance/org-dashboard.git",
    "homepage": "https://digital-ai-finance.github.io/org-dashboard",
    "language": "Python",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 13622,
    "default_branch": "main",
    "created_at": "2025-11-21T21:59:13+00:00",
    "updated_at": "2025-12-05T03:18:59+00:00",
    "pushed_at": "2025-12-05T03:18:57+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": false,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 2,
    "readme": "# GitHub Organization Research Platform\n\nA comprehensive, automated research platform for academic GitHub organizations. Combines repository monitoring with advanced research features including publication tracking, citation analysis, reproducibility scoring, and community verification.\n\nLive Demo: https://digital-ai-finance.github.io/org-dashboard/\n\n## Core Features\n\n- Automatic daily updates via GitHub Actions\n- Beautiful, searchable documentation site\n- Repository catalog with detailed information\n- Statistics and analytics\n- Organization by language and topics\n- Mobile-responsive design\n- Zero server costs (runs entirely on GitHub)\n\n## Research Platform Features\n\n### Publication Tracking\n- Automatic extraction of DOIs, arXiv IDs, SSRN papers from READMEs\n- Academic database integration (CrossRef, arXiv)\n- Publication metadata enrichment\n- Citation count tracking\n\n### Code & Data\n- Jupyter notebook rendering\n- Dataset detection and cataloging\n- Dependency analysis\n- Code language statistics\n\n### Reproducibility\n- Automated reproducibility scoring (100-point scale)\n- Badge system (Gold/Silver/Bronze)\n- Environment configuration detection\n- Docker support tracking\n\n### Community Features\n- Replication attempt tracking\n- Community verification system\n- Peer review ratings\n- Success rate metrics\n\n### Advanced Search\n- Full-text search across all repositories\n- Faceted navigation\n- TF-IDF relevance scoring\n- Autocomplete suggestions\n\n### Visualizations\n- Interactive Plotly charts\n- Citation network graphs\n- Publication timelines\n- Language distribution\n- Collaboration networks\n\n## Architecture\n\n- **Data Fetching**: Python script using PyGithub to fetch org data + research metadata extraction\n- **Academic APIs**: CrossRef, arXiv integration for publication enrichment\n- **Analysis**: Citation tracking, reproducibility scoring, search indexing\n- **Markdown Generation**: Jinja2 templates for dynamic content\n- **Visualizations**: Interactive Plotly charts, network graphs\n- **Static Site**: MkDocs Material theme\n- **Automation**: GitHub Actions for scheduled updates\n- **Hosting**: GitHub Pages (free)\n\n## Live Demo\n\nVisit our live demo at: https://digital-ai-finance.github.io/org-dashboard/\n\nThe demo includes 4 example repositories showcasing different research features:\n\n1. **org-dashboard** - This repository (meta!)\n2. **portfolio-optimization-ml** - ML research with arXiv papers and DOIs\n3. **credit-risk-prediction** - Neural networks with SSRN publications and notebooks\n4. **market-microstructure** - HFT research with Zenodo datasets and Docker\n\nFeatures demonstrated:\n- Publications with DOI/arXiv links and citation counts\n- Reproducibility scores and badges\n- Community replication attempts and reviews\n- Dataset cataloging\n- Interactive visualizations\n\n## Setup Instructions\n\n### 1. Create GitHub Personal Access Token\n\n1. Go to GitHub Settings > Developer settings > Personal access tokens > Tokens (classic)\n2. Click \"Generate new token (classic)\"\n3. Give it a descriptive name (e.g., \"Org Dashboard\")\n4. Select scopes:\n   - `repo` (Full control of private repositories)\n   - `read:org` (Read org and team membership)\n5. Click \"Generate token\"\n6. Copy the token immediately (you will not see it again)\n\n### 2. Fork or Create This Repository\n\n1. Create a new repository in your GitHub organization\n2. Clone this repository or copy all files to your new repo\n3. Push to GitHub\n\n### 3. Configure Repository Secrets\n\n1. Go to your repository Settings > Secrets and variables > Actions\n2. Add the following secrets:\n   - `GH_PAT`: Your GitHub Personal Access Token\n   - `ORG_NAME`: Your organization name (e.g., \"my-org\")\n\n### 4. Enable GitHub Pages\n\n1. Go to repository Settings > Pages\n2. Source: Deploy from a branch\n3. Branch: `gh-pages` / `root`\n4. Save\n\n### 5. Run Initial Workflow\n\n1. Go to Actions tab\n2. Select \"Update Dashboard\" workflow\n3. Click \"Run workflow\"\n4. Wait for completion (2-5 minutes)\n\n### 6. Access Your Dashboard\n\nYour dashboard will be available at:\n`https://YOUR_ORG.github.io/REPO_NAME/`\n\n## Configuration\n\n### Update Frequency\n\nEdit `.github/workflows/update-dashboard.yml` to change schedule:\n\n```yaml\nschedule:\n  - cron: '0 2 * * *'  # Daily at 2 AM UTC\n  # - cron: '0 */6 * * *'  # Every 6 hours\n  # - cron: '0 0 * * 0'  # Weekly on Sunday\n```\n\n### Customize Appearance\n\nEdit `mkdocs.yml` to change:\n- Site name and description\n- Theme colors\n- Navigation structure\n- Enabled features\n\n### Customize Templates\n\nEdit files in `templates/` directory to change:\n- Page layouts\n- Content structure\n- Displayed information\n\n## Local Development\n\n### Prerequisites\n\n- Python 3.11+\n- Git\n\n### Install Dependencies\n\n```bash\npip install -r requirements.txt\n```\n\n### Fetch Data Locally\n\n```bash\nexport GITHUB_TOKEN='your_token_here'\nexport GITHUB_ORG='your_org_name'\npython scripts/fetch_org_data.py\n```\n\nOr with arguments:\n\n```bash\nexport GITHUB_TOKEN='your_token_here'\npython scripts/fetch_org_data.py your_org_name\n```\n\n### Generate Markdown\n\n```bash\npython scripts/generate_markdown.py\n```\n\n### Preview Site Locally\n\n```bash\nmkdocs serve\n```\n\nThen open http://127.0.0.1:8000 in your browser.\n\n### Build Static Site\n\n```bash\nmkdocs build\n```\n\nOutput will be in `site/` directory.\n\n## Project Structure\n\n```\n.\n‚îú‚îÄ‚îÄ .github/\n‚îÇ   ‚îî‚îÄ‚îÄ workflows/\n‚îÇ       ‚îî‚îÄ‚îÄ update-dashboard.yml  # GitHub Actions workflow\n‚îú‚îÄ‚îÄ scripts/\n‚îÇ   ‚îú‚îÄ‚îÄ fetch_org_data.py        # Fetch data from GitHub API\n‚îÇ   ‚îî‚îÄ‚îÄ generate_markdown.py     # Generate markdown from data\n‚îú‚îÄ‚îÄ templates/                   # Jinja2 templates\n‚îÇ   ‚îú‚îÄ‚îÄ index.md.j2\n‚îÇ   ‚îú‚îÄ‚îÄ stats.md.j2\n‚îÇ   ‚îú‚îÄ‚îÄ repo.md.j2\n‚îÇ   ‚îî‚îÄ‚îÄ ...\n‚îú‚îÄ‚îÄ docs/                        # Generated markdown (committed)\n‚îÇ   ‚îú‚îÄ‚îÄ index.md\n‚îÇ   ‚îú‚îÄ‚îÄ stats.md\n‚îÇ   ‚îú‚îÄ‚îÄ repos/\n‚îÇ   ‚îú‚îÄ‚îÄ by-language/\n‚îÇ   ‚îî‚îÄ‚îÄ by-topic/\n‚îú‚îÄ‚îÄ data/                        # Generated JSON data (committed)\n‚îÇ   ‚îú‚îÄ‚îÄ repos.json\n‚îÇ   ‚îî‚îÄ‚îÄ stats.json\n‚îú‚îÄ‚îÄ mkdocs.yml                   # MkDocs configuration\n‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies\n‚îî‚îÄ‚îÄ README.md\n```\n\n## Troubleshooting\n\n### Workflow Fails with Authentication Error\n\n- Verify `GH_PAT` secret is set correctly\n- Ensure token has required scopes (`repo`, `read:org`)\n- Token may have expired - regenerate and update secret\n\n### No Changes Detected\n\n- Check if organization has any repositories\n- Verify `ORG_NAME` secret is correct\n- Check workflow logs for errors\n\n### Pages Not Deploying\n\n- Ensure GitHub Pages is enabled in repository settings\n- Check that `gh-pages` branch exists\n- Verify workflow has `contents: write` permission\n\n### Rate Limiting\n\n- GitHub API has rate limits (5000 requests/hour for authenticated requests)\n- For large organizations, consider caching or reducing update frequency\n- Workflow implements basic rate limit handling\n\n### Local Development Issues\n\n**Module not found:**\n```bash\npip install -r requirements.txt\n```\n\n**Permission denied:**\n```bash\nchmod +x scripts/*.py\n```\n\n**Data files missing:**\nRun fetch script first before generating markdown.\n\n## Customization Ideas\n\n- Add CI/CD status badges\n- Include code quality metrics\n- Show dependency vulnerabilities\n- Add commit activity graphs\n- Display contributor statistics\n- Track issue response times\n- Monitor PR merge times\n\n## GitHub Actions Free Tier Limits\n\n- 2000 minutes/month for free accounts\n- This workflow uses ~2-5 minutes per run\n- Daily runs: ~150 minutes/month\n- Well within free tier limits\n\n## Contributing\n\nContributions welcome! Please feel free to submit issues or pull requests.\n\n## License\n\nMIT License - feel free to use this for your organization.\n\n## Credits\n\nBuilt with:\n- [PyGithub](https://github.com/PyGithub/PyGithub)\n- [MkDocs](https://www.mkdocs.org/)\n- [Material for MkDocs](https://squidfunk.github.io/mkdocs-material/)\n- [Jinja2](https://jinja.palletsprojects.com/)\n",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "org-dashboard",
      "research": {
        "title": "Automated dashboard for monitoring all repositories in the Digital-AI-Finance organization",
        "abstract": "A comprehensive, automated research platform for academic GitHub organizations. Combines repository monitoring with advanced research features including publication tracking, citation analysis, reproducibility scoring, and community verification.",
        "keywords": [],
        "authors": [
          {
            "name": "language and topics"
          }
        ]
      },
      "publications": [],
      "code": {
        "languages": [
          "Python"
        ],
        "notebooks": [],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": true,
        "has_dockerfile": false,
        "has_environment_yml": false,
        "has_makefile": false,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:02:24.409133",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": [
        {
          "name": "coverage.json",
          "path": "coverage.json",
          "format": ".json",
          "size_bytes": 60741
        },
        {
          "name": "data",
          "path": "data",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "build_log.json",
          "path": "data/build_log.json",
          "format": ".json",
          "size_bytes": 4148
        },
        {
          "name": "citation_history.json",
          "path": "data/citation_history.json",
          "format": ".json",
          "size_bytes": 54591
        },
        {
          "name": "citation_report.json",
          "path": "data/citation_report.json",
          "format": ".json",
          "size_bytes": 3814
        },
        {
          "name": "code_quality_report.json",
          "path": "data/code_quality_report.json",
          "format": ".json",
          "size_bytes": 17189
        },
        {
          "name": "collaboration_network.json",
          "path": "data/collaboration_network.json",
          "format": ".json",
          "size_bytes": 6146
        },
        {
          "name": "ml_topic_analysis.json",
          "path": "data/ml_topic_analysis.json",
          "format": ".json",
          "size_bytes": 29920
        },
        {
          "name": "repos.json",
          "path": "data/repos.json",
          "format": ".json",
          "size_bytes": 274876
        },
        {
          "name": "repository_health_report.json",
          "path": "data/repository_health_report.json",
          "format": ".json",
          "size_bytes": 52493
        },
        {
          "name": "reproducibility_report.json",
          "path": "data/reproducibility_report.json",
          "format": ".json",
          "size_bytes": 31104
        },
        {
          "name": "research_metadata.json",
          "path": "data/research_metadata.json",
          "format": ".json",
          "size_bytes": 97859
        },
        {
          "name": "search_index.pkl",
          "path": "data/search_index.pkl",
          "format": ".pkl",
          "size_bytes": 227971
        },
        {
          "name": "stats.json",
          "path": "data/stats.json",
          "format": ".json",
          "size_bytes": 2978
        },
        {
          "name": "data",
          "path": "docs/data",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "search_data.json",
          "path": "docs/data/search_data.json",
          "format": ".json",
          "size_bytes": 657936
        },
        {
          "name": "search_data_minimal.json",
          "path": "docs/data/search_data_minimal.json",
          "format": ".json",
          "size_bytes": 468829
        },
        {
          "name": "citation_network.json",
          "path": "docs/visualizations/citation_network.json",
          "format": ".json",
          "size_bytes": 337
        },
        {
          "name": "collaboration_network.json",
          "path": "docs/visualizations/collaboration_network.json",
          "format": ".json",
          "size_bytes": 1450
        },
        {
          "name": "research_metadata_schema.json",
          "path": "schemas/research_metadata_schema.json",
          "format": ".json",
          "size_bytes": 5576
        }
      ]
    }
  },
  {
    "name": "portfolio-optimization-ml",
    "full_name": "Digital-AI-Finance/portfolio-optimization-ml",
    "description": "Machine learning approaches to portfolio optimization using deep reinforcement learning",
    "url": "https://github.com/Digital-AI-Finance/portfolio-optimization-ml",
    "clone_url": "https://github.com/Digital-AI-Finance/portfolio-optimization-ml.git",
    "homepage": "",
    "language": "Unknown",
    "topics": [
      "finance",
      "machine-learning",
      "portfolio-optimization",
      "reinforcement-learning"
    ],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 2,
    "default_branch": "main",
    "created_at": "2025-11-21T22:50:46+00:00",
    "updated_at": "2025-11-21T22:51:57+00:00",
    "pushed_at": "2025-11-21T22:51:54+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": false,
    "has_pages": false,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "# Portfolio Optimization with Deep Reinforcement Learning\n\nThis repository contains code and data for the paper \"Deep Reinforcement Learning for Portfolio Optimization\" (arXiv:2103.12345).\n\n## Authors\n\n- John Smith (University of Finance)\n- Jane Doe (Tech Institute)\n\n## Abstract\n\nWe propose a novel deep reinforcement learning approach for portfolio optimization that outperforms traditional mean-variance optimization. Our method uses a Deep Q-Network (DQN) to learn optimal trading strategies from historical market data.\n\n## Published Paper\n\n- **arXiv**: arXiv:2103.12345\n- **DOI**: 10.1016/j.jfineco.2023.01.001\n- **Published**: Journal of Financial Economics, 2023\n\n## Citation\n\nIf you use this code or data, please cite:\n\n```bibtex\n@article{smith2023deep,\n  title={Deep Reinforcement Learning for Portfolio Optimization},\n  author={Smith, John and Doe, Jane},\n  journal={Journal of Financial Economics},\n  year={2023},\n  doi={10.1016/j.jfineco.2023.01.001}\n}\n```\n\n## Datasets\n\n- `data/stock_prices.csv` - Historical stock prices (S&P 500, 2010-2023)\n- `data/portfolio_returns.csv` - Simulated portfolio returns\n\n## Requirements\n\n```\nnumpy>=1.21.0\npandas>=1.3.0\ntensorflow>=2.8.0\ngym>=0.21.0\n```\n\n## Reproducibility\n\nAll experiments can be reproduced using:\n\n```bash\npython train_dqn.py --config configs/default.yaml\npython evaluate.py --model checkpoints/best_model.h5\n```\n\n## References\n\nThis work builds on:\n- Mnih et al. (2015) - Human-level control through deep reinforcement learning\n- Jiang et al. (2017) - A deep reinforcement learning framework for the financial portfolio\n",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "portfolio-optimization-ml",
      "research": {
        "title": "Machine learning approaches to portfolio optimization using deep reinforcement learning",
        "abstract": "We propose a novel deep reinforcement learning approach for portfolio optimization that outperforms traditional mean-variance optimization. Our method uses a Deep Q-Network (DQN) to learn optimal trading strategies from historical market data.",
        "keywords": [],
        "authors": [
          {
            "name": "John Smith",
            "affiliation": "University of Finance"
          },
          {
            "name": "Jane Doe",
            "affiliation": "Tech Institute"
          }
        ]
      },
      "publications": [
        {
          "type": "journal-article",
          "doi": "10.1016/j.jfineco.2023.01.001",
          "url": "https://doi.org/10.1016/j.jfineco.2023.01.001",
          "title": "Financial markets and unemployment",
          "abstract": "",
          "published": "2023-03-01",
          "year": 2023,
          "authors": [
            {
              "name": "Tommaso Monacelli"
            },
            {
              "name": "Vincenzo Quadrini"
            },
            {
              "name": "Antonella Trigari"
            }
          ],
          "venue": "Journal of Financial Economics",
          "publisher": "Elsevier BV",
          "citation_count": 16,
          "reference_count": 27
        },
        {
          "type": "preprint",
          "arxiv_id": "2103.12345",
          "url": "https://arxiv.org/abs/2103.12345",
          "title": "The Success of AdaBoost and Its Application in Portfolio Management",
          "abstract": "We develop a novel approach to explain why AdaBoost is a successful classifier. By introducing a measure of the influence of the noise points (ION) in the training data for the binary classification problem, we prove that there is a strong connection between the ION and the test error. We further identify that the ION of AdaBoost decreases as the iteration number or the complexity of the base learners increases. We confirm that it is impossible to obtain a consistent classifier without deep trees as the base learners of AdaBoost in some complicated situations. We apply AdaBoost in portfolio management via empirical studies in the Chinese market, which corroborates our theoretical propositions.",
          "published": "2021-03-23T06:41:42Z",
          "updated": "2021-03-23T06:41:42Z",
          "pdf_url": "https://arxiv.org/pdf/2103.12345.pdf",
          "authors": [
            {
              "name": "Yijian Chuan"
            },
            {
              "name": "Chaoyi Zhao"
            },
            {
              "name": "Zhenrui He"
            },
            {
              "name": "Lan Wu"
            }
          ],
          "categories": [
            "stat.ML",
            "cs.LG",
            "q-fin.PM"
          ],
          "year": 2021
        }
      ],
      "code": {
        "languages": [
          "Unknown"
        ],
        "notebooks": [],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": true,
        "has_dockerfile": false,
        "has_environment_yml": false,
        "has_makefile": false,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [
          "@article{smith2023deep,\n  title={Deep Reinforcement Learning for Portfolio Optimization},\n  author={Smith, John and Doe, Jane},\n  journal={Journal of Financial Economics},\n  year={2023},\n  doi={10.1016/j.jfineco.2023.01.001}\n}"
        ],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:02:26.292105",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": []
    }
  },
  {
    "name": "credit-risk-prediction",
    "full_name": "Digital-AI-Finance/credit-risk-prediction",
    "description": "Neural network models for credit risk prediction with explainable AI",
    "url": "https://github.com/Digital-AI-Finance/credit-risk-prediction",
    "clone_url": "https://github.com/Digital-AI-Finance/credit-risk-prediction.git",
    "homepage": "",
    "language": "Unknown",
    "topics": [
      "credit-risk",
      "explainable-ai",
      "finance",
      "neural-networks"
    ],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 2,
    "default_branch": "main",
    "created_at": "2025-11-21T22:50:51+00:00",
    "updated_at": "2025-11-21T22:52:01+00:00",
    "pushed_at": "2025-11-21T22:51:59+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": false,
    "has_pages": false,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "# Explainable Credit Risk Prediction\n\nRepository for \"Explainable Neural Networks for Credit Risk Assessment\" (SSRN:3456789).\n\n## Authors\n\n- Maria Garcia (Financial Analytics Lab)\n- Robert Chen (AI Research Center)\n\n## Abstract\n\nWe develop an explainable neural network architecture for credit risk prediction that achieves state-of-the-art performance while providing interpretable feature importance scores using SHAP values.\n\n## Published Paper\n\n- **SSRN**: https://ssrn.com/abstract=3456789\n- **DOI**: 10.2139/ssrn.3456789\n\n## Dataset\n\nWe use the German Credit Dataset and a proprietary dataset from a major bank:\n- `data/german_credit.csv` - Public German Credit Data\n- `data/features.csv` - Engineered features\n\n## Notebooks\n\n- `notebooks/01_data_exploration.ipynb` - Exploratory data analysis\n- `notebooks/02_feature_engineering.ipynb` - Feature creation and selection\n- `notebooks/03_model_training.ipynb` - Neural network training\n- `notebooks/04_explainability.ipynb` - SHAP analysis and interpretation\n\n## Requirements\n\n```\ntensorflow>=2.8.0\nscikit-learn>=1.0.0\nshap>=0.40.0\npandas>=1.3.0\nmatplotlib>=3.4.0\njupyter>=1.0.0\n```\n\n## Citation\n\n```bibtex\n@article{garcia2023explainable,\n  title={Explainable Neural Networks for Credit Risk Assessment},\n  author={Garcia, Maria and Chen, Robert},\n  journal={SSRN Electronic Journal},\n  year={2023},\n  doi={10.2139/ssrn.3456789}\n}\n```\n\n## Replication\n\nTo replicate our results:\n\n1. Install dependencies: `pip install -r requirements.txt`\n2. Run notebooks in order: `jupyter notebook`\n3. Train model: `python train.py`\n4. Generate predictions: `python predict.py`\n",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "credit-risk-prediction",
      "research": {
        "title": "Neural network models for credit risk prediction with explainable AI",
        "abstract": "We develop an explainable neural network architecture for credit risk prediction that achieves state-of-the-art performance while providing interpretable feature importance scores using SHAP values.",
        "keywords": [],
        "authors": [
          {
            "name": "Maria Garcia",
            "affiliation": "Financial Analytics Lab"
          },
          {
            "name": "Robert Chen",
            "affiliation": "AI Research Center"
          }
        ]
      },
      "publications": [
        {
          "type": "journal",
          "doi": "10.2139/ssrn.3456789",
          "url": "https://doi.org/10.2139/ssrn.3456789"
        },
        {
          "type": "working_paper",
          "ssrn_id": "3456789",
          "url": "https://ssrn.com/abstract=3456789",
          "note": "Limited metadata available (no SSRN API)"
        }
      ],
      "code": {
        "languages": [
          "Unknown"
        ],
        "notebooks": [],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": true,
        "has_dockerfile": false,
        "has_environment_yml": false,
        "has_makefile": false,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [
          "@article{garcia2023explainable,\n  title={Explainable Neural Networks for Credit Risk Assessment},\n  author={Garcia, Maria and Chen, Robert},\n  journal={SSRN Electronic Journal},\n  year={2023},\n  doi={10.2139/ssrn.3456789}\n}"
        ],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:02:28.206627",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": []
    }
  },
  {
    "name": "market-microstructure",
    "full_name": "Digital-AI-Finance/market-microstructure",
    "description": "High-frequency trading analysis and market microstructure research",
    "url": "https://github.com/Digital-AI-Finance/market-microstructure",
    "clone_url": "https://github.com/Digital-AI-Finance/market-microstructure.git",
    "homepage": "",
    "language": "Unknown",
    "topics": [
      "finance",
      "high-frequency-trading",
      "market-microstructure",
      "time-series"
    ],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 2,
    "default_branch": "main",
    "created_at": "2025-11-21T22:50:56+00:00",
    "updated_at": "2025-11-21T22:52:05+00:00",
    "pushed_at": "2025-11-21T22:52:03+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": false,
    "has_pages": false,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "# Market Microstructure Analysis\n\nCode repository for \"Price Discovery in High-Frequency Markets\" (Journal of Finance, 2024).\n\n## Authors\n\n- David Lee (Quantitative Finance Department)\n- Sarah Williams (Trading Systems Lab)\n\n## Abstract\n\nWe analyze price discovery mechanisms in high-frequency trading environments using millisecond-level order book data from major exchanges.\n\n## Published Papers\n\n- **Main Paper**: DOI: 10.1111/jofi.2024.12345\n- **Working Paper**: arXiv:2201.67890\n\n## Data\n\n- `data/orderbook/` - Limit order book snapshots (10ms frequency)\n- `data/trades/` - Trade and quote data\n- Total size: ~50GB (available via Zenodo: 10.5281/zenodo.1234567)\n\n## Code Structure\n\n```\nsrc/\n  orderbook.py - Order book reconstruction\n  metrics.py - Microstructure metrics\n  analysis.py - Statistical analysis\nnotebooks/\n  analysis.ipynb - Main analysis notebook\ntests/\n  test_orderbook.py - Unit tests\n```\n\n## Requirements\n\n```\nnumpy>=1.21.0\npandas>=1.3.0\nnumba>=0.55.0\npytest>=6.2.0\n```\n\n## Docker\n\nFor reproducibility, we provide a Docker environment:\n\n```bash\ndocker build -t market-microstructure .\ndocker run -v $(pwd)/data:/data market-microstructure python src/analysis.py\n```\n\n## Citation\n\n```bibtex\n@article{lee2024price,\n  title={Price Discovery in High-Frequency Markets},\n  author={Lee, David and Williams, Sarah},\n  journal={Journal of Finance},\n  year={2024},\n  doi={10.1111/jofi.2024.12345}\n}\n```\n\n## References\n\nThis work cites and extends:\n- Hasbrouck (1991) - Measuring the information content of stock trades\n- Biais et al. (1995) - An empirical analysis of the limit order book\n",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "market-microstructure",
      "research": {
        "title": "High-frequency trading analysis and market microstructure research",
        "abstract": "We analyze price discovery mechanisms in high-frequency trading environments using millisecond-level order book data from major exchanges.",
        "keywords": [],
        "authors": [
          {
            "name": "David Lee",
            "affiliation": "Quantitative Finance Department"
          },
          {
            "name": "Sarah Williams",
            "affiliation": "Trading Systems Lab"
          }
        ]
      },
      "publications": [
        {
          "type": "journal",
          "doi": "10.5281/zenodo.1234567)",
          "url": "https://doi.org/10.5281/zenodo.1234567)"
        },
        {
          "type": "journal",
          "doi": "10.1111/jofi.2024.12345",
          "url": "https://doi.org/10.1111/jofi.2024.12345"
        },
        {
          "type": "preprint",
          "arxiv_id": "2201.67890",
          "url": "https://arxiv.org/abs/2201.67890"
        }
      ],
      "code": {
        "languages": [
          "Unknown"
        ],
        "notebooks": [],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": true,
        "has_dockerfile": false,
        "has_environment_yml": false,
        "has_makefile": false,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [
          "@article{lee2024price,\n  title={Price Discovery in High-Frequency Markets},\n  author={Lee, David and Williams, Sarah},\n  journal={Journal of Finance},\n  year={2024},\n  doi={10.1111/jofi.2024.12345}\n}"
        ],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:02:30.131016",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": []
    }
  },
  {
    "name": "Green-Finance",
    "full_name": "Digital-AI-Finance/Green-Finance",
    "description": "Academic course generation system for Green Finance Professional Certificate - 8-week course with learning-goal-driven pedagogy, Beamer slides, and interactive React app",
    "url": "https://github.com/Digital-AI-Finance/Green-Finance",
    "clone_url": "https://github.com/Digital-AI-Finance/Green-Finance.git",
    "homepage": "",
    "language": "TeX",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 5294,
    "default_branch": "main",
    "created_at": "2025-11-22T07:02:13+00:00",
    "updated_at": "2025-11-22T20:38:15+00:00",
    "pushed_at": "2025-11-22T20:38:12+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": false,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "# Green Finance Professional Certificate\n\nAcademic course generation system for 8-week Green Finance course with learning-goal-driven pedagogy, Beamer LaTeX slides, and interactive React web application.\n\n## Live Interactive App\n\n**üåê https://digital-ai-finance.github.io/Green-Finance**\n\nInteractive Week 1 learning platform with 30 slides, charts, and self-assessment.\n\n---\n\n## Project Overview\n\n### Course Structure\n- **Duration:** 8 weeks\n- **Format:** Academic professional certificate\n- **Pedagogy:** Learning-goal-driven (3 goals per week)\n- **Delivery:** Beamer PDF slides + Interactive web app\n\n### Week 1 Status\n‚úÖ **Complete** - 37 slides, 17 charts, fully validated\n- Core: 30 slides (3 learning goals √ó 10 slides)\n- Supplementary: 7 empirical validation slides\n- Interactive React app deployed\n\n### Weeks 2-8 Status\n‚è≥ **Ready for generation** using proven Week 1 template\n\n---\n\n## Quick Start\n\n### View Interactive App\nVisit: **https://digital-ai-finance.github.io/Green-Finance**\n\n### Run Locally\n```bash\ncd react-app\nnpm install\nnpm start\n# Opens at http://localhost:3000\n```\n\n### Compile LaTeX Slides\n```bash\n# Compile Week 1\npdflatex -interaction=nonstopmode 20251121_2306_Week1_v2_GreenFinanceFoundations.tex\npdflatex -interaction=nonstopmode 20251121_2306_Week1_v2_GreenFinanceFoundations.tex\n\n# Clean up auxiliary files\nMove-Item -Path *.aux,*.log,*.out,*.nav,*.toc,*.snm -Destination temp\\ -ErrorAction SilentlyContinue\n```\n\n### Generate Charts\n```powershell\n# Generate all Week 1 charts\nGet-ChildItem charts\\week1\\*.py | ForEach-Object { python $_.FullName }\n\n# Generate Graphviz diagrams (requires Graphviz installed)\nGet-ChildItem charts\\week1\\*.dot | ForEach-Object {\n    dot -Tpdf $_.FullName -o ($_.FullName -replace '\\.dot$','.pdf')\n}\n```\n\n---\n\n## Repository Structure\n\n```\nGreen-Finance/\n‚îú‚îÄ‚îÄ react-app/              # Interactive web app (Week 1)\n‚îÇ   ‚îú‚îÄ‚îÄ src/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/     # React components\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ charts/         # Interactive chart components\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data/           # Course content (slides)\n‚îÇ   ‚îú‚îÄ‚îÄ DEPLOYMENT.md       # Deployment guide\n‚îÇ   ‚îî‚îÄ‚îÄ package.json\n‚îÇ\n‚îú‚îÄ‚îÄ charts/week1/           # Chart generation scripts\n‚îÇ   ‚îú‚îÄ‚îÄ week1_v2_goal1_*.py     # Goal 1 charts (matplotlib)\n‚îÇ   ‚îú‚îÄ‚îÄ week1_v2_goal2_*.py     # Goal 2 charts (matplotlib)\n‚îÇ   ‚îú‚îÄ‚îÄ week1_v2_goal3_*.py     # Goal 3 charts (matplotlib)\n‚îÇ   ‚îî‚îÄ‚îÄ *.dot                    # Graphviz diagrams\n‚îÇ\n‚îú‚îÄ‚îÄ agents/                 # Multi-agent system specs\n‚îÇ   ‚îú‚îÄ‚îÄ AGENT_3_ContentPlanner_v2.md\n‚îÇ   ‚îú‚îÄ‚îÄ AGENT_4_SlideGenerator_v2_LaTeXTemplates.md\n‚îÇ   ‚îî‚îÄ‚îÄ README_v2.md\n‚îÇ\n‚îú‚îÄ‚îÄ *.tex                   # LaTeX source files\n‚îú‚îÄ‚îÄ *.pdf                   # Compiled slide PDFs\n‚îú‚îÄ‚îÄ template_beamer_final.tex   # Madrid theme template\n‚îú‚îÄ‚îÄ CLAUDE.md               # Project documentation\n‚îú‚îÄ‚îÄ COURSE_GENERATOR_v2.md  # Generation system spec\n‚îî‚îÄ‚îÄ DEPLOY_QUICKSTART.md    # Deployment quick start\n```\n\n---\n\n## Features\n\n### Interactive Web App\n- ‚úÖ 30 slides with keyboard navigation\n- ‚úÖ 3 learning goals with progress tracking\n- ‚úÖ Interactive charts (Recharts)\n- ‚úÖ Self-assessment quizzes\n- ‚úÖ LocalStorage persistence\n- ‚úÖ Mobile responsive\n- ‚úÖ GitHub Pages deployment\n\n### LaTeX Slides (v2.0)\n- ‚úÖ Madrid theme (8pt, 16:9)\n- ‚úÖ 3 learning goals per week\n- ‚úÖ Goal-driven narrative structure\n- ‚úÖ 4 specialized slide types\n- ‚úÖ 17 charts (33%+ ratio)\n- ‚úÖ Bottom notes with goal tracking\n\n### Multi-Agent System\n- ‚úÖ Course Orchestrator\n- ‚úÖ Guidelines Expert\n- ‚úÖ Content Planner v2.0\n- ‚úÖ Slide Generator v2.0\n- ‚úÖ YAML-based communication\n\n---\n\n## Technologies\n\n### Web App\n- React 18.2\n- Material-UI 5.14\n- Recharts 2.8 (charts)\n- Framer Motion 10.16 (animations)\n- D3 7.8 (advanced visualizations)\n\n### Slides & Charts\n- LaTeX Beamer (Madrid theme)\n- Python 3.x + matplotlib\n- Graphviz (diagrams)\n- pdflatex\n\n### Deployment\n- GitHub Pages (automatic)\n- GitHub Actions (CI/CD)\n\n---\n\n## Learning Goals (Week 1)\n\n### Goal 1: Market Microstructure Theory\nUnderstand the theoretical foundations explaining why green finance markets exist and how they function.\n\n**Slides:** 1-10 | **Type:** Theoretical | **Charts:** Ecosystem diagrams\n\n### Goal 2: Quantify Market Size & Growth\nQuantify the size, growth trajectory, and composition of global green finance markets using empirical data.\n\n**Slides:** 11-20 | **Type:** Quantitative | **Charts:** Time series, distributions\n\n### Goal 3: Derive Pricing Models\nDerive and apply mathematical models for pricing green financial instruments, incorporating greenium and ESG factors.\n\n**Slides:** 21-30 | **Type:** Mathematical | **Charts:** Yield curves, risk-return\n\n---\n\n## Documentation\n\n- **[CLAUDE.md](CLAUDE.md)** - Complete project documentation\n- **[DEPLOY_QUICKSTART.md](DEPLOY_QUICKSTART.md)** - Deploy in 3 steps\n- **[react-app/DEPLOYMENT.md](react-app/DEPLOYMENT.md)** - Full deployment guide\n- **[COURSE_GENERATOR_v2.md](COURSE_GENERATOR_v2.md)** - System specification\n- **[agents/README_v2.md](agents/README_v2.md)** - Multi-agent architecture\n\n---\n\n## Deployment\n\n### Automatic (GitHub Actions)\n1. Enable GitHub Pages: Settings ‚Üí Pages ‚Üí Source: **GitHub Actions**\n2. Push changes: `git push origin main`\n3. Visit: https://digital-ai-finance.github.io/Green-Finance\n\nSee **[DEPLOY_QUICKSTART.md](DEPLOY_QUICKSTART.md)** for detailed steps.\n\n### Manual\n```bash\ncd react-app\nnpm run deploy\n```\n\n---\n\n## Development Workflow\n\n### Add New Week\n1. Create content outline: `weekN_v2_content_outline.yaml`\n2. Generate charts: `python charts/weekN/*.py`\n3. Generate LaTeX: Use `AGENT_4_SlideGenerator_v2_LaTeXTemplates.md`\n4. Compile: `pdflatex YYYYMMDD_HHMM_WeekN_Title.tex` (2x)\n5. Validate: 30 slides, 10+ charts, 3 goals\n\n### Update React App\n1. Edit content: `react-app/src/data/week1Slides.js`\n2. Test locally: `npm start`\n3. Commit and push: Auto-deploys via GitHub Actions\n\n---\n\n## Quality Standards\n\n### Per Week Requirements\n- **Core slides:** 30 (3 goals √ó 10 slides)\n- **Charts:** 10-11 minimum\n- **Chart ratio:** ‚â•33%\n- **Learning goals:** Exactly 3 (typed with narrative roles)\n- **Statistics:** All verified via web search\n\n### Color Scheme (Consistent)\n- Primary (mlpurple): `#3333B2`\n- Secondary (mllavender): `#ADADE0`\n- Success (mlgreen): `#2CA02C`\n- Warning (mlorange): `#FF7F0E`\n\n---\n\n## Prerequisites\n\n- **Node.js** 18+ (for React app)\n- **Python** 3.x (for charts)\n- **pdflatex** (TeX Live or MiKTeX)\n- **Graphviz** (for diagrams)\n- **Git** (version control)\n\n---\n\n## Contributing\n\nThis is an academic project. For questions or improvements:\n1. Open an issue\n2. Submit a pull request\n3. Contact: [Digital-AI-Finance organization](https://github.com/Digital-AI-Finance)\n\n---\n\n## License\n\nAcademic use. All rights reserved.\n\n---\n\n## Next Steps\n\n- [ ] Generate Weeks 2-8\n- [ ] Add more interactive features to web app\n- [ ] Create student exercises\n- [ ] Add video explanations\n- [ ] Develop assessment module\n\n---\n\n**Version:** 2.0 (Learning-Goal-Driven)\n**Status:** Week 1 Complete, Production Ready\n**Live App:** https://digital-ai-finance.github.io/Green-Finance\n",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "Green-Finance",
      "research": {
        "title": "Academic course generation system for Green Finance Professional Certificate - 8-week course with learning-goal-driven pedagogy, Beamer slides, and interactive React app",
        "abstract": "Academic course generation system for 8-week Green Finance course with learning-goal-driven pedagogy, Beamer LaTeX slides, and interactive React web application.",
        "keywords": [],
        "authors": []
      },
      "publications": [],
      "code": {
        "languages": [
          "TeX"
        ],
        "notebooks": [],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": false,
        "has_dockerfile": false,
        "has_environment_yml": false,
        "has_makefile": false,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:02:38.370177",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": [
        {
          "name": "settings.local.json",
          "path": ".claude/settings.local.json",
          "format": ".json",
          "size_bytes": 159
        },
        {
          "name": "academic_citations.json",
          "path": "academic_citations.json",
          "format": ".json",
          "size_bytes": 2577
        },
        {
          "name": "package-lock.json",
          "path": "react-app/package-lock.json",
          "format": ".json",
          "size_bytes": 709487
        },
        {
          "name": "package.json",
          "path": "react-app/package.json",
          "format": ".json",
          "size_bytes": 1435
        },
        {
          "name": "data",
          "path": "react-app/src/data",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "week1Slides.js",
          "path": "react-app/src/data/week1Slides.js",
          "format": ".js",
          "size_bytes": 29245
        },
        {
          "name": "week1Slides_backup.js",
          "path": "react-app/src/data/week1Slides_backup.js",
          "format": ".js",
          "size_bytes": 27087
        },
        {
          "name": "week1Slides_updated.js",
          "path": "react-app/src/data/week1Slides_updated.js",
          "format": ".js",
          "size_bytes": 29245
        },
        {
          "name": "week1Slides_v3.js",
          "path": "react-app/src/data/week1Slides_v3.js",
          "format": ".js",
          "size_bytes": 31673
        },
        {
          "name": "verified_statistics.json",
          "path": "verified_statistics.json",
          "format": ".json",
          "size_bytes": 8551
        }
      ]
    }
  },
  {
    "name": "Natural-Language-Processing",
    "full_name": "Digital-AI-Finance/Natural-Language-Processing",
    "description": "NLP Course 2025: From N-grams to Transformers - Complete 12-week curriculum with discovery-based pedagogy",
    "url": "https://github.com/Digital-AI-Finance/Natural-Language-Processing",
    "clone_url": "https://github.com/Digital-AI-Finance/Natural-Language-Processing.git",
    "homepage": "",
    "language": "Jupyter Notebook",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 218696,
    "default_branch": "main",
    "created_at": "2025-11-22T07:10:08+00:00",
    "updated_at": "2025-12-04T11:55:44+00:00",
    "pushed_at": "2025-12-04T11:55:41+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": false,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "# NLP Course 2025: From N-grams to Transformers\n\n<p align=\"center\">\n  <a href=\"https://quantlet.de\">\n    <img src=\"logo/quantlet.png\" alt=\"QuantLet Logo\" width=\"120\"/>\n  </a>\n</p>\n\n<p align=\"center\">\n  <strong>QuantLet-Compatible Course Materials</strong>\n</p>\n\n![Course Status](https://img.shields.io/badge/weeks-12%2F12%20complete-brightgreen)\n![Framework](https://img.shields.io/badge/framework-100%25%20applied-success)\n![Labs](https://img.shields.io/badge/labs-12%20notebooks-blue)\n![Charts](https://img.shields.io/badge/charts-168%20visualizations-purple)\n![License](https://img.shields.io/badge/license-MIT-orange)\n\n> A comprehensive Natural Language Processing course covering statistical foundations through modern transformer architectures. Build ChatGPT from scratch!\n\n## Quick Start (3 Steps)\n\n```bash\n# 1. Clone the repository\ngit clone https://github.com/josterri/2025_NLP_Lectures.git\ncd 2025_NLP_Lectures\n\n# 2. Install dependencies\npip install -r requirements.txt\n\n# 3. Start learning!\njupyter lab NLP_slides/week02_neural_lm/lab/week02_word_embeddings_lab.ipynb\n```\n\n## What You'll Learn\n\nThis course takes you from foundational statistical methods to state-of-the-art neural architectures:\n\n- **Weeks 1-2:** Statistical language models and word embeddings (Word2Vec, GloVe)\n- **Weeks 3-4:** Sequential models (RNN/LSTM) and sequence-to-sequence with attention\n- **Weeks 5-7:** Transformers, BERT, GPT, and advanced architectures\n- **Weeks 8-10:** Tokenization, decoding strategies, and fine-tuning\n- **Weeks 11-12:** Efficiency optimization and ethical AI deployment\n\nBy the end, you'll build a working transformer from scratch and understand the architecture behind ChatGPT and Claude.\n\n## Course Structure\n\n### Core Materials (12 Weeks)\nEach week includes:\n- **Presentation:** LaTeX/Beamer slides with optimal readability\n- **Lab Notebook:** Interactive Jupyter notebook with hands-on exercises\n- **Handouts:** Pre-class discovery exercises and post-class technical practice\n\n### Supplementary Modules\n- **Neural Network Primer:** Zero pre-knowledge intro to neural networks\n- **LSTM Primer:** Comprehensive deep dive into LSTM architecture (32 slides)\n- **Embeddings Module:** Standalone word embedding module with 3D visualizations\n\n### Total Content\n- 60+ presentations (including versions and supplements)\n- 12 interactive lab notebooks\n- 40+ handout documents\n- 100+ Python-generated figures\n- 8 progressive visualization notebooks\n\n## Prerequisites\n\n- **Required:**\n  - Python 3.8 or higher\n  - Basic linear algebra (vectors, matrices)\n  - Basic probability theory\n  - Comfortable with Python programming\n\n- **Helpful but not required:**\n  - PyTorch experience\n  - Understanding of backpropagation\n  - Machine learning fundamentals\n\n**New to neural networks?** Start with our Neural Network Primer module before Week 2.\n\n## Installation\n\n### Option 1: pip (Recommended)\n```bash\npip install -r requirements.txt\n```\n\n### Option 2: conda\n```bash\nconda env create -f environment.yml\nconda activate nlp2025\n```\n\n### GPU Support\nFor GPU acceleration (recommended for Weeks 5+):\n```bash\n# CUDA 11.8\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# CUDA 12.1\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n```\n\nSee [INSTALLATION.md](INSTALLATION.md) for detailed setup instructions and troubleshooting.\n\n## Course Navigation\n\n### Week-by-Week Guide\nFull navigation with topics, prerequisites, and learning objectives: [COURSE_INDEX.md](COURSE_INDEX.md)\n\n### Week Highlights\n\n| Week | Topic | Key Concepts | Lab |\n|------|-------|--------------|-----|\n| 1 | Foundations | N-grams, perplexity, statistical LM | - |\n| 2 | Word Embeddings | Word2Vec, GloVe, neural LM | Implement embeddings |\n| 3 | RNN/LSTM | Sequential models, BPTT | Build LSTM from scratch |\n| 4 | Seq2Seq | Attention mechanism, translation | Machine translation |\n| 5 | Transformers | Self-attention, multi-head | Build transformer |\n| 6 | Pre-trained | BERT, GPT, transfer learning | Fine-tune BERT |\n| 7 | Advanced | T5, GPT-3, scaling laws | Experiment with GPT |\n| 8 | Tokenization | BPE, WordPiece, SentencePiece | Implement tokenizer |\n| 9 | Decoding | Beam, sampling, nucleus, **contrastive** | Compare 6 methods |\n| 10 | Fine-tuning | LoRA, prompt engineering | Adapt models |\n| 11 | Efficiency | Quantization, distillation | Optimize models |\n| 12 | Ethics | Bias, fairness, safety | Measure bias |\n\n## Quantlet Charts\n\nAll Python-generated visualizations follow the [Quantlet](https://quantlet.de) standard format with:\n- Numbered folders (`01_chart_name/`, `02_chart_name/`, etc.)\n- Self-contained Python scripts\n- Standard `metainfo.txt` with description, keywords, and usage\n\n### Final Lecture Charts\nSee [FinalLecture/](FinalLecture/) for 8 Quantlet-formatted visualizations covering:\n- Vector database architecture\n- HNSW nearest neighbor search\n- RAG conditional probabilities\n- Hybrid search flow\n\n## Project Structure\n\n```\n‚îú‚îÄ‚îÄ FinalLecture/               # Quantlet-formatted charts (Final Lecture)\n‚îú‚îÄ‚îÄ logo/                       # Quantlet branding\n‚îú‚îÄ‚îÄ NLP_slides/\n‚îÇ   ‚îú‚îÄ‚îÄ week01_foundations/      # Week 1: Statistical LM\n‚îÇ   ‚îú‚îÄ‚îÄ week02_neural_lm/        # Week 2: Word embeddings\n‚îÇ   ‚îú‚îÄ‚îÄ week03_rnn/              # Week 3: RNN/LSTM/GRU\n‚îÇ   ‚îú‚îÄ‚îÄ ...                      # Weeks 4-12\n‚îÇ   ‚îú‚îÄ‚îÄ nn_primer/               # Neural network primer\n‚îÇ   ‚îú‚îÄ‚îÄ lstm_primer/             # LSTM deep dive\n‚îÇ   ‚îî‚îÄ‚îÄ common/                  # Shared templates and utils\n‚îú‚îÄ‚îÄ embeddings/                  # Standalone embeddings module\n‚îú‚îÄ‚îÄ exercises/                   # Additional practice\n‚îú‚îÄ‚îÄ figures/                     # Shared visualizations\n‚îú‚îÄ‚îÄ requirements.txt             # Python dependencies\n‚îú‚îÄ‚îÄ environment.yml              # Conda environment\n‚îî‚îÄ‚îÄ COURSE_INDEX.md              # Full course navigation\n```\n\n## Key Learning Milestones\n\n- ‚úÖ **After Week 2:** Understand and implement word embeddings\n- ‚úÖ **After Week 3:** Build RNN and LSTM from scratch\n- ‚úÖ **After Week 5:** Comprehend transformer architecture completely\n- ‚úÖ **After Week 6:** Fine-tune pre-trained models (BERT, GPT)\n- ‚úÖ **After Week 9:** Control text generation quality and diversity\n- ‚úÖ **After Week 12:** Deploy models responsibly with ethical considerations\n\n## Usage Examples\n\n### Run a Lab Notebook\n```bash\n# Start Jupyter Lab\njupyter lab\n\n# Navigate to a week's lab folder\ncd NLP_slides/week05_transformers/lab\njupyter notebook week05_transformer_lab.ipynb\n```\n\n### Compile a Presentation\n```bash\ncd NLP_slides/week02_neural_lm/presentations\npdflatex week02_neural_lm.tex\n```\n\n### Generate Figures\n```bash\ncd NLP_slides/week05_transformers/python\npython generate_week05_optimal_charts.py\n```\n\n## Testing the Course\n\nTest all lab notebooks for execution:\n```bash\npython test_notebooks.py\n```\n\nThis validates that all 12 lab notebooks execute correctly in your environment.\n\n## Course Delivery Options\n\n### Standard 12-Week Semester\n- One week per topic\n- Weekly labs and assignments\n- Suitable for undergraduate/graduate courses\n\n### Intensive 8-Week Course\n- Combine Weeks 1-2, skip some advanced topics\n- Accelerated pace for bootcamps\n- Focus on core transformer concepts\n\n### Self-Paced Learning\n- Progress at your own speed\n- Complete prerequisite modules first\n- Focus on labs and hands-on practice\n\n## Documentation\n\n- **[COURSE_INDEX.md](COURSE_INDEX.md)** - Complete week-by-week navigation\n- **[INSTALLATION.md](INSTALLATION.md)** - Detailed setup instructions\n- **[CLAUDE.md](CLAUDE.md)** - Development guide and conventions\n- **[status.md](status.md)** - Project status and completion tracking\n- **[changelog.md](changelog.md)** - Change history\n\n## Support and Resources\n\n- **Issues:** Report problems at [GitHub Issues](https://github.com/josterri/2025_NLP_Lectures/issues)\n- **Prerequisites:** Check the Neural Network Primer if you're new to deep learning\n- **GPU Requirements:** Most labs work on CPU; Weeks 5+ benefit from GPU\n\n## Contributing\n\nContributions are welcome! Areas for contribution:\n- Additional exercises and examples\n- Translations to other languages\n- MSc-level challenge problems\n- Bug fixes and improvements\n\n## License\n\nThis course is released under the MIT License. See LICENSE for details.\n\n## Acknowledgments\n\nCourse materials developed with pedagogical focus on:\n- Discovery-based learning\n- Concrete-to-abstract progression\n- Hands-on implementation\n- Real-world applications\n\nBuilt with LaTeX/Beamer, Python, PyTorch, and Jupyter.\n\n## Citation\n\nIf you use these materials in your course or research, please cite:\n\n```bibtex\n@misc{nlp2025course,\n  title={NLP Course 2025: From N-grams to Transformers},\n  author={Joerg Osterrieder},\n  year={2025},\n  url={https://github.com/josterri/2025_NLP_Lectures}\n}\n```\n\n---\n\n**Ready to start?** Check [INSTALLATION.md](INSTALLATION.md) for setup, then dive into Week 2's word embeddings lab!\n\n**Questions?** See [COURSE_INDEX.md](COURSE_INDEX.md) for complete navigation and prerequisites.",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "Natural-Language-Processing",
      "research": {
        "title": "NLP Course 2025: From N-grams to Transformers - Complete 12-week curriculum with discovery-based pedagogy",
        "abstract": "<p align=\"center\">\n  <a href=\"https://quantlet.de\">\n    <img src=\"logo/quantlet.png\" alt=\"QuantLet Logo\" width=\"120\"/>\n  </a>\n</p>",
        "keywords": [],
        "authors": [
          {
            "name": "the end, you'll build a working transformer from scratch and understand the architecture behind ChatGPT and Claude."
          }
        ]
      },
      "publications": [],
      "code": {
        "languages": [
          "Jupyter Notebook"
        ],
        "notebooks": [
          {
            "path": "NLP_slides/summarization_module/lab/llm_summarization_lab.ipynb",
            "title": "llm_summarization_lab",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "NLP_slides/week01_foundations/lab/week01_ngrams_lab.ipynb",
            "title": "week01_ngrams_lab",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "NLP_slides/week02_neural_lm/lab/week02_word_embeddings_lab.ipynb",
            "title": "week02_word_embeddings_lab",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "NLP_slides/week03_rnn/lab/week03_rnn_lab.ipynb",
            "title": "week03_rnn_lab",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "NLP_slides/week03_rnn/lab/week03_rnn_lab_enhanced.ipynb",
            "title": "week03_rnn_lab_enhanced",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "NLP_slides/week04_seq2seq/lab/week04_part1_basic_seq2seq.ipynb",
            "title": "week04_part1_basic_seq2seq",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "NLP_slides/week04_seq2seq/lab/week04_part2_attention.ipynb",
            "title": "week04_part2_attention",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "NLP_slides/week04_seq2seq/lab/week04_part3_advanced.ipynb",
            "title": "week04_part3_advanced",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "NLP_slides/week04_seq2seq/lab/week04_seq2seq_lab.ipynb",
            "title": "week04_seq2seq_lab",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "NLP_slides/week04_seq2seq/lab/week04_seq2seq_lab_enhanced.ipynb",
            "title": "week04_seq2seq_lab_enhanced",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "NLP_slides/week05_transformers/lab/week05_transformer_lab.ipynb",
            "title": "week05_transformer_lab",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "NLP_slides/week06_pretrained/lab/week06_bert_finetuning.ipynb",
            "title": "week06_bert_finetuning",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "NLP_slides/week06_pretrained/lab/week06_pretrained_feature_extraction.ipynb",
            "title": "week06_pretrained_feature_extraction",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "NLP_slides/week07_advanced/lab/week07_advanced_transformers_lab.ipynb",
            "title": "week07_advanced_transformers_lab",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "NLP_slides/week08_tokenization/lab/week08_tokenization_lab.ipynb",
            "title": "week08_tokenization_lab",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "NLP_slides/week09_decoding/lab/week09_decoding_lab.ipynb",
            "title": "week09_decoding_lab",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "NLP_slides/week09_decoding/lab/week09_decoding_simplified.ipynb",
            "title": "week09_decoding_simplified",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "NLP_slides/week10_finetuning/lab/week10_finetuning_lab.ipynb",
            "title": "week10_finetuning_lab",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "NLP_slides/week11_efficiency/lab/week11_efficiency_lab.ipynb",
            "title": "week11_efficiency_lab",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "NLP_slides/week12_ethics/lab/week12_ethics_lab.ipynb",
            "title": "week12_ethics_lab",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "demos/demo_agent_multistep.ipynb",
            "title": "demo_agent_multistep",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "demos/demo_rag_simple.ipynb",
            "title": "demo_rag_simple",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "demos/demo_reasoning_compare.ipynb",
            "title": "demo_reasoning_compare",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "embeddings/handouts/discovery_notebook.ipynb",
            "title": "discovery_notebook",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "embeddings/word_embeddings_3d_msc.ipynb",
            "title": "word_embeddings_3d_msc",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/ngrams_Alice_in_Wonderland.ipynb",
            "title": "ngrams_Alice_in_Wonderland",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/shakespeare/shakespeare_sonnets_simple_bsc.ipynb",
            "title": "shakespeare_sonnets_simple_bsc",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "notebooks/visualizations/1_simple_ngrams.ipynb",
            "title": "1_simple_ngrams",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "notebooks/visualizations/2_word_embeddings.ipynb",
            "title": "2_word_embeddings",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "notebooks/visualizations/3_simple_neural_net.ipynb",
            "title": "3_simple_neural_net",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "notebooks/visualizations/4_compare_NLP_methods.ipynb",
            "title": "4_compare_NLP_methods",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "notebooks/visualizations/5_Tokens Journey Through a Transformer.ipynb",
            "title": "5_Tokens Journey Through a Transformer",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "notebooks/visualizations/6_Transformers in 3D A Visual Journey.ipynb",
            "title": "6_Transformers in 3D A Visual Journey",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "notebooks/visualizations/7_Transformers_in_3d_simplified.ipynb",
            "title": "7_Transformers_in_3d_simplified",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "notebooks/visualizations/8_How_Transformers_Learn_Training_in_3D.ipynb",
            "title": "8_How_Transformers_Learn_Training_in_3D",
            "language": "python",
            "type": "jupyter"
          }
        ],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": true,
        "has_dockerfile": false,
        "has_environment_yml": true,
        "has_makefile": true,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [
          "@misc{nlp2025course,\n  title={NLP Course 2025: From N-grams to Transformers},\n  author={Joerg Osterrieder},\n  year={2025},\n  url={https://github.com/josterri/2025_NLP_Lectures}\n}"
        ],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:03:48.442537",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": [
        {
          "name": "package-lock.json",
          "path": "NLP_slides/week09_decoding/learning-app/package-lock.json",
          "format": ".json",
          "size_bytes": 153181
        },
        {
          "name": "package.json",
          "path": "NLP_slides/week09_decoding/learning-app/package.json",
          "format": ".json",
          "size_bytes": 917
        },
        {
          "name": "package-lock.json",
          "path": "NLP_slides/week09_decoding/react-app/package-lock.json",
          "format": ".json",
          "size_bytes": 166558
        },
        {
          "name": "package.json",
          "path": "NLP_slides/week09_decoding/react-app/package.json",
          "format": ".json",
          "size_bytes": 974
        },
        {
          "name": "search.json",
          "path": "docs/search.json",
          "format": ".json",
          "size_bytes": 6309
        },
        {
          "name": "notebook_test_results.json",
          "path": "notebook_test_results.json",
          "format": ".json",
          "size_bytes": 9703
        }
      ]
    }
  },
  {
    "name": "neural-networks",
    "full_name": "Digital-AI-Finance/neural-networks",
    "description": "Pedagogical Beamer presentation: Neural Networks for Business Applications - Complete course with 20 data-driven visualizations using real trained models",
    "url": "https://github.com/Digital-AI-Finance/neural-networks",
    "clone_url": "https://github.com/Digital-AI-Finance/neural-networks.git",
    "homepage": "",
    "language": "TeX",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 8407,
    "default_branch": "main",
    "created_at": "2025-11-24T21:13:02+00:00",
    "updated_at": "2025-12-04T10:55:01+00:00",
    "pushed_at": "2025-12-04T10:54:57+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": true,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "No README available",
    "latest_release": null
  },
  {
    "name": "ShareBuybacks",
    "full_name": "Digital-AI-Finance/ShareBuybacks",
    "description": "Share Buyback Strategy Simulation App - Streamlit with GBM and trading strategies",
    "url": "https://github.com/Digital-AI-Finance/ShareBuybacks",
    "clone_url": "https://github.com/Digital-AI-Finance/ShareBuybacks.git",
    "homepage": "",
    "language": "Python",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 1,
    "size": 529,
    "default_branch": "main",
    "created_at": "2025-11-27T12:42:48+00:00",
    "updated_at": "2025-11-30T21:26:15+00:00",
    "pushed_at": "2025-11-30T21:26:11+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": true,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "No README available",
    "latest_release": {
      "tag": "v1.5.0",
      "name": "v1.5.0",
      "published_at": "2025-11-28T10:29:35+00:00"
    }
  },
  {
    "name": "quantlet-branding-tools",
    "full_name": "Digital-AI-Finance/quantlet-branding-tools",
    "description": "Automated tools for adding QuantLet branding (logo, QR codes, clickable URLs) to LaTeX Beamer presentations",
    "url": "https://github.com/Digital-AI-Finance/quantlet-branding-tools",
    "clone_url": "https://github.com/Digital-AI-Finance/quantlet-branding-tools.git",
    "homepage": "",
    "language": "Python",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 78,
    "default_branch": "master",
    "created_at": "2025-11-28T07:50:54+00:00",
    "updated_at": "2025-11-30T06:50:46+00:00",
    "pushed_at": "2025-11-30T06:50:42+00:00",
    "license": "MIT License",
    "has_issues": true,
    "has_wiki": false,
    "has_pages": false,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "private",
    "contributors_count": 1,
    "readme": "# QuantLet LaTeX Branding Tools\n\nAutomated tools for adding QuantLet branding (logo, QR codes, clickable URLs) to LaTeX Beamer presentations. Add professional GitHub links to all your chart slides with just 3 commands!\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\n**Note:** This is a private repository. For public access, use the GitLab mirror at git.fhgr.ch/digital-finance/quantlet-branding-tools\n\n## üöÄ Quick Start\n\n```bash\n# 1. Clone this repository\ngit clone https://github.com/Digital-AI-Finance/quantlet-branding-tools.git\n\n# 2. Copy to your project\ncp -r quantlet-branding-tools /path/to/your/latex/project/\n\n# 3. Run the 3-step process\npython generate_qr_codes.py\npython add_latex_branding.py\npdflatex your_slides.tex\n```\n\n**Result:** All chart slides now have logo + QR code + clickable URL!\n\n---\n\n## üìñ Documentation\n\nStart here based on your needs:\n\n| Document | Purpose | Best For |\n|----------|---------|----------|\n| **[QUICK_REFERENCE.md](QUICK_REFERENCE.md)** | One-page cheat sheet | Quick lookup, already used these tools |\n| **[BRANDING_GUIDE.md](BRANDING_GUIDE.md)** | Complete step-by-step tutorial | First time using, new project setup |\n| **[TOOLS_REFERENCE.md](TOOLS_REFERENCE.md)** | Detailed technical documentation | Understanding internals, customization |\n\n---\n\n## ‚ú® Features\n\n- **Automatic URL Generation**: Creates GitHub links from chart folder names\n- **LaTeX-Level Branding**: Logo and QR codes added at compile time (not embedded in chart PDFs)\n- **Clickable Elements**: Logo, QR code, and URL text all link to GitHub\n- **Professional Appearance**:\n  - Logo: Fully visible (100% opacity)\n  - QR Code: Slightly transparent (80% opacity)\n  - Bottom-right positioning\n- **Multiple Repository Support**: Sync between main repo and QuantLet mirror\n- **Zero Code Changes**: Works with existing chart PDFs\n\n---\n\n## üìÅ What This Repository Contains\n\n```\nquantlet-branding-tools/\n‚îú‚îÄ‚îÄ add_latex_branding.py          # Main script - adds branding to .tex files\n‚îú‚îÄ‚îÄ remove_duplicate_branding.py   # Remove all branding blocks\n‚îú‚îÄ‚îÄ sync_to_quantlet.py            # Sync to QuantLet repo with updated URLs\n‚îú‚îÄ‚îÄ generate_qr_codes_template.py  # Template for generating QR codes\n‚îú‚îÄ‚îÄ generate_metainfo.py            # Generate metainfo.txt files\n‚îú‚îÄ‚îÄ regenerate_all_charts.py        # Regenerate chart PDFs\n‚îú‚îÄ‚îÄ remove_chart_branding.py        # Remove embedded branding from charts\n‚îú‚îÄ‚îÄ logo/\n‚îÇ   ‚îî‚îÄ‚îÄ quantlet.png               # QuantLet logo (transparent PNG)\n‚îú‚îÄ‚îÄ utils/                          # Legacy embedded branding utilities\n‚îú‚îÄ‚îÄ example/                        # Example project structure\n‚îú‚îÄ‚îÄ README.md                       # This file\n‚îú‚îÄ‚îÄ QUICK_REFERENCE.md             # One-page quick reference\n‚îú‚îÄ‚îÄ BRANDING_GUIDE.md              # Complete tutorial\n‚îî‚îÄ‚îÄ TOOLS_REFERENCE.md             # Technical reference\n```\n\n---\n\n## üéØ Use Cases\n\n### **For Academic Presentations**\nAdd QuantLet branding to all your Beamer slides with charts:\n- Automatic GitHub links for reproducibility\n- QR codes for easy access during talks\n- Professional appearance for conferences\n\n### **For Research Repositories**\nMake your research more discoverable:\n- Link each chart to its source code on GitHub\n- Audience can scan QR codes to access code instantly\n- Maintain two versions (main repo + QuantLet) with different URLs\n\n### **For Teaching Materials**\nHelp students find resources:\n- Each slide links to the code that generated it\n- Students can explore the repository during/after class\n- Easy to update URLs when moving between repositories\n\n---\n\n## üõ†Ô∏è Installation & Setup\n\n### **Requirements**\n\n- Python 3.6+\n- LaTeX packages: `tikz`, `hyperref`, `graphicx`\n- Optional: `qrcode` Python package (for QR code generation)\n\n### **Quick Install**\n\n```bash\n# Install QR code generator (optional but recommended)\npip install qrcode[pil]\n\n# Clone this repository\ngit clone https://github.com/Digital-AI-Finance/quantlet-branding-tools.git\n\n# Copy to your project\ncp -r quantlet-branding-tools/* /path/to/your/project/\n```\n\n---\n\n## üìö Usage\n\n### **Basic Workflow (3 Steps)**\n\n**1. Generate QR Codes**\n```bash\npython generate_qr_codes.py\n```\nThis reads `CHART_METADATA['url']` from each chart's Python script and generates QR codes.\n\n**2. Add Branding to LaTeX**\n```bash\npython add_latex_branding.py\n```\nThis adds tikz overlays with logo + QR + URL to all chart frames in your .tex file.\n\n**3. Compile PDF**\n```bash\npdflatex your_slides.tex\n```\n\nDone! Your slides now have QuantLet branding.\n\n---\n\n### **Project Structure Requirements**\n\nYour LaTeX project should have this structure:\n\n```\nyour_project/\n‚îú‚îÄ‚îÄ 01_chart_name/\n‚îÇ   ‚îú‚îÄ‚îÄ chart_script.py        # Must contain CHART_METADATA\n‚îÇ   ‚îú‚îÄ‚îÄ chart.pdf\n‚îÇ   ‚îî‚îÄ‚îÄ qr_code.png            # Auto-generated by step 1\n‚îú‚îÄ‚îÄ 02_another_chart/\n‚îÇ   ‚îú‚îÄ‚îÄ script.py\n‚îÇ   ‚îú‚îÄ‚îÄ chart.pdf\n‚îÇ   ‚îî‚îÄ‚îÄ qr_code.png\n‚îú‚îÄ‚îÄ slides.tex                  # Your main LaTeX file\n‚îú‚îÄ‚îÄ generate_qr_codes.py        # Copy from generate_qr_codes_template.py\n‚îî‚îÄ‚îÄ add_latex_branding.py       # From this repository\n```\n\n**Each chart script must have:**\n```python\nCHART_METADATA = {\n    'title': 'Chart Title',\n    'url': 'https://github.com/YourOrg/repo/tree/main/01_chart_name'\n}\n```\n\nSee `example/` directory for a complete working example.\n\n---\n\n## üé® Customization\n\nEdit `add_latex_branding.py` to customize:\n\n| Element | Current Value | How to Change |\n|---------|---------------|---------------|\n| Logo size | `width=0.8cm` | Line 100: Change width |\n| Logo opacity | `opacity=1.0` (fully visible) | Line 99: Change opacity (0.0-1.0) |\n| QR size | `width=0.6cm` | Line 104: Change width |\n| QR opacity | `opacity=0.8` (20% transparent) | Line 103: Change opacity |\n| Position | Bottom-right corner | Lines 99, 103, 107: Adjust xshift/yshift |\n| Font size | `\\tiny` | Line 108: Change to `\\scriptsize`, `\\footnotesize`, etc. |\n\n---\n\n## üîÑ Syncing to QuantLet Repository\n\nIf you maintain both a main repository and a QuantLet mirror:\n\n```bash\npython sync_to_quantlet.py\n```\n\nThis script automatically:\n1. Clones/updates the QuantLet repository\n2. Copies all files\n3. Updates all URLs to point to QuantLet\n4. Regenerates QR codes\n5. Prepares for commit\n\nSee [BRANDING_GUIDE.md](BRANDING_GUIDE.md) for details.\n\n---\n\n## üìñ Examples\n\n### **Before**\n```latex\n\\begin{frame}{Loss Landscape}\n  \\includegraphics[width=0.95\\textwidth]{07_loss_landscape/chart.pdf}\n\\end{frame}\n```\n\n### **After** (automatically added)\n```latex\n\\begin{frame}{Loss Landscape}\n  \\includegraphics[width=0.95\\textwidth]{07_loss_landscape/chart.pdf}\n\n  % Quantlet branding (auto-generated)\n  \\begin{tikzpicture}[remember picture,overlay]\n    % Logo (clickable, fully visible)\n    \\node[...] {\\href{https://github.com/...}{\\includegraphics{logo/quantlet.png}}};\n    % QR Code (clickable, slightly transparent)\n    \\node[...] {\\href{https://github.com/...}{\\includegraphics{qr_code.png}}};\n    % URL text (clickable)\n    \\node[...] {\\href{https://github.com/...}{\\texttt{07\\_loss\\_landscape}}};\n  \\end{tikzpicture}\n\\end{frame}\n```\n\n---\n\n## üêõ Troubleshooting\n\n| Issue | Solution |\n|-------|----------|\n| Logo not showing | Check `logo/quantlet.png` exists |\n| QR codes missing | Run `python generate_qr_codes.py` |\n| Duplicate branding | Run `python remove_duplicate_branding.py` then re-add |\n| Wrong repository URLs | Edit CHART_METADATA in chart scripts |\n| Compilation errors | Ensure `tikz` and `hyperref` packages are installed |\n\nSee [BRANDING_GUIDE.md](BRANDING_GUIDE.md) for complete troubleshooting.\n\n---\n\n## ü§ù Contributing\n\nContributions welcome! Please:\n1. Fork the repository\n2. Create a feature branch\n3. Make your changes\n4. Test with example project\n5. Submit a pull request\n\n---\n\n## üìÑ License\n\nMIT License - Free to use and modify\n\n---\n\n## üîó Links\n\n- **GitHub (Private)**: https://github.com/Digital-AI-Finance/quantlet-branding-tools\n  - Visibility: Private (Digital-AI-Finance organization members only)\n  - Primary development repository\n- **GitLab (Private Mirror)**: git.fhgr.ch/digital-finance/quantlet-branding-tools\n  - Visibility: Private (Digital Finance group members only)\n  - Auto-syncs from GitHub\n- **QuantLet**: https://quantlet.de\n- **Documentation**: See BRANDING_GUIDE.md and GITLAB_SETUP.md\n\n---\n\n## üí° Tips\n\n1. **Keep chart PDFs clean**: Don't embed branding in chart images\n2. **Use CHART_METADATA**: Always add URLs to your chart scripts\n3. **Test before presenting**: Verify all links work\n4. **Use version control**: Commit before adding branding (easy to revert)\n5. **Reuse this repo**: Clone once, use in many projects\n\n---\n\n## üìû Support\n\nFor issues or questions:\n- Open an issue on GitHub\n- Check the documentation in `BRANDING_GUIDE.md`\n- See examples in `example/` directory\n\n---\n\n**Made with ‚ù§Ô∏è for the QuantLet community**\n",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "quantlet-branding-tools",
      "research": {
        "title": "Automated tools for adding QuantLet branding (logo, QR codes, clickable URLs) to LaTeX Beamer presentations",
        "abstract": "Automated tools for adding QuantLet branding (logo, QR codes, clickable URLs) to LaTeX Beamer presentations. Add professional GitHub links to all your chart slides with just 3 commands!",
        "keywords": [],
        "authors": [
          {
            "name": "step 1"
          }
        ]
      },
      "publications": [],
      "code": {
        "languages": [
          "Python"
        ],
        "notebooks": [],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": false,
        "has_dockerfile": false,
        "has_environment_yml": false,
        "has_makefile": false,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:03:55.889464",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": [
        {
          "name": "branding_config.json",
          "path": "utils/branding_config.json",
          "format": ".json",
          "size_bytes": 552
        }
      ]
    }
  },
  {
    "name": "demo-repository",
    "full_name": "Digital-AI-Finance/demo-repository",
    "description": "A code repository designed to show the best GitHub has to offer.",
    "url": "https://github.com/Digital-AI-Finance/demo-repository",
    "clone_url": "https://github.com/Digital-AI-Finance/demo-repository.git",
    "homepage": "",
    "language": "HTML",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 2,
    "default_branch": "main",
    "created_at": "2025-11-29T08:24:29+00:00",
    "updated_at": "2025-11-29T08:24:33+00:00",
    "pushed_at": "2025-11-29T08:24:30+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": false,
    "has_pages": false,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "private",
    "contributors_count": 1,
    "readme": "# Welcome to your organization's demo respository\nThis code repository (or \"repo\") is designed to demonstrate the best GitHub has to offer with the least amount of noise.\n\nThe repo includes an `index.html` file (so it can render a web page), two GitHub Actions workflows, and a CSS stylesheet dependency.\n",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "demo-repository",
      "research": {
        "title": "A code repository designed to show the best GitHub has to offer.",
        "abstract": "The repo includes an `index.html` file (so it can render a web page), two GitHub Actions workflows, and a CSS stylesheet dependency.",
        "keywords": [],
        "authors": []
      },
      "publications": [],
      "code": {
        "languages": [
          "HTML"
        ],
        "notebooks": [],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": false,
        "has_dockerfile": false,
        "has_environment_yml": false,
        "has_makefile": false,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:03:58.790783",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": [
        {
          "name": "package.json",
          "path": "package.json",
          "format": ".json",
          "size_bytes": 163
        }
      ]
    }
  },
  {
    "name": "neural-networks-introduction",
    "full_name": "Digital-AI-Finance/neural-networks-introduction",
    "description": "BSc-level neural networks lecture series",
    "url": "https://github.com/Digital-AI-Finance/neural-networks-introduction",
    "clone_url": "https://github.com/Digital-AI-Finance/neural-networks-introduction.git",
    "homepage": "",
    "language": "TeX",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 50024,
    "default_branch": "main",
    "created_at": "2025-11-29T08:35:22+00:00",
    "updated_at": "2025-12-05T19:56:34+00:00",
    "pushed_at": "2025-12-05T19:56:30+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": true,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 2,
    "readme": "No README available",
    "latest_release": null
  },
  {
    "name": "Natural-Language-Processing-Details",
    "full_name": "Digital-AI-Finance/Natural-Language-Processing-Details",
    "description": "Task-oriented NLP educational materials for undergraduate courses",
    "url": "https://github.com/Digital-AI-Finance/Natural-Language-Processing-Details",
    "clone_url": "https://github.com/Digital-AI-Finance/Natural-Language-Processing-Details.git",
    "homepage": "",
    "language": "Jupyter Notebook",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 53389,
    "default_branch": "master",
    "created_at": "2025-11-29T10:52:47+00:00",
    "updated_at": "2025-11-29T11:11:53+00:00",
    "pushed_at": "2025-11-29T11:11:43+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": false,
    "has_pages": false,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "private",
    "contributors_count": 1,
    "readme": "# Natural Language Processing - Educational Materials\n\n**Repository**: https://git.fhgr.ch/digital-finance/Natural-Language-Processing-Details\n**Organization**: Digital Finance @ FHGR\n**Purpose**: Task-oriented NLP course materials from classification to generation\n**GitLab Pages**: https://osterrijoerg.git.fhgr.ch/digital-finance/Natural-Language-Processing-Details\n\n## Quick Start\n\n### Prerequisites\n- Python 3.8+\n- LaTeX distribution (MiKTeX, TeX Live, or MacTeX)\n- Required packages:\n  ```bash\n  pip install pandas numpy matplotlib seaborn scikit-learn sentence-transformers torch jupyter\n  ```\n\n## Task-Based Organization\n\nThis repository is organized by **NLP tasks** (what you want to accomplish) rather than methodologies (how you accomplish it):\n\n```\ntasks/                              # NLP Task Implementations\n‚îú‚îÄ‚îÄ classification/                 # Text Classification\n‚îÇ   ‚îî‚îÄ‚îÄ 6 models: Logistic, Naive Bayes, Decision Tree, Random Forest, SVM, Neural Net\n‚îú‚îÄ‚îÄ text_generation/                # Text Generation\n‚îÇ   ‚îî‚îÄ‚îÄ N-gram & Neural language models\n‚îú‚îÄ‚îÄ clustering/                     # Document Organization\n‚îÇ   ‚îî‚îÄ‚îÄ K-Means, Hierarchical, DBSCAN + PCA, t-SNE, UMAP\n‚îú‚îÄ‚îÄ semantic_search/                # Similarity Search\n‚îÇ   ‚îî‚îÄ‚îÄ Embedding-based semantic search\n‚îú‚îÄ‚îÄ summarization/                  # Text Summarization\n‚îÇ   ‚îî‚îÄ‚îÄ Extractive (TextRank) & Abstractive (BART)\n‚îú‚îÄ‚îÄ sentiment_analysis/             # Sentiment Classification\n‚îÇ   ‚îî‚îÄ‚îÄ 4 models: Logistic, SVM, LSTM, BERT\n‚îî‚îÄ‚îÄ ner/                            # Named Entity Recognition\n    ‚îî‚îÄ‚îÄ BERT-based token classification with BIO tagging\n\nfoundations/                        # Foundational Knowledge\n‚îî‚îÄ‚îÄ neural_networks/                # 7 Architectures (not a task, but building blocks)\n    ‚îú‚îÄ‚îÄ Perceptron ‚Üí Transformer ‚Üí Autoencoder\n    ‚îî‚îÄ‚îÄ notebooks/                  # 4 implementation notebooks\n\nexercises/                          # Practice Exercises\n‚îú‚îÄ‚îÄ basic/                          # 14 notebooks (7 tasks √ó 2)\n‚îî‚îÄ‚îÄ intermediate/                   # 14 notebooks (7 tasks √ó 2)\n\ndata/                               # All Datasets & Artifacts\n‚îú‚îÄ‚îÄ raw/                            # Original CSV datasets\n‚îú‚îÄ‚îÄ processed/                      # Generated embeddings, models, visualizations\n‚îî‚îÄ‚îÄ results/                        # Model outputs and predictions\n```\n\n### Why Task-Based?\n\n**Traditional (Method-Based)**:\n- \"Learn supervised learning\"\n- Abstract, methodology-focused\n- \"I studied embeddings\"\n\n**Task-Based (This Repo)**:\n- \"Build a text classifier\"\n- Concrete, goal-focused\n- \"I built a semantic search system\"\n\nStudents learn **WHAT to solve** (tasks) before **HOW to solve it** (methods).\n\n## The Seven Core NLP Tasks\n\n### 1. Classification (`tasks/classification/`)\n\n**What**: Assign categories to text\n**Example**: \"President announces policy\" ‚Üí Politics\n**Models**: 6 algorithms (85-95% accuracy)\n**Use Cases**: Spam filtering, sentiment analysis, topic categorization\n\n**Quick Start:**\n```bash\ncd tasks/classification\npython train_models.py                    # Train all 6 models (~3 min)\ncd presentation && python generate_charts.py   # Generate 26 charts\npdflatex 20251006_1256_supervised_tutorial.tex # Compile 39-slide presentation\n```\n\n**Output**: Trained models in `data/processed/models/classification/*.pkl`\n\n### 2. Text Generation (`tasks/text_generation/`)\n\n**What**: Generate coherent, human-like text\n**Example**: \"The president\" ‚Üí \"announced a new economic policy\"\n**Models**: 5-gram statistical + LSTM neural\n**Use Cases**: Content creation, chatbots, code completion\n\n**Quick Start:**\n```bash\ncd tasks/text_generation\npython train_5gram.py                     # Train 5-gram model (~30 sec)\npython generate_half_page.py              # Generate 200-word samples\n```\n\n**Output**: Model in `data/processed/models/text_generation/5gram_extended.pkl`\n\n### 3. Clustering (`tasks/clustering/`)\n\n**What**: Group similar documents without labels\n**Example**: Automatically discover that sports headlines cluster together\n**Algorithms**: K-Means, Hierarchical, DBSCAN + PCA, t-SNE, UMAP\n**Use Cases**: Topic discovery, document organization, exploratory analysis\n\n**Quick Start:**\n```bash\ncd tasks/clustering/presentation\npython generate_charts.py                 # Generate 24 charts (~3 min)\npdflatex 20251003_2206_unsupervised_tutorial.tex  # Compile 33-slide presentation\n```\n\n**Input**: Pre-computed embeddings from semantic_search task\n\n### 4. Semantic Search (`tasks/semantic_search/`)\n\n**What**: Find similar documents by meaning (not keywords)\n**Example**: \"president policy\" finds \"leader regulation\" (different words, same meaning)\n**Model**: Sentence-transformers (384-D embeddings)\n**Use Cases**: Document retrieval, Q&A, recommendations, duplicate detection\n\n**Quick Start:**\n```bash\ncd tasks/semantic_search\npython generate_embeddings.py             # Generate 10K embeddings (~2 min)\npython semantic_search.py --interactive   # Interactive search demo\n```\n\n**Output**: `data/processed/embeddings/headlines_embeddings.npy` (15 MB)\n\n### 5. Summarization (`tasks/summarization/`)\n\n**What**: Generate concise summaries of longer texts\n**Example**: 70-word article ‚Üí 7-word headline\n**Models**: TextRank (extractive) + BART (abstractive)\n**Use Cases**: News aggregation, document digests, meeting notes\n\n**Quick Start:**\n```bash\ncd tasks/summarization\npython extractive_summary.py              # TextRank (~2-3 min)\npython train_abstractive.py               # BART fine-tuning (30-60 min GPU)\ncd presentation && python generate_charts.py\npdflatex 20251123_1604_summarization_tutorial.tex\n```\n\n**Output**: Summaries with ROUGE metrics, trained BART model\n\n### 6. Sentiment Analysis (`tasks/sentiment_analysis/`)\n\n**What**: Determine emotional tone (positive, negative, neutral)\n**Example**: \"The president announced a groundbreaking policy\" ‚Üí Positive\n**Models**: Logistic Regression, SVM, LSTM, BERT (4 models)\n**Use Cases**: Brand monitoring, customer feedback, market sentiment\n\n**Quick Start:**\n```bash\ncd tasks/sentiment_analysis\npython create_sentiment_labels.py         # Label 10K headlines\npython train_sentiment_models.py          # Train 4 models (~5 min)\ncd presentation && python generate_all_charts.py\npdflatex 20251128_1150_sentiment_tutorial.tex\n```\n\n**Output**: Trained models, BERT achieves 61.7% accuracy\n\n### 7. Named Entity Recognition (`tasks/ner/`)\n\n**What**: Extract entities (people, places, organizations) from text\n**Example**: \"President Biden announced policy in Washington\" ‚Üí PERSON: Biden, GPE: Washington\n**Model**: BERT fine-tuned for token classification with BIO tagging\n**Use Cases**: Information extraction, knowledge graphs, document indexing\n\n**Quick Start:**\n```bash\ncd tasks/ner\npython annotate_entities.py               # Annotate 10K headlines (~3 min)\npython train_ner_model.py                 # Fine-tune BERT (~15 min GPU)\ncd presentation && python generate_all_charts.py\npdflatex 20251128_1202_ner_tutorial.tex\n```\n\n**Output**: NER model with ~0.92 F1, entity-tagged dataset\n\n## Foundational Knowledge\n\n### Neural Networks (`foundations/neural_networks/`)\n\n**Why \"foundations\"?** Neural networks aren't an NLP task - they're the **building blocks** for solving tasks.\n\n**7 Architectures Covered:**\n1. Perceptron - Linear classification\n2. Feedforward - Universal approximation\n3. CNN - Spatial patterns\n4. RNN - Short-term sequence memory\n5. LSTM - Long-term sequence memory\n6. Transformer - Attention mechanism\n7. Autoencoder - Representation learning\n\n**3 Presentation Variants:**\n- Advanced/Comprehensive (89 slides) - All 7 architectures\n- Feedforward Standard (58 slides) - Iris classification example\n- Feedforward UAT (50 slides) - Universal approximation theorem proof\n\n## Complete Directory Structure\n\n```\nNLP_Data/\n‚îú‚îÄ‚îÄ tasks/                          # MAIN: NLP Task Implementations\n‚îÇ   ‚îú‚îÄ‚îÄ classification/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_models.py\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md               # Comprehensive task guide\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ presentation/           # 39-slide tutorial, 26 charts\n‚îÇ   ‚îú‚îÄ‚îÄ text_generation/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ngram_model.py\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_5gram.py\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ neural_lm.py\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ngram_analysis.ipynb\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md\n‚îÇ   ‚îú‚îÄ‚îÄ clustering/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.md\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ presentation/           # 33-slide tutorial, 24 charts\n‚îÇ   ‚îî‚îÄ‚îÄ semantic_search/\n‚îÇ       ‚îú‚îÄ‚îÄ generate_embeddings.py\n‚îÇ       ‚îú‚îÄ‚îÄ semantic_search.py\n‚îÇ       ‚îú‚îÄ‚îÄ embedding_analysis.ipynb\n‚îÇ       ‚îú‚îÄ‚îÄ README.md\n‚îÇ       ‚îî‚îÄ‚îÄ presentation/           # 28-slide tutorial, 18 charts\n‚îÇ\n‚îú‚îÄ‚îÄ foundations/                    # Foundational Techniques\n‚îÇ   ‚îî‚îÄ‚îÄ neural_networks/\n‚îÇ       ‚îú‚îÄ‚îÄ README.md               # When to use which architecture\n‚îÇ       ‚îî‚îÄ‚îÄ presentation/\n‚îÇ           ‚îú‚îÄ‚îÄ advanced_comprehensive/     # 89 slides\n‚îÇ           ‚îú‚îÄ‚îÄ feedforward_standard/       # 58 slides\n‚îÇ           ‚îî‚îÄ‚îÄ feedforward_uat/            # 50 slides\n‚îÇ\n‚îú‚îÄ‚îÄ data/                           # All Datasets & Artifacts\n‚îÇ   ‚îú‚îÄ‚îÄ raw/                        # Original CSV datasets\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ basic/                  # 400 headlines\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ extended/               # 10,000 headlines + splits\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ articles/               # 1,000 articles + splits\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ large/                  # Experimental\n‚îÇ   ‚îú‚îÄ‚îÄ processed/                  # Generated artifacts\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddings/             # 10,000 √ó 384 vectors (15 MB)\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models/\n‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ classification/     # 6 trained classifiers (15 MB)\n‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ text_generation/    # 5-gram model (902 KB)\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ visualizations/         # PNG preview images\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ samples/                # Generated text\n‚îÇ   ‚îî‚îÄ‚îÄ results/                    # Model outputs\n‚îÇ       ‚îî‚îÄ‚îÄ classification/         # Predictions, metrics\n‚îÇ\n‚îú‚îÄ‚îÄ generators/                     # Dataset Generation\n‚îÇ   ‚îú‚îÄ‚îÄ generate_headlines.py\n‚îÇ   ‚îú‚îÄ‚îÄ generate_extended_headlines.py\n‚îÇ   ‚îú‚îÄ‚îÄ generate_articles.py\n‚îÇ   ‚îî‚îÄ‚îÄ create_splits.py\n‚îÇ\n‚îú‚îÄ‚îÄ docs/                           # Documentation\n‚îÇ   ‚îú‚îÄ‚îÄ DATASET_OVERVIEW.md\n‚îÇ   ‚îú‚îÄ‚îÄ ZIPF_LAW_ANALYSIS.md\n‚îÇ   ‚îú‚îÄ‚îÄ EDUCATIONAL_PRESENTATION_FRAMEWORK.md\n‚îÇ   ‚îî‚îÄ‚îÄ presentations/              # Presentation docs\n‚îÇ\n‚îú‚îÄ‚îÄ wiki_pages/                     # 13 Comprehensive Wiki Pages\n‚îÇ   ‚îú‚îÄ‚îÄ home.md\n‚îÇ   ‚îú‚îÄ‚îÄ Setup-and-Prerequisites.md\n‚îÇ   ‚îú‚îÄ‚îÄ Task-Classification.md      # NEW: 1,748 words\n‚îÇ   ‚îú‚îÄ‚îÄ Task-Text-Generation.md     # NEW: 1,737 words\n‚îÇ   ‚îú‚îÄ‚îÄ Task-Clustering.md          # NEW: 1,985 words\n‚îÇ   ‚îú‚îÄ‚îÄ Task-Semantic-Search.md     # NEW: 1,849 words\n‚îÇ   ‚îú‚îÄ‚îÄ Foundations-Neural-Networks.md  # NEW: 1,840 words\n‚îÇ   ‚îú‚îÄ‚îÄ Dataset-Overview.md         # NEW: 1,448 words\n‚îÇ   ‚îú‚îÄ‚îÄ Repository-Structure.md     # NEW: 1,585 words\n‚îÇ   ‚îú‚îÄ‚îÄ Reproducibility-Guide.md    # NEW: 1,766 words\n‚îÇ   ‚îú‚îÄ‚îÄ FAQ.md                      # NEW: 1,869 words\n‚îÇ   ‚îî‚îÄ‚îÄ Contributing.md             # NEW: 1,708 words\n‚îÇ\n‚îú‚îÄ‚îÄ public_source/                  # GitLab Pages Source (NEW)\n‚îÇ   ‚îú‚îÄ‚îÄ index.html                  # Landing page\n‚îÇ   ‚îú‚îÄ‚îÄ css/style.css               # Purple/gray theme\n‚îÇ   ‚îú‚îÄ‚îÄ slides/index.html           # All presentations\n‚îÇ   ‚îú‚îÄ‚îÄ datasets/index.html         # Interactive browser\n‚îÇ   ‚îî‚îÄ‚îÄ docs/index.html             # Documentation hub\n‚îÇ\n‚îú‚îÄ‚îÄ scripts/                        # GitLab Pages Generation (NEW)\n‚îÇ   ‚îú‚îÄ‚îÄ generate_dataset_samples.py\n‚îÇ   ‚îî‚îÄ‚îÄ generate_pages_index.py\n‚îÇ\n‚îú‚îÄ‚îÄ archive/                        # Historical Versions\n‚îÇ   ‚îú‚îÄ‚îÄ scripts/                    # Old generation scripts\n‚îÇ   ‚îî‚îÄ‚îÄ presentations/              # Old .tex/.pdf versions\n‚îÇ\n‚îú‚îÄ‚îÄ .gitlab-ci.yml                  # GitLab Pages Deployment (NEW)\n‚îú‚îÄ‚îÄ CLAUDE.md                       # Project instructions\n‚îú‚îÄ‚îÄ README.md                       # This file\n‚îî‚îÄ‚îÄ template_beamer_final.tex       # Beamer template\n```\n\n## Educational Progression\n\n### Week 1-2: Basic Text Analysis\n- Dataset: `data/raw/basic/` (400 headlines)\n- Topics: Tokenization, word counts, Zipf's law\n- Exercises: basic/classification_basic.ipynb, basic/text_generation_basic.ipynb\n\n### Week 3-4: Semantic Search Task\n- Learn: How to find similar documents by meaning\n- Code: `tasks/semantic_search/`\n- Presentation: 28 slides, hands-on embedding tutorial\n- Output: Search system that finds semantically similar headlines\n- Exercises: basic/semantic_search_basic.ipynb, intermediate/semantic_search_intermediate.ipynb\n\n### Week 5-6: Classification Task\n- Learn: How to categorize text automatically\n- Code: `tasks/classification/`\n- Presentation: 39 slides, 6 model comparison\n- Output: Trained classifier (85-95% accuracy)\n- Exercises: intermediate/classification_intermediate.ipynb\n\n### Week 7-8: Clustering Task\n- Learn: How to discover topics without labels\n- Code: `tasks/clustering/`\n- Presentation: 33 slides, unsupervised methods\n- Output: Document clusters, 2D visualizations\n- Exercises: basic/clustering_basic.ipynb, intermediate/clustering_intermediate.ipynb\n\n### Week 9-10: Neural Networks Foundations\n- Learn: Building blocks for advanced NLP\n- Code: `foundations/neural_networks/`\n- Presentations: 3 variants (choose based on audience)\n- Notebooks: feedforward_from_scratch, rnn_sentiment, cnn_text, transformer_attention\n- Output: Understanding of 7 architectures\n\n### Week 11-12: Text Generation Task\n- Learn: How to generate coherent text\n- Code: `tasks/text_generation/`\n- Presentation: 50 slides, N-gram to Transformer\n- Output: Working text generator (n-gram + neural)\n- Exercises: basic/text_generation_basic.ipynb, intermediate/text_generation_intermediate.ipynb\n\n### Week 13-14: Summarization Task\n- Learn: How to create concise summaries\n- Code: `tasks/summarization/`\n- Presentation: 45 slides, extractive and abstractive methods\n- Output: TextRank and BART summarizers\n- Exercises: Basic summarization exercises (to be created)\n\n### Week 15-16: Sentiment Analysis Task\n- Learn: How to detect emotional tone\n- Code: `tasks/sentiment_analysis/`\n- Presentation: 35 slides, 4 model comparison\n- Output: Sentiment classifier (BERT achieves 61.7% accuracy)\n- Exercises: basic/sentiment_basic.ipynb, intermediate/sentiment_intermediate.ipynb\n\n### Week 17-18: Named Entity Recognition Task\n- Learn: How to extract structured information\n- Code: `tasks/ner/`\n- Presentation: 40 slides, BIO tagging and BERT-NER\n- Output: Entity extractor (~0.92 F1 score)\n- Exercises: basic/ner_basic.ipynb, intermediate/ner_intermediate.ipynb\n\n## Common Workflows\n\n### I want to... (Quick-Finding Guide)\n\n| I want to... | Go to... |\n|--------------|----------|\n| **Build a text classifier** | `tasks/classification/` |\n| **Generate new text** | `tasks/text_generation/` |\n| **Find similar documents** | `tasks/semantic_search/` |\n| **Cluster documents** | `tasks/clustering/` |\n| **Summarize articles** | `tasks/summarization/` |\n| **Analyze sentiment** | `tasks/sentiment_analysis/` |\n| **Extract entities** | `tasks/ner/` |\n| **Learn neural architectures** | `foundations/neural_networks/` |\n| **Practice with exercises** | `exercises/basic/` or `exercises/intermediate/` |\n| **Understand the datasets** | `docs/DATASET_OVERVIEW.md` |\n| **Regenerate everything** | `wiki_pages/Reproducibility-Guide.md` |\n| **View presentations online** | GitLab Pages (see URL above) |\n\n### Generate All Presentations\n\nEach task has its own presentation:\n\n```bash\n# Classification (39 slides)\ncd tasks/classification/presentation\npython generate_charts.py && pdflatex 20251006_1256_supervised_tutorial.tex\n\n# Clustering (33 slides)\ncd tasks/clustering/presentation\npython generate_charts.py && pdflatex 20251003_2206_unsupervised_tutorial.tex\n\n# Semantic Search (28 slides)\ncd tasks/semantic_search/presentation\npython generate_charts.py && pdflatex 20251003_1430_tsne_tutorial.tex\n\n# Neural Networks (choose variant)\ncd foundations/neural_networks/presentation\npython generate_charts.py && python generate_graphviz_charts.py\ncd advanced_comprehensive\npdflatex 20251024_0452_neural_networks.tex\n```\n\n## GitLab Pages (NEW!)\n\n**Public Website**: https://osterrijoerg.git.fhgr.ch/digital-finance/Natural-Language-Processing-Details\n\n**Features:**\n- All presentation PDFs online\n- Interactive dataset browser (search, filter, sort)\n- Complete documentation\n- No git clone needed for students\n\n**Deploy:**\n```bash\ngit push origin master  # Automatic deployment via .gitlab-ci.yml\n```\n\n## Documentation\n\n### Complete Wiki (13 Pages)\n\nUpload `wiki_pages/*.md` to GitLab Wiki for comprehensive course documentation:\n\n- **Setup & Getting Started** (4 pages)\n- **Task Tutorials** (4 pages) - One per task\n- **Reference** (5 pages) - Datasets, structure, FAQ, contributing\n\n### Technical Documentation\n\n- `CLAUDE.md` - Complete project documentation and workflows\n- Task READMEs - Detailed guide for each task\n- `docs/` - Dataset specs, Zipf analysis, pedagogical framework\n\n## Key Features\n\n- **Task-Oriented**: Learn by doing (classify, generate, search, cluster)\n- **Complete Implementations**: Working code for all tasks\n- **Publication-Quality**: 200+ professional charts across presentations\n- **Fully Reproducible**: All generation scripts included\n- **Interactive Learning**: Jupyter notebooks with hands-on exercises\n- **Professional Slides**: Beamer presentations (Madrid theme, 8pt, purple accent)\n- **Comprehensive Docs**: 13 wiki pages, task READMEs, technical specs\n- **GitLab Pages**: Public website with dataset browser\n\n## Statistics\n\n- **7 Core Tasks** (classification, generation, clustering, search, summarization, sentiment, NER)\n- **1 Foundation** (neural networks with 7 architectures + 4 implementation notebooks)\n- **13 Presentations** (420+ slides total)\n- **10,000 Headlines** (main dataset for most tasks)\n- **1,000 Articles** (summarization dataset)\n- **240+ Charts** (all publication-quality, modular structure)\n- **14 Wiki Pages** (20,000+ words of documentation)\n- **7 Task READMEs** (comprehensive task guides)\n- **28 Exercise Notebooks** (14 basic + 14 intermediate with solutions)\n\n## Contributing\n\nSee `wiki_pages/Contributing.md` for:\n- Adding new tasks\n- Updating presentations\n- Version control policy\n- Quality checklist\n\n## License\n\nEducational materials for FHGR courses. Not for commercial use.\n\n---\n\n**Last Updated**: 2025-11-28\n**Maintainer**: Digital Finance @ FHGR\n**Repository**: https://git.fhgr.ch/digital-finance/Natural-Language-Processing-Details\n**Website**: https://osterrijoerg.git.fhgr.ch/digital-finance/Natural-Language-Processing-Details\n",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "Natural-Language-Processing-Details",
      "research": {
        "title": "Task-oriented NLP educational materials for undergraduate courses",
        "abstract": "**Repository**: https://git.fhgr.ch/digital-finance/Natural-Language-Processing-Details\n**Organization**: Digital Finance @ FHGR\n**Purpose**: Task-oriented NLP course materials from classification to generation\n**GitLab Pages**: https://osterrijoerg.git.fhgr.ch/digital-finance/Natural-Language-Processing-Details",
        "keywords": [],
        "authors": [
          {
            "name": "NLP tasks**  rather than methodologies :",
            "affiliation": "what you want to accomplish"
          },
          {
            "name": "meaning",
            "affiliation": "not keywords"
          },
          {
            "name": "meaning"
          },
          {
            "name": "doing",
            "affiliation": "classify, generate, search, cluster"
          }
        ]
      },
      "publications": [],
      "code": {
        "languages": [
          "Jupyter Notebook"
        ],
        "notebooks": [
          {
            "path": "data/raw/basic/nlp_basics_homework.ipynb",
            "title": "nlp_basics_homework",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "data/raw/basic/nlp_basics_solutions.ipynb",
            "title": "nlp_basics_solutions",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "data/raw/extended/statistical_analysis.ipynb",
            "title": "statistical_analysis",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "data/raw/extended/zipf_law_analysis.ipynb",
            "title": "zipf_law_analysis",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "data/raw/large/zipf_law_analysis.ipynb",
            "title": "zipf_law_analysis",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/basic/classification_basic.ipynb",
            "title": "classification_basic",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/basic/classification_basic_solutions.ipynb",
            "title": "classification_basic_solutions",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/basic/clustering_basic.ipynb",
            "title": "clustering_basic",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/basic/clustering_basic_solutions.ipynb",
            "title": "clustering_basic_solutions",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/basic/ner_basic.ipynb",
            "title": "ner_basic",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/basic/ner_basic_solutions.ipynb",
            "title": "ner_basic_solutions",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/basic/semantic_search_basic.ipynb",
            "title": "semantic_search_basic",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/basic/semantic_search_basic_solutions.ipynb",
            "title": "semantic_search_basic_solutions",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/basic/sentiment_basic.ipynb",
            "title": "sentiment_basic",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/basic/sentiment_basic_solutions.ipynb",
            "title": "sentiment_basic_solutions",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/basic/text_generation_basic.ipynb",
            "title": "text_generation_basic",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/basic/text_generation_basic_solutions.ipynb",
            "title": "text_generation_basic_solutions",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/intermediate/classification_intermediate.ipynb",
            "title": "classification_intermediate",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/intermediate/classification_intermediate_solutions.ipynb",
            "title": "classification_intermediate_solutions",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/intermediate/clustering_intermediate.ipynb",
            "title": "clustering_intermediate",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/intermediate/clustering_intermediate_solutions.ipynb",
            "title": "clustering_intermediate_solutions",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/intermediate/ner_intermediate.ipynb",
            "title": "ner_intermediate",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/intermediate/ner_intermediate_solutions.ipynb",
            "title": "ner_intermediate_solutions",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/intermediate/semantic_search_intermediate.ipynb",
            "title": "semantic_search_intermediate",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/intermediate/semantic_search_intermediate_solutions.ipynb",
            "title": "semantic_search_intermediate_solutions",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/intermediate/sentiment_intermediate.ipynb",
            "title": "sentiment_intermediate",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/intermediate/sentiment_intermediate_solutions.ipynb",
            "title": "sentiment_intermediate_solutions",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/intermediate/text_generation_intermediate.ipynb",
            "title": "text_generation_intermediate",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "exercises/intermediate/text_generation_intermediate_solutions.ipynb",
            "title": "text_generation_intermediate_solutions",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "foundations/neural_networks/notebooks/cnn_text_classification.ipynb",
            "title": "cnn_text_classification",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "foundations/neural_networks/notebooks/feedforward_from_scratch.ipynb",
            "title": "feedforward_from_scratch",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "foundations/neural_networks/notebooks/rnn_sentiment_example.ipynb",
            "title": "rnn_sentiment_example",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "foundations/neural_networks/notebooks/transformer_attention_demo.ipynb",
            "title": "transformer_attention_demo",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "tasks/classification/classification_tutorial.ipynb",
            "title": "classification_tutorial",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "tasks/ner/ner_analysis.ipynb",
            "title": "ner_analysis",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "tasks/semantic_search/embedding_analysis.ipynb",
            "title": "embedding_analysis",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "tasks/sentiment_analysis/sentiment_analysis.ipynb",
            "title": "sentiment_analysis",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "tasks/summarization/summarization_analysis.ipynb",
            "title": "summarization_analysis",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "tasks/text_generation/compare_models.ipynb",
            "title": "compare_models",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "tasks/text_generation/ngram_analysis.ipynb",
            "title": "ngram_analysis",
            "language": "python",
            "type": "jupyter"
          }
        ],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": true,
        "has_dockerfile": false,
        "has_environment_yml": false,
        "has_makefile": false,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:04:25.458271",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": [
        {
          "name": "data",
          "path": "data",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "processed",
          "path": "data/processed",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "embeddings",
          "path": "data/processed/embeddings",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "embeddings_metadata.json",
          "path": "data/processed/embeddings/embeddings_metadata.json",
          "format": ".json",
          "size_bytes": 754450
        },
        {
          "name": "headlines_embeddings.npy",
          "path": "data/processed/embeddings/headlines_embeddings.npy",
          "format": ".npy",
          "size_bytes": 15360128
        },
        {
          "name": "models",
          "path": "data/processed/models",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "classification",
          "path": "data/processed/models/classification",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "sentiment",
          "path": "data/processed/models/sentiment",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "text_generation",
          "path": "data/processed/models/text_generation",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "samples",
          "path": "data/processed/samples",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "sample_20251001_190000_1.txt",
          "path": "data/processed/samples/sample_20251001_190000_1.txt",
          "format": ".txt",
          "size_bytes": 226
        },
        {
          "name": "sample_20251001_190000_2.txt",
          "path": "data/processed/samples/sample_20251001_190000_2.txt",
          "format": ".txt",
          "size_bytes": 231
        },
        {
          "name": "sample_20251001_190000_3.txt",
          "path": "data/processed/samples/sample_20251001_190000_3.txt",
          "format": ".txt",
          "size_bytes": 221
        },
        {
          "name": "sample_20251001_190033_1.txt",
          "path": "data/processed/samples/sample_20251001_190033_1.txt",
          "format": ".txt",
          "size_bytes": 1481
        },
        {
          "name": "sample_20251001_190033_2.txt",
          "path": "data/processed/samples/sample_20251001_190033_2.txt",
          "format": ".txt",
          "size_bytes": 1508
        },
        {
          "name": "sample_20251001_190033_3.txt",
          "path": "data/processed/samples/sample_20251001_190033_3.txt",
          "format": ".txt",
          "size_bytes": 1500
        },
        {
          "name": "visualizations",
          "path": "data/processed/visualizations",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "clustering_comparison.png",
          "path": "data/processed/visualizations/clustering_comparison.png",
          "format": ".png",
          "size_bytes": 863008
        },
        {
          "name": "pca_visualization.png",
          "path": "data/processed/visualizations/pca_visualization.png",
          "format": ".png",
          "size_bytes": 598739
        },
        {
          "name": "similarity_distribution.png",
          "path": "data/processed/visualizations/similarity_distribution.png",
          "format": ".png",
          "size_bytes": 88082
        },
        {
          "name": "tsne_visualization.png",
          "path": "data/processed/visualizations/tsne_visualization.png",
          "format": ".png",
          "size_bytes": 301935
        },
        {
          "name": "raw",
          "path": "data/raw",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "articles",
          "path": "data/raw/articles",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "news_articles_dataset.csv",
          "path": "data/raw/articles/news_articles_dataset.csv",
          "format": ".csv",
          "size_bytes": 558473
        },
        {
          "name": "test.csv",
          "path": "data/raw/articles/test.csv",
          "format": ".csv",
          "size_bytes": 83655
        },
        {
          "name": "train.csv",
          "path": "data/raw/articles/train.csv",
          "format": ".csv",
          "size_bytes": 391050
        },
        {
          "name": "val.csv",
          "path": "data/raw/articles/val.csv",
          "format": ".csv",
          "size_bytes": 83942
        },
        {
          "name": "basic",
          "path": "data/raw/basic",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "VERIFICATION_REPORT.md",
          "path": "data/raw/basic/VERIFICATION_REPORT.md",
          "format": ".md",
          "size_bytes": 4849
        },
        {
          "name": "generate_headlines.py",
          "path": "data/raw/basic/generate_headlines.py",
          "format": ".py",
          "size_bytes": 10878
        },
        {
          "name": "news_headlines_dataset.csv",
          "path": "data/raw/basic/news_headlines_dataset.csv",
          "format": ".csv",
          "size_bytes": 28551
        },
        {
          "name": "nlp_basics_homework.ipynb",
          "path": "data/raw/basic/nlp_basics_homework.ipynb",
          "format": ".ipynb",
          "size_bytes": 15062
        },
        {
          "name": "nlp_basics_solutions.ipynb",
          "path": "data/raw/basic/nlp_basics_solutions.ipynb",
          "format": ".ipynb",
          "size_bytes": 561113
        },
        {
          "name": "extended",
          "path": "data/raw/extended",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "news_headlines_extended.csv",
          "path": "data/raw/extended/news_headlines_extended.csv",
          "format": ".csv",
          "size_bytes": 741412
        },
        {
          "name": "statistical_analysis.ipynb",
          "path": "data/raw/extended/statistical_analysis.ipynb",
          "format": ".ipynb",
          "size_bytes": 584777
        },
        {
          "name": "test.csv",
          "path": "data/raw/extended/test.csv",
          "format": ".csv",
          "size_bytes": 111287
        },
        {
          "name": "train.csv",
          "path": "data/raw/extended/train.csv",
          "format": ".csv",
          "size_bytes": 519243
        },
        {
          "name": "val.csv",
          "path": "data/raw/extended/val.csv",
          "format": ".csv",
          "size_bytes": 110986
        },
        {
          "name": "zipf_law_analysis.ipynb",
          "path": "data/raw/extended/zipf_law_analysis.ipynb",
          "format": ".ipynb",
          "size_bytes": 360467
        },
        {
          "name": "extended_ner",
          "path": "data/raw/extended_ner",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "README.md",
          "path": "data/raw/extended_ner/README.md",
          "format": ".md",
          "size_bytes": 8702
        },
        {
          "name": "extended_sentiment",
          "path": "data/raw/extended_sentiment",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "README.md",
          "path": "data/raw/extended_sentiment/README.md",
          "format": ".md",
          "size_bytes": 6046
        },
        {
          "name": "news_headlines_extended_sentiment.csv",
          "path": "data/raw/extended_sentiment/news_headlines_extended_sentiment.csv",
          "format": ".csv",
          "size_bytes": 878200
        },
        {
          "name": "test.csv",
          "path": "data/raw/extended_sentiment/test.csv",
          "format": ".csv",
          "size_bytes": 131752
        },
        {
          "name": "train.csv",
          "path": "data/raw/extended_sentiment/train.csv",
          "format": ".csv",
          "size_bytes": 614698
        },
        {
          "name": "val.csv",
          "path": "data/raw/extended_sentiment/val.csv",
          "format": ".csv",
          "size_bytes": 131900
        },
        {
          "name": "large",
          "path": "data/raw/large",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "news_headlines_large.csv",
          "path": "data/raw/large/news_headlines_large.csv",
          "format": ".csv",
          "size_bytes": 1867798
        },
        {
          "name": "zipf_law_analysis.ipynb",
          "path": "data/raw/large/zipf_law_analysis.ipynb",
          "format": ".ipynb",
          "size_bytes": 355132
        },
        {
          "name": "results",
          "path": "data/results",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "classification",
          "path": "data/results/classification",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "decision_tree_predictions.json",
          "path": "data/results/classification/decision_tree_predictions.json",
          "format": ".json",
          "size_bytes": 172853
        },
        {
          "name": "detailed_results.json",
          "path": "data/results/classification/detailed_results.json",
          "format": ".json",
          "size_bytes": 3012
        },
        {
          "name": "logistic_regression_predictions.json",
          "path": "data/results/classification/logistic_regression_predictions.json",
          "format": ".json",
          "size_bytes": 172531
        },
        {
          "name": "model_comparison.csv",
          "path": "data/results/classification/model_comparison.csv",
          "format": ".csv",
          "size_bytes": 1013
        },
        {
          "name": "naive_bayes_predictions.json",
          "path": "data/results/classification/naive_bayes_predictions.json",
          "format": ".json",
          "size_bytes": 171904
        },
        {
          "name": "neural_network_predictions.json",
          "path": "data/results/classification/neural_network_predictions.json",
          "format": ".json",
          "size_bytes": 172571
        },
        {
          "name": "random_forest_predictions.json",
          "path": "data/results/classification/random_forest_predictions.json",
          "format": ".json",
          "size_bytes": 172559
        },
        {
          "name": "svm_predictions.json",
          "path": "data/results/classification/svm_predictions.json",
          "format": ".json",
          "size_bytes": 172571
        },
        {
          "name": "sentiment",
          "path": "data/results/sentiment",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "model_comparison.csv",
          "path": "data/results/sentiment/model_comparison.csv",
          "format": ".csv",
          "size_bytes": 475
        },
        {
          "name": "datasets",
          "path": "public_source/datasets",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "index.html",
          "path": "public_source/datasets/index.html",
          "format": ".html",
          "size_bytes": 8496
        },
        {
          "name": "js",
          "path": "public_source/datasets/js",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "dataset-browser.js",
          "path": "public_source/datasets/js/dataset-browser.js",
          "format": ".js",
          "size_bytes": 8129
        }
      ]
    }
  },
  {
    "name": "digital-ai-in-finance",
    "full_name": "Digital-AI-Finance/digital-ai-in-finance",
    "description": "CCG application for AI for Digital Finance workshop - Swiss-MENA Research Network",
    "url": "https://github.com/Digital-AI-Finance/digital-ai-in-finance",
    "clone_url": "https://github.com/Digital-AI-Finance/digital-ai-in-finance.git",
    "homepage": "",
    "language": "HTML",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 20519,
    "default_branch": "main",
    "created_at": "2025-11-29T11:14:34+00:00",
    "updated_at": "2025-12-05T18:28:41+00:00",
    "pushed_at": "2025-12-05T18:28:36+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": true,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "# AI for Digital Finance: Swiss-MENA Research Network\n\n## Overview\n\nCCG (Connect & Collaborate Grant) application materials for the \"AI for Digital Finance: Swiss-MENA Research Network\" workshop.\n\n- **Event**: April 21-23, 2026\n- **Location**: American University of Sharjah, UAE\n- **Co-organizers**: Prof. Joerg Osterrieder (FHGR) & Prof. Stephen Chan (AUS)\n\n## Submitted Materials\n\nThe `FinalSubmission/` folder contains the official submitted documents:\n\n| File | Description |\n|------|-------------|\n| `proposal_ai-in-digital-finance.pdf` | Main CCG application proposal |\n| `workshop_program_2026.pdf` | Detailed 3-day workshop program |\n| `lhmena-ccg-2025-budget_table-ai-for-digital-finance.xlsx` | Budget breakdown |\n| `gannt_chart.pdf` | Project timeline visualization |\n| `osterrieder_cv.pdf` | CV - Prof. Joerg Osterrieder |\n| `cv_chan_stephen.pdf` | CV - Prof. Stephen Chan |\n\n## Budget Summary\n\n- **Total Project Cost**: CHF 24,500\n- **CCG Request**: CHF 5,000 (20.4%)\n- **AUS Co-funding**: CHF 5,000 (20.4%)\n- **Other Sources**: CHF 14,500 (59.2%)\n\n## Workshop Focus\n\n- AI/ML Technologies in Finance\n- Large Language Models (LLMs)\n- Explainable AI for Compliance\n- Blockchain Security and Fraud Detection\n- Digital Banking Innovation\n\n## Documentation\n\nDetailed project documentation is available in the [`docs/`](docs/) folder:\n\n| Document | Description |\n|----------|-------------|\n| [Overview](docs/README.md) | Project overview and quick links |\n| [Budget](docs/Budget.md) | Detailed budget breakdown by category |\n| [Workshop Content](docs/Workshop-Content.md) | Full 3-day program and sessions |\n| [Timeline](docs/Timeline.md) | Project milestones and Gantt summary |\n| [Partners](docs/Partners.md) | Institutional partners and collaborators |\n| [Application Materials](docs/Application-Materials.md) | CCG application summary |\n\n## Archive\n\nThe `archive/` folder contains working drafts, scripts, and intermediate files used during application preparation.\n\n## Contact\n\n- Prof. Joerg Osterrieder: joerg.osterrieder@fhgr.ch\n- Prof. Stephen Chan: schan@aus.edu\n",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "digital-ai-in-finance",
      "research": {
        "title": "CCG application for AI for Digital Finance workshop - Swiss-MENA Research Network",
        "abstract": "CCG (Connect & Collaborate Grant) application materials for the \"AI for Digital Finance: Swiss-MENA Research Network\" workshop.\n\n- Event: April 21-23, 2026\n- Location: American University of Sharjah, UAE\n- Co-organizers: Prof. Joerg Osterrieder (FHGR) & Prof. Stephen Chan (AUS)",
        "keywords": [],
        "authors": [
          {
            "name": "category |"
          }
        ]
      },
      "publications": [],
      "code": {
        "languages": [
          "HTML"
        ],
        "notebooks": [],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": false,
        "has_dockerfile": false,
        "has_environment_yml": false,
        "has_makefile": false,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:04:33.675278",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": [
        {
          "name": "lhmena-ccg-2025-budget_table-ai-for-digital-finance.xlsx",
          "path": "FinalSubmission/lhmena-ccg-2025-budget_table-ai-for-digital-finance.xlsx",
          "format": ".xlsx",
          "size_bytes": 39771
        },
        {
          "name": "20241030_1700_budget_3day_workshop.xlsx",
          "path": "archive/budget/20241030_1700_budget_3day_workshop.xlsx",
          "format": ".xlsx",
          "size_bytes": 7514
        },
        {
          "name": "lhmena-ccg-2025-budget_table AI for Digital Finance.xlsx",
          "path": "archive/budget/lhmena-ccg-2025-budget_table AI for Digital Finance.xlsx",
          "format": ".xlsx",
          "size_bytes": 39771
        },
        {
          "name": "20241031_CCG_Workshop_Gantt_Chart.xlsx",
          "path": "archive/charts/20241031_CCG_Workshop_Gantt_Chart.xlsx",
          "format": ".xlsx",
          "size_bytes": 12431
        },
        {
          "name": "20241031_CCG_Workshop_Gantt_Chart_Updated.xlsx",
          "path": "archive/charts/20241031_CCG_Workshop_Gantt_Chart_Updated.xlsx",
          "format": ".xlsx",
          "size_bytes": 11645
        },
        {
          "name": "gantt_data.xlsx",
          "path": "archive/charts/gantt_data.xlsx",
          "format": ".xlsx",
          "size_bytes": 6775
        },
        {
          "name": "package-lock.json",
          "path": "archive/drafts/package-lock.json",
          "format": ".json",
          "size_bytes": 7861
        },
        {
          "name": "package.json",
          "path": "archive/drafts/package.json",
          "format": ".json",
          "size_bytes": 49
        },
        {
          "name": "data",
          "path": "data",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "msca_bios.json",
          "path": "data/msca_bios.json",
          "format": ".json",
          "size_bytes": 7056
        },
        {
          "name": "msca_members_correct.json",
          "path": "data/msca_members_correct.json",
          "format": ".json",
          "size_bytes": 6154
        },
        {
          "name": "msca_members_merged.json",
          "path": "data/msca_members_merged.json",
          "format": ".json",
          "size_bytes": 6569
        },
        {
          "name": "msca_page_screenshot.png",
          "path": "data/msca_page_screenshot.png",
          "format": ".png",
          "size_bytes": 4796025
        },
        {
          "name": "msca_people.json",
          "path": "data/msca_people.json",
          "format": ".json",
          "size_bytes": 53791
        },
        {
          "name": "msca_people_named.json",
          "path": "data/msca_people_named.json",
          "format": ".json",
          "size_bytes": 4343
        },
        {
          "name": "msca_raw_data.json",
          "path": "data/msca_raw_data.json",
          "format": ".json",
          "size_bytes": 125910
        },
        {
          "name": "scientific_committee.json",
          "path": "data/scientific_committee.json",
          "format": ".json",
          "size_bytes": 574
        },
        {
          "name": "data",
          "path": "docs/data",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "publications.json",
          "path": "docs/data/publications.json",
          "format": ".json",
          "size_bytes": 4272
        }
      ]
    }
  },
  {
    "name": "Publications",
    "full_name": "Digital-AI-Finance/Publications",
    "description": "",
    "url": "https://github.com/Digital-AI-Finance/Publications",
    "clone_url": "https://github.com/Digital-AI-Finance/Publications.git",
    "homepage": "",
    "language": "Unknown",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 10728,
    "default_branch": "main",
    "created_at": "2025-11-30T06:50:55+00:00",
    "updated_at": "2025-11-30T06:51:10+00:00",
    "pushed_at": "2025-11-30T06:51:05+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": false,
    "has_pages": false,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "private",
    "contributors_count": 0,
    "readme": "# Publications Repository\n\nThis repository contains metadata for all publications retrieved from OpenAlex.org.\n\n## Overview\n\n- **Total Publications**: 131\n- **Total Citations**: 1083\n- **Open Access**: 125 (95.4%)\n- **Last Updated**: 2025-11-22 10:53:40\n\n## Data Files\n\n- `publications_latest.json` - Complete publication metadata in JSON format\n\n## Metadata Structure\n\nEach publication entry contains:\n\n- Basic bibliographic information (title, DOI, year, type)\n- Abstract (where available)\n- Citation metrics and trends\n- Open access status and PDF links\n- Author information and institutional affiliations\n- Subject classifications (topics, concepts, keywords)\n- Publication venue details\n- Funding information (where available)\n\n## Usage\n\nThe JSON file can be processed using Python, R, or any JSON-compatible tool for:\n\n- Bibliometric analysis\n- Citation network visualization\n- Collaboration analysis\n- Research trend identification\n- Impact assessment\n\n## Data Source\n\nAll data retrieved from [OpenAlex](https://openalex.org/), an open catalog of scholarly papers, authors, institutions, and venues.\n\n## Citation\n\nIf you use this data in research, please cite OpenAlex:\n\n> Priem, J., Piwowar, H., & Orr, R. (2022). OpenAlex: A fully-open index of scholarly works, authors, venues, institutions, and concepts. ArXiv. https://arxiv.org/abs/2205.01833\n\n## License\n\nPublication metadata is provided under CC0 license by OpenAlex.\n",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "Publications",
      "research": {
        "title": "",
        "abstract": "- Total Publications: 131\n- Total Citations: 1083\n- Open Access: 125 (95.4%)\n- Last Updated: 2025-11-22 10:53:40",
        "keywords": [],
        "authors": [
          {
            "name": "OpenAlex."
          }
        ]
      },
      "publications": [
        {
          "type": "preprint",
          "arxiv_id": "2205.01833",
          "url": "https://arxiv.org/abs/2205.01833",
          "title": "OpenAlex: A fully-open index of scholarly works, authors, venues, institutions, and concepts",
          "abstract": "OpenAlex is a new, fully-open scientific knowledge graph (SKG), launched to replace the discontinued Microsoft Academic Graph (MAG). It contains metadata for 209M works (journal articles, books, etc); 2013M disambiguated authors; 124k venues (places that host works, such as journals and online repositories); 109k institutions; and 65k Wikidata concepts (linked to works via an automated hierarchical multi-tag classifier). The dataset is fully and freely available via a web-based GUI, a full data dump, and high-volume REST API. The resource is under active development and future work will improve accuracy and coverage of citation information and author/institution parsing and deduplication.",
          "published": "2022-05-04T00:57:11Z",
          "updated": "2022-06-17T00:34:23Z",
          "pdf_url": "https://arxiv.org/pdf/2205.01833.pdf",
          "authors": [
            {
              "name": "Jason Priem"
            },
            {
              "name": "Heather Piwowar"
            },
            {
              "name": "Richard Orr"
            }
          ],
          "categories": [
            "cs.DL"
          ],
          "year": 2022
        }
      ],
      "code": {
        "languages": [
          "Unknown"
        ],
        "notebooks": [],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": false,
        "has_dockerfile": false,
        "has_environment_yml": false,
        "has_makefile": false,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:04:35.576853",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": [
        {
          "name": "publications_latest.json",
          "path": "publications_latest.json",
          "format": ".json",
          "size_bytes": 755608
        }
      ]
    }
  },
  {
    "name": "Natural-Language-Processing-Decoding-Strategies",
    "full_name": "Digital-AI-Finance/Natural-Language-Processing-Decoding-Strategies",
    "description": "",
    "url": "https://github.com/Digital-AI-Finance/Natural-Language-Processing-Decoding-Strategies",
    "clone_url": "https://github.com/Digital-AI-Finance/Natural-Language-Processing-Decoding-Strategies.git",
    "homepage": "",
    "language": "TeX",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 29925,
    "default_branch": "main",
    "created_at": "2025-11-30T06:51:07+00:00",
    "updated_at": "2025-11-30T06:51:27+00:00",
    "pushed_at": "2025-11-30T06:51:22+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": false,
    "has_pages": false,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "private",
    "contributors_count": 1,
    "readme": "# Natural Language Processing: Decoding Strategies\n\n**Week 9 Module** - Interactive learning platform for understanding text generation decoding methods in Large Language Models.\n\n## Overview\n\nThis repository contains a complete educational module on decoding strategies, featuring:\n- **2 Interactive React Applications** for hands-on learning\n- **Comprehensive Teaching Materials** (slides, notebooks, charts)\n- **6 Decoding Algorithm Implementations** (greedy, beam, temperature, top-k, nucleus, contrastive)\n- **67 Professional Visualizations** (Python-generated PDF charts)\n\n## Quick Start\n\n### Learning App (Structured Course)\n```bash\ncd learning-app\nnpm install\nnpm run dev\n# Open http://localhost:5180\n```\n\n### Interactive Playground (Experimentation)\n```bash\ncd react-app\nnpm install\nnpm run dev\n# Open http://localhost:5173\n```\n\n## Repository Structure\n\n```\n.\n‚îú‚îÄ‚îÄ learning-app/              # Material-UI learning application\n‚îÇ   ‚îú‚îÄ‚îÄ src/                   # React components (16 files)\n‚îÇ   ‚îú‚îÄ‚îÄ public/figures/        # 67 PDF charts\n‚îÇ   ‚îî‚îÄ‚îÄ README.md              # App-specific documentation\n‚îÇ\n‚îú‚îÄ‚îÄ react-app/                 # Tailwind interactive playground\n‚îÇ   ‚îú‚îÄ‚îÄ src/                   # React components + algorithms (26 files)\n‚îÇ   ‚îú‚îÄ‚îÄ public/figures/        # 67 PDF charts\n‚îÇ   ‚îî‚îÄ‚îÄ README.md              # App-specific documentation\n‚îÇ\n‚îú‚îÄ‚îÄ presentations/             # LaTeX Beamer slides\n‚îÇ   ‚îú‚îÄ‚îÄ *.tex                  # 6 LaTeX source files\n‚îÇ   ‚îî‚îÄ‚îÄ *.pdf                  # 14 compiled presentations\n‚îÇ\n‚îú‚îÄ‚îÄ figures/                   # 67 Python-generated charts\n‚îú‚îÄ‚îÄ python/                    # 19 chart generation scripts\n‚îú‚îÄ‚îÄ lab/                       # Jupyter notebooks (2 notebooks)\n‚îî‚îÄ‚îÄ docs/                      # Additional documentation\n```\n\n## Features\n\n### Learning App\n- **3 Structured Learning Goals** with progress tracking\n- **62 Educational Slides** organized pedagogically\n- **Sidebar Navigation** with visual progress bars\n- **Material-UI Design** with purple theme (#3333B2)\n- **Progress Persistence** via localStorage\n\n### Interactive Playground\n- **Live Algorithm Demonstrations** for all 6 methods\n- **Parameter Tuning** with real-time results\n- **Side-by-Side Comparison** of different methods\n- **Quality Metrics** (repetition rate, distinct-n scores)\n- **Preset Configurations** (Factual, Creative, Balanced)\n\n## Decoding Methods Covered\n\n1. **Greedy Decoding** - Always select highest probability (deterministic)\n2. **Beam Search** - Maintain top-k sequences (deterministic)\n3. **Temperature Sampling** - Reshape distribution for creativity (stochastic)\n4. **Top-k Sampling** - Filter to top-k then sample (stochastic)\n5. **Nucleus (Top-p)** - Adaptive cumulative probability cutoff (stochastic)\n6. **Contrastive Search** - Penalize repetition and similar tokens (deterministic)\n\n## Teaching Materials\n\n### Presentations\n- **Canonical Version**: `20251119_1135_week09_improved_readability.pdf` (66 slides)\n- **Pedagogical Structure**: Extremes ‚Üí Toolbox ‚Üí Problems ‚Üí Integration\n- **LaTeX Source**: Available for customization\n\n### Lab Notebooks\n- **Full Lab**: `week09_decoding_lab.ipynb` (comprehensive exercises)\n- **Simplified**: `week09_decoding_simplified.ipynb` (beginner-friendly)\n- **HTML Export**: For web viewing without Jupyter\n\n### Visualizations\n- **67 Professional Charts** in BSc Discovery color scheme\n- **Python Scripts**: All generation scripts included\n- **Chart Types**: Graphviz diagrams, matplotlib plots, seaborn heatmaps\n\n## Technology Stack\n\n### Learning App\n- React 19 + Vite\n- Material-UI 7\n- Framer Motion (animations)\n- react-pdf (PDF viewing)\n- Recharts (charting)\n\n### Interactive Playground\n- React 18 + Vite\n- Tailwind CSS 4\n- React Router 7\n- D3.js (visualizations)\n- Math.js (calculations)\n\n## Development\n\n### Prerequisites\n- Node.js 18+ and npm\n- Git\n- (Optional) Python 3.8+ for chart generation\n\n### Installation\n```bash\n# Clone repository\ngit clone git@git.fhgr.ch:digital-finance/Natural-Language-Processing-Decoding-Strategies.git\ncd Natural-Language-Processing-Decoding-Strategies\n\n# Install learning-app\ncd learning-app\nnpm install\n\n# Install react-app\ncd ../react-app\nnpm install\n```\n\n### Build for Production\n```bash\n# Learning app\ncd learning-app\nnpm run build\nnpm run preview\n\n# Interactive playground\ncd react-app\nnpm run build\nnpm run preview\n```\n\n## Educational Design\n\nBased on **BSc Discovery Pedagogy**:\n1. **Problem before solution** (slides show extremes first)\n2. **Concrete before abstract** (worked examples with actual numbers)\n3. **Worked examples** (real decoding scenarios)\n4. **Dual-slide pattern** (visual + detail)\n5. **Checkpoint quizzes** (3 quizzes at key points)\n\n## Chart Generation\n\nAll 67 charts are generated using Python scripts:\n```bash\ncd python\npython generate_week09_enhanced_charts.py\n```\n\n## Color Scheme\n\n**BSc Discovery Colors**:\n- Purple (`#3333B2`): Primary brand\n- Dark Gray (`#404040`): Main text\n- Lavender shades: Backgrounds\n- Green (`#2CA02C`): Success\n- Red (`#D62728`): Error\n- Orange (`#FF7F0E`): Warning\n\n## Documentation\n\n- **[learning-app/README.md](learning-app/README.md)** - Learning app features\n- **[react-app/README.md](react-app/README.md)** - Playground technical details\n- **[docs/COMPLETION_REPORT.md](docs/COMPLETION_REPORT.md)** - Development summary\n- **[docs/IMPLEMENTATION_SUMMARY.md](docs/IMPLEMENTATION_SUMMARY.md)** - Technical details\n\n## License\n\nPart of NLP Course 2025 materials. Educational use permitted.\n\n## Course Context\n\nThis is **Week 9** of a 12-week NLP course covering:\n- Week 1-2: Foundations and embeddings\n- Week 3-4: RNN/LSTM and Seq2Seq\n- Week 5-7: Transformers and pre-trained models\n- **Week 9: Decoding Strategies** ‚Üê You are here\n- Week 10-12: Fine-tuning, efficiency, ethics\n\n## Authors\n\n- Course Design: Prof. Joerg Osterrieder\n- Interactive Apps: AI-assisted development\n- Charts: Python matplotlib/seaborn/graphviz\n\n## Support\n\nFor issues or questions:\n- Check the Wiki for setup guides\n- Review app-specific README files\n- Contact course instructor\n\n---\n\n**Last Updated**: November 22, 2025\n**Version**: 1.0.0\n**Status**: Production Ready\n",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "Natural-Language-Processing-Decoding-Strategies",
      "research": {
        "title": "",
        "abstract": "This repository contains a complete educational module on decoding strategies, featuring:\n- 2 Interactive React Applications for hands-on learning\n- Comprehensive Teaching Materials (slides, notebooks, charts)\n- 6 Decoding Algorithm Implementations (greedy, beam, temperature, top-k, nucleus, contrastive)\n- 67 Professional Visualizations (Python-generated PDF charts)",
        "keywords": [],
        "authors": [
          {
            "name": "Course Design: Prof. Joerg Osterrieder"
          },
          {
            "name": "Interactive Apps: AI-assisted development"
          },
          {
            "name": "Charts: Python matplotlib/seaborn/graphviz"
          }
        ]
      },
      "publications": [],
      "code": {
        "languages": [
          "TeX"
        ],
        "notebooks": [
          {
            "path": "lab/week09_decoding_lab.ipynb",
            "title": "week09_decoding_lab",
            "language": "python",
            "type": "jupyter"
          },
          {
            "path": "lab/week09_decoding_simplified.ipynb",
            "title": "week09_decoding_simplified",
            "language": "python",
            "type": "jupyter"
          }
        ],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": false,
        "has_dockerfile": false,
        "has_environment_yml": false,
        "has_makefile": false,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:04:47.153436",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": [
        {
          "name": "package.json",
          "path": "learning-app/package.json",
          "format": ".json",
          "size_bytes": 917
        },
        {
          "name": "data",
          "path": "learning-app/src/data",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "learningGoals.js",
          "path": "learning-app/src/data/learningGoals.js",
          "format": ".js",
          "size_bytes": 3228
        },
        {
          "name": "week09_slides_complete.json",
          "path": "learning-app/src/week09_slides_complete.json",
          "format": ".json",
          "size_bytes": 100295
        },
        {
          "name": "package.json",
          "path": "react-app/package.json",
          "format": ".json",
          "size_bytes": 974
        },
        {
          "name": "data",
          "path": "react-app/src/data",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "extractedSlides.json",
          "path": "react-app/src/data/extractedSlides.json",
          "format": ".json",
          "size_bytes": 66753
        },
        {
          "name": "quantitativeExamples.js",
          "path": "react-app/src/data/quantitativeExamples.js",
          "format": ".js",
          "size_bytes": 7001
        },
        {
          "name": "slideContent.js",
          "path": "react-app/src/data/slideContent.js",
          "format": ".js",
          "size_bytes": 7901
        },
        {
          "name": "week09_slides_complete.json",
          "path": "react-app/src/data/week09_slides_complete.json",
          "format": ".json",
          "size_bytes": 100295
        }
      ]
    }
  },
  {
    "name": "Statistics",
    "full_name": "Digital-AI-Finance/Statistics",
    "description": "",
    "url": "https://github.com/Digital-AI-Finance/Statistics",
    "clone_url": "https://github.com/Digital-AI-Finance/Statistics.git",
    "homepage": "",
    "language": "TeX",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 21196,
    "default_branch": "main",
    "created_at": "2025-11-30T06:51:23+00:00",
    "updated_at": "2025-11-30T06:51:39+00:00",
    "pushed_at": "2025-11-30T06:51:33+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": false,
    "has_pages": false,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "private",
    "contributors_count": 1,
    "readme": "# GitHub Organization Dashboard\n\nAutomated interactive dashboard for monitoring repository statistics in the Digital-AI-Finance organization. Features real-time data visualization with daily updates via GitHub Actions.\n\n## Features\n\n- **Automated Data Collection**: Daily updates via GitHub Actions\n- **Interactive Visualizations**: Powered by Plotly for rich, interactive charts\n- **Comprehensive Metrics**:\n  - Repository activity timeline\n  - Commit patterns and trends\n  - Language distribution analysis\n  - Repository maturity scoring\n  - Research focus areas\n  - Contributor analytics\n- **Modern UI**: Dark theme with GitHub-inspired design\n- **Zero Maintenance**: Fully automated pipeline with scheduled updates\n- **GitHub Pages Deployment**: Accessible via web browser\n\n## Quick Start\n\n### Local Development\n\n1. **Clone the repository**\n   ```powershell\n   git clone https://github.com/Digital-AI-Finance/dashboard.git\n   cd dashboard\n   ```\n\n2. **Install dependencies**\n   ```powershell\n   pip install -r requirements.txt\n   ```\n\n3. **Set up GitHub token** (optional but recommended)\n   ```powershell\n   # Windows PowerShell\n   $env:GITHUB_TOKEN=\"your_github_personal_access_token\"\n\n   # Or create a .env file\n   echo \"GITHUB_TOKEN=your_token_here\" > .env\n   ```\n\n4. **Run the dashboard update**\n   ```powershell\n   python run_update.py\n   ```\n\n5. **View the dashboard**\n   Open `index.html` in your browser\n\n### GitHub Token Setup\n\nFor higher API rate limits (5000 requests/hour vs 60), create a GitHub Personal Access Token:\n\n1. Go to GitHub Settings > Developer settings > Personal access tokens > Tokens (classic)\n2. Click \"Generate new token (classic)\"\n3. Select scopes: `public_repo`, `read:org`\n4. Copy the token\n5. Set as environment variable `GITHUB_TOKEN`\n\n## Configuration\n\nEdit `config.yml` to customize:\n\n```yaml\n# Organization to monitor\norganization: \"Digital-AI-Finance\"\n\n# Dashboard settings\ndashboard:\n  title: \"Your Custom Title\"\n  update_frequency: \"daily\"\n\n# Visualization toggles\ncharts:\n  repository_activity:\n    enabled: true\n    days_history: 90  # Adjust time window\n\n  technology_stack:\n    enabled: true\n\n  research_impact:\n    enabled: true\n\n  repository_maturity:\n    enabled: true\n\n# Color scheme\ncolors:\n  background: \"#0d1117\"\n  primary: \"#58a6ff\"\n  # ... customize colors\n```\n\n## GitHub Actions Automation\n\nThe dashboard automatically updates daily via GitHub Actions:\n\n### Setup\n\n1. **Enable GitHub Pages**\n   - Go to repository Settings > Pages\n   - Source: Deploy from a branch\n   - Branch: `gh-pages` / root\n\n2. **Verify workflow permissions**\n   - Go to Settings > Actions > General\n   - Workflow permissions: Read and write permissions\n   - Save\n\n3. **Manual trigger** (optional)\n   - Go to Actions tab\n   - Select \"Update Dashboard\" workflow\n   - Click \"Run workflow\"\n\n### Schedule\n\n- **Automatic**: Daily at midnight UTC\n- **On push**: When code changes are pushed\n- **Manual**: Via Actions tab\n\n## Project Structure\n\n```\ngithub-dashboard/\n|-- .github/\n|   `-- workflows/\n|       `-- update-dashboard.yml    # GitHub Actions workflow\n|\n|-- data/\n|   |-- repository_stats.json       # Fetched GitHub data (generated)\n|   `-- cache/                      # API response cache (generated)\n|\n|-- fetch_github_data.py            # GitHub API data fetcher\n|-- generate_dashboard.py           # Dashboard HTML generator\n|-- run_update.py                   # Convenience script to run both\n|\n|-- config.yml                      # Configuration file\n|-- requirements.txt                # Python dependencies\n|-- .gitignore                      # Git ignore rules\n|\n|-- index.html                      # Generated dashboard (output)\n`-- README.md                       # This file\n```\n\n## Scripts Overview\n\n### fetch_github_data.py\n\nFetches comprehensive data from GitHub API:\n- Organization metadata\n- Repository details (commits, languages, contributors)\n- Activity metrics\n- Maturity scoring\n- Rate limit management\n\n**Usage:**\n```powershell\npython fetch_github_data.py\n```\n\n### generate_dashboard.py\n\nGenerates interactive HTML dashboard with Plotly charts:\n- Commit timeline\n- Language distribution\n- Repository statistics\n- Maturity gauges\n- Research focus areas\n- Contributor activity\n\n**Usage:**\n```powershell\npython generate_dashboard.py\n```\n\n### run_update.py\n\nRuns complete update pipeline (fetch + generate):\n\n**Usage:**\n```powershell\npython run_update.py\n```\n\n## Visualizations\n\n### 1. Commit Activity Timeline\nLine chart showing commit activity across all repositories over the last 90 days.\n\n### 2. Technology Stack Distribution\nPie chart displaying programming language usage across the organization.\n\n### 3. Repository Statistics\nBar charts comparing commits, stars, and forks across repositories.\n\n### 4. Repository Maturity Scores\nGauge charts showing maturity levels based on:\n- Commit activity (20%)\n- Documentation (20%)\n- Testing infrastructure (20%)\n- License presence (15%)\n- CI/CD setup (15%)\n- Community engagement (10%)\n\n**Maturity Stages:**\n- Production: 80-100%\n- Beta: 60-79%\n- Alpha: 40-59%\n- Planning: 20-39%\n- Concept: 0-19%\n\n### 5. Research Focus Areas\nBar chart of research topics extracted from repository descriptions.\n\n### 6. Contributor Activity\nBar chart showing contributor counts per repository.\n\n## Customization\n\n### Modify Charts\n\nEdit `generate_dashboard.py` to customize visualizations:\n\n```python\ndef create_custom_chart(self, data: Dict) -> str:\n    # Your custom Plotly chart code\n    fig = go.Figure(...)\n    return fig.to_html(full_html=False, include_plotlyjs='cdn')\n```\n\nAdd to `generate_html()` method:\n```python\ncustom_chart = self.create_custom_chart(data)\n```\n\nUpdate HTML template to include:\n```html\n<div class=\"chart-section\">\n    <h2>Custom Chart Title</h2>\n    {{ custom_chart|safe }}\n</div>\n```\n\n### Change Color Scheme\n\nEdit `config.yml` colors section:\n\n```yaml\ncolors:\n  background: \"#your_color\"\n  primary: \"#your_color\"\n  # ... etc\n```\n\n### Adjust Data Collection\n\nModify `fetch_github_data.py`:\n- Change `since_days` parameter for longer/shorter history\n- Add additional API endpoints\n- Customize maturity scoring weights\n\n## Troubleshooting\n\n### Rate Limit Errors\n\n**Problem**: API rate limit exceeded\n\n**Solution**:\n- Set `GITHUB_TOKEN` environment variable\n- Reduce update frequency\n- Enable caching in config.yml\n\n### Missing Data\n\n**Problem**: Charts show no data\n\n**Solution**:\n- Verify `data/repository_stats.json` exists\n- Check GitHub token permissions\n- Run `python fetch_github_data.py` manually to see errors\n\n### GitHub Pages Not Updating\n\n**Problem**: Dashboard not showing latest data\n\n**Solution**:\n- Check Actions tab for workflow errors\n- Verify gh-pages branch exists\n- Ensure workflow has write permissions\n- Clear browser cache\n\n### Import Errors\n\n**Problem**: Module not found errors\n\n**Solution**:\n```powershell\npip install -r requirements.txt --upgrade\n```\n\n## Dependencies\n\n- **requests** (2.31.0+): GitHub API communication\n- **plotly** (5.18.0+): Interactive visualizations\n- **pandas** (2.1.0+): Data processing\n- **pyyaml** (6.0.1+): Configuration parsing\n- **jinja2** (3.1.2+): HTML templating\n- **python-dateutil** (2.8.2+): Date handling\n\n## Development\n\n### Adding New Metrics\n\n1. **Fetch data** in `fetch_github_data.py`:\n   ```python\n   def get_new_metric(self, repo_name: str) -> Dict:\n       url = f\"{self.base_url}/repos/{self.org_name}/{repo_name}/endpoint\"\n       return self._make_request(url)\n   ```\n\n2. **Create visualization** in `generate_dashboard.py`:\n   ```python\n   def create_new_metric_chart(self, data: Dict) -> str:\n       # Chart logic\n       return fig.to_html(full_html=False, include_plotlyjs='cdn')\n   ```\n\n3. **Add to dashboard** in HTML template\n\n### Testing Locally\n\n```powershell\n# Test data fetching\npython fetch_github_data.py\n\n# Test dashboard generation\npython generate_dashboard.py\n\n# Open in browser\nstart index.html\n```\n\n## Performance\n\n- **API Calls**: Approximately 10-20 per repository\n- **Rate Limits**: 5000/hour with token, 60/hour without\n- **Generation Time**: 1-3 minutes for 4 repositories\n- **Dashboard Size**: Approximately 2-5 MB HTML file\n\n## Security\n\n- **GitHub Token**: Never commit tokens to repository\n- **Use secrets**: Store in GitHub Secrets for Actions\n- **Environment variables**: Use `.env` file for local development (git-ignored)\n\n## License\n\nMIT License - see LICENSE file for details\n\n## Contributing\n\nContributions welcome! Please:\n1. Fork the repository\n2. Create feature branch\n3. Make changes\n4. Test thoroughly\n5. Submit pull request\n\n## Support\n\nFor issues or questions:\n- Open an issue on GitHub\n- Check existing issues for solutions\n- Review GitHub Actions logs for errors\n\n## Roadmap\n\nFuture enhancements:\n- [ ] Historical trend tracking\n- [ ] Comparison with other organizations\n- [ ] Email notifications on significant changes\n- [ ] Custom metric definitions\n- [ ] Export to PDF/PNG\n- [ ] Mobile app view\n- [ ] Real-time updates via webhooks\n\n## Acknowledgments\n\n- Built with Plotly for interactive visualizations\n- Deployed via GitHub Pages\n- Automated with GitHub Actions\n- Inspired by modern tech dashboard designs\n",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "Statistics",
      "research": {
        "title": "",
        "abstract": "Automated interactive dashboard for monitoring repository statistics in the Digital-AI-Finance organization. Features real-time data visualization with daily updates via GitHub Actions.",
        "keywords": [],
        "authors": [
          {
            "name": "Plotly for rich, interactive charts"
          },
          {
            "name": "modern tech dashboard designs"
          }
        ]
      },
      "publications": [],
      "code": {
        "languages": [
          "TeX"
        ],
        "notebooks": [],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": true,
        "has_dockerfile": false,
        "has_environment_yml": false,
        "has_makefile": false,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:04:56.581729",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": [
        {
          "name": "data",
          "path": "data",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": ".gitkeep",
          "path": "data/.gitkeep",
          "format": "",
          "size_bytes": 104
        },
        {
          "name": "datasets",
          "path": "lesson2_hypothesis/datasets",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "README.md",
          "path": "lesson2_hypothesis/datasets/README.md",
          "format": ".md",
          "size_bytes": 3679
        },
        {
          "name": "agricultural_experiment.csv",
          "path": "lesson2_hypothesis/datasets/agricultural_experiment.csv",
          "format": ".csv",
          "size_bytes": 3898
        },
        {
          "name": "clinical_trial.csv",
          "path": "lesson2_hypothesis/datasets/clinical_trial.csv",
          "format": ".csv",
          "size_bytes": 2233
        },
        {
          "name": "drug_dosage_study.csv",
          "path": "lesson2_hypothesis/datasets/drug_dosage_study.csv",
          "format": ".csv",
          "size_bytes": 3502
        },
        {
          "name": "education_intervention.csv",
          "path": "lesson2_hypothesis/datasets/education_intervention.csv",
          "format": ".csv",
          "size_bytes": 616
        },
        {
          "name": "employee_satisfaction.csv",
          "path": "lesson2_hypothesis/datasets/employee_satisfaction.csv",
          "format": ".csv",
          "size_bytes": 3491
        },
        {
          "name": "environmental_study.csv",
          "path": "lesson2_hypothesis/datasets/environmental_study.csv",
          "format": ".csv",
          "size_bytes": 6517
        },
        {
          "name": "manufacturing_quality.csv",
          "path": "lesson2_hypothesis/datasets/manufacturing_quality.csv",
          "format": ".csv",
          "size_bytes": 659
        },
        {
          "name": "marketing_campaigns.csv",
          "path": "lesson2_hypothesis/datasets/marketing_campaigns.csv",
          "format": ".csv",
          "size_bytes": 4123
        },
        {
          "name": "medical_treatment.csv",
          "path": "lesson2_hypothesis/datasets/medical_treatment.csv",
          "format": ".csv",
          "size_bytes": 2376
        },
        {
          "name": "reaction_time_study.csv",
          "path": "lesson2_hypothesis/datasets/reaction_time_study.csv",
          "format": ".csv",
          "size_bytes": 3059
        },
        {
          "name": "website_testing.csv",
          "path": "lesson2_hypothesis/datasets/website_testing.csv",
          "format": ".csv",
          "size_bytes": 73631
        }
      ]
    }
  },
  {
    "name": "Anomaly_and_Fraud_Detection_in_Blockchain_Networks",
    "full_name": "Digital-AI-Finance/Anomaly_and_Fraud_Detection_in_Blockchain_Networks",
    "description": "Anomaly and Fraud Detection in Blockchain Networks - Research by Stephen Chan (PI) and Joerg Osterrieder (Co-PI)",
    "url": "https://github.com/Digital-AI-Finance/Anomaly_and_Fraud_Detection_in_Blockchain_Networks",
    "clone_url": "https://github.com/Digital-AI-Finance/Anomaly_and_Fraud_Detection_in_Blockchain_Networks.git",
    "homepage": "",
    "language": "HTML",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 4331,
    "default_branch": "main",
    "created_at": "2025-11-30T16:24:34+00:00",
    "updated_at": "2025-12-04T11:20:51+00:00",
    "pushed_at": "2025-12-04T11:20:47+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": false,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "No README available",
    "latest_release": null
  },
  {
    "name": "book-next-word-prediction",
    "full_name": "Digital-AI-Finance/book-next-word-prediction",
    "description": "PhD-level Springer textbook: Predicting the Next Word - A Mathematical Foundation of Language Models",
    "url": "https://github.com/Digital-AI-Finance/book-next-word-prediction",
    "clone_url": "https://github.com/Digital-AI-Finance/book-next-word-prediction.git",
    "homepage": "",
    "language": "HTML",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 3,
    "size": 96699,
    "default_branch": "main",
    "created_at": "2025-12-01T09:14:36+00:00",
    "updated_at": "2025-12-05T20:58:14+00:00",
    "pushed_at": "2025-12-05T20:58:09+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": false,
    "has_pages": false,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "private",
    "contributors_count": 2,
    "readme": "# book-next-word-prediction\n\nSpringer textbook: Predicting the Next Word - A Mathematical Foundation of Language Models\n\n## Getting started\n\nTo make it easy for you to get started with GitLab, here's a list of recommended next steps.\n\nAlready a pro? Just edit this README.md and make it your own. Want to make it easy? [Use the template at the bottom](#editing-this-readme)!\n\n## Add your files\n\n- [ ] [Create](https://docs.gitlab.com/ee/user/project/repository/web_editor.html#create-a-file) or [upload](https://docs.gitlab.com/ee/user/project/repository/web_editor.html#upload-a-file) files\n- [ ] [Add files using the command line](https://docs.gitlab.com/topics/git/add_files/#add-files-to-a-git-repository) or push an existing Git repository with the following command:\n\n```\ncd existing_repo\ngit remote add origin https://git.fhgr.ch/digital-finance/book-next-word-prediction.git\ngit branch -M main\ngit push -uf origin main\n```\n\n## Integrate with your tools\n\n- [ ] [Set up project integrations](http://git.fhgr.ch/digital-finance/book-next-word-prediction/-/settings/integrations)\n\n## Collaborate with your team\n\n- [ ] [Invite team members and collaborators](https://docs.gitlab.com/ee/user/project/members/)\n- [ ] [Create a new merge request](https://docs.gitlab.com/ee/user/project/merge_requests/creating_merge_requests.html)\n- [ ] [Automatically close issues from merge requests](https://docs.gitlab.com/ee/user/project/issues/managing_issues.html#closing-issues-automatically)\n- [ ] [Enable merge request approvals](https://docs.gitlab.com/ee/user/project/merge_requests/approvals/)\n- [ ] [Set auto-merge](https://docs.gitlab.com/user/project/merge_requests/auto_merge/)\n\n## Test and Deploy\n\nUse the built-in continuous integration in GitLab.\n\n- [ ] [Get started with GitLab CI/CD](https://docs.gitlab.com/ee/ci/quick_start/)\n- [ ] [Analyze your code for known vulnerabilities with Static Application Security Testing (SAST)](https://docs.gitlab.com/ee/user/application_security/sast/)\n- [ ] [Deploy to Kubernetes, Amazon EC2, or Amazon ECS using Auto Deploy](https://docs.gitlab.com/ee/topics/autodevops/requirements.html)\n- [ ] [Use pull-based deployments for improved Kubernetes management](https://docs.gitlab.com/ee/user/clusters/agent/)\n- [ ] [Set up protected environments](https://docs.gitlab.com/ee/ci/environments/protected_environments.html)\n\n***\n\n# Editing this README\n\nWhen you're ready to make this README your own, just edit this file and use the handy template below (or feel free to structure it however you want - this is just a starting point!). Thanks to [makeareadme.com](https://www.makeareadme.com/) for this template.\n\n## Suggestions for a good README\n\nEvery project is different, so consider which of these sections apply to yours. The sections used in the template are suggestions for most open source projects. Also keep in mind that while a README can be too long and detailed, too long is better than too short. If you think your README is too long, consider utilizing another form of documentation rather than cutting out information.\n\n## Name\nChoose a self-explaining name for your project.\n\n## Description\nLet people know what your project can do specifically. Provide context and add a link to any reference visitors might be unfamiliar with. A list of Features or a Background subsection can also be added here. If there are alternatives to your project, this is a good place to list differentiating factors.\n\n## Badges\nOn some READMEs, you may see small images that convey metadata, such as whether or not all the tests are passing for the project. You can use Shields to add some to your README. Many services also have instructions for adding a badge.\n\n## Visuals\nDepending on what you are making, it can be a good idea to include screenshots or even a video (you'll frequently see GIFs rather than actual videos). Tools like ttygif can help, but check out Asciinema for a more sophisticated method.\n\n## Installation\nWithin a particular ecosystem, there may be a common way of installing things, such as using Yarn, NuGet, or Homebrew. However, consider the possibility that whoever is reading your README is a novice and would like more guidance. Listing specific steps helps remove ambiguity and gets people to using your project as quickly as possible. If it only runs in a specific context like a particular programming language version or operating system or has dependencies that have to be installed manually, also add a Requirements subsection.\n\n## Usage\nUse examples liberally, and show the expected output if you can. It's helpful to have inline the smallest example of usage that you can demonstrate, while providing links to more sophisticated examples if they are too long to reasonably include in the README.\n\n## Support\nTell people where they can go to for help. It can be any combination of an issue tracker, a chat room, an email address, etc.\n\n## Roadmap\nIf you have ideas for releases in the future, it is a good idea to list them in the README.\n\n## Contributing\nState if you are open to contributions and what your requirements are for accepting them.\n\nFor people who want to make changes to your project, it's helpful to have some documentation on how to get started. Perhaps there is a script that they should run or some environment variables that they need to set. Make these steps explicit. These instructions could also be useful to your future self.\n\nYou can also document commands to lint the code or run tests. These steps help to ensure high code quality and reduce the likelihood that the changes inadvertently break something. Having instructions for running tests is especially helpful if it requires external setup, such as starting a Selenium server for testing in a browser.\n\n## Authors and acknowledgment\nShow your appreciation to those who have contributed to the project.\n\n## License\nFor open source projects, say how it is licensed.\n\n## Project status\nIf you have run out of energy or time for your project, put a note at the top of the README saying that development has slowed down or stopped completely. Someone may choose to fork your project or volunteer to step in as a maintainer or owner, allowing your project to keep going. You can also make an explicit request for maintainers.\n",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "book-next-word-prediction",
      "research": {
        "title": "PhD-level Springer textbook: Predicting the Next Word - A Mathematical Foundation of Language Models",
        "abstract": "Already a pro? Just edit this README.md and make it your own. Want to make it easy? [Use the template at the bottom](#editing-this-readme)!",
        "keywords": [],
        "authors": []
      },
      "publications": [],
      "code": {
        "languages": [
          "HTML"
        ],
        "notebooks": [],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": false,
        "has_dockerfile": false,
        "has_environment_yml": false,
        "has_makefile": false,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:05:07.091618",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": []
    }
  },
  {
    "name": "Narrative-Digital-Finance",
    "full_name": "Digital-AI-Finance/Narrative-Digital-Finance",
    "description": "SNSF Narrative Digital Finance: A tale of structural breaks, bubbles & market narratives - Research Project of the Swiss National Science Foundation",
    "url": "https://github.com/Digital-AI-Finance/Narrative-Digital-Finance",
    "clone_url": "https://github.com/Digital-AI-Finance/Narrative-Digital-Finance.git",
    "homepage": "",
    "language": "HTML",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 112,
    "default_branch": "main",
    "created_at": "2025-12-01T10:18:51+00:00",
    "updated_at": "2025-12-04T07:39:54+00:00",
    "pushed_at": "2025-12-04T07:39:51+00:00",
    "license": "MIT License",
    "has_issues": true,
    "has_wiki": true,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "# Narrative Digital Finance\n\n**A tale of structural breaks, bubbles & market narratives**\n\nResearch Project of the Swiss National Science Foundation (SNSF)\n\n[![SNSF](https://img.shields.io/badge/Funded%20by-SNSF-red)](https://www.mysnf.ch/grants/grant.aspx?id=c8d8081e-6eee-4418-92bb-21dc2c89566a)\n[![EU Horizon](https://img.shields.io/badge/EU-Horizon%20Europe-blue)](https://cordis.europa.eu/project/id/101119635)\n\n## Overview\n\nLarge fluctuations, instabilities, trends and uncertainty of financial markets constitute a substantial challenge for asset management companies, pension funds and regulators. This project develops a comprehensive framework that utilizes advanced machine learning and NLP techniques to predict market outcomes, detect asset price bubbles, and identify structural breaks using diverse data sources.\n\n## Research Objectives\n\n### Overall Objectives\nDevelop a comprehensive framework utilizing advanced machine learning and NLP techniques to:\n- Predict market outcomes\n- Detect asset price bubbles\n- Identify structural breaks\n- Analyze diverse data sources including financial data and narrative content from text, speech, and multimedia\n\n### Specific Aims\n1. Validate and refine existing econometric models using real-world financial data\n2. Integrate narrative analysis to understand and predict market behaviors and asset price dynamics\n3. Create a multidimensional AI and ML framework that enhances the detection of market anomalies and forecasts financial trends\n\n## Methods\n\nThe approach involves:\n- Collecting and processing stock prices, macroeconomic indicators, and textual content from the web\n- Employing text mining and NLP techniques to analyze sentiment, narrative structures, and their impact on market movements\n- Developing and testing new AI models that combine traditional financial analysis with narrative insights\n\n## Team\n\n**Cooperation between University of Twente (Netherlands) and Bern Business School (Switzerland)**\n\n| Name | Role | Affiliation |\n|------|------|-------------|\n| **Joerg Osterrieder** | Principal Investigator | Bern Business School, University of Twente |\n| **Marius Jan Klein** | Team Member | Bern Business School |\n| **Branka Hadji Misheva** | Team Member | Bern Business School |\n| **Gabin Taibi** | Team Member | Bern Business School, University of Twente |\n\n## Collaborations\n\n- **Deutsche Borse** - Dr. Stefan Schlamp (Head of Quantitative Analytics)\n- **COST Action CA19130** - Fintech and Artificial Intelligence in Finance\n- **MSCA Industrial Doctoral Network on Digital Finance**\n\n## Publications\n\n1. **Hypothesizing Multimodal Influence: Assessing the Impact of Textual and Non-Textual Data on Financial Instrument Pricing Using NLP and Generative AI**\n   - Bolesta, K., Taibi, G., Codruta, M., Osterrieder, J., Hadji-Misheva, B. & Hopp, C. (2024). SSRN.\n\n## Project Output\n\n| Category | Count |\n|----------|-------|\n| Scientific Publications | 1 |\n| Datasets | 3 |\n| Collaborations | 1 |\n\n## Funding\n\n- **Swiss National Science Foundation (SNSF)**\n  - Proposal Number: 100018E_213370\n- **Horizon Europe - Marie Sklodowska-Curie Actions**\n  - Grant Agreement No. 101119635\n\n## Repository Structure\n\n```\nNarrative-Digital-Finance/\n‚îú‚îÄ‚îÄ README.md           # This file\n‚îú‚îÄ‚îÄ docs/               # Documentation\n‚îÇ   ‚îú‚îÄ‚îÄ objectives.md   # Research objectives\n‚îÇ   ‚îú‚îÄ‚îÄ team.md         # Team information\n‚îÇ   ‚îî‚îÄ‚îÄ funding.md      # Funding details\n‚îú‚îÄ‚îÄ src/                # Source code\n‚îú‚îÄ‚îÄ data/               # Data files\n‚îî‚îÄ‚îÄ LICENSE             # License file\n```\n\n## Links\n\n- [Project Website](https://www.digital-finance-msca.com/snsf-narrative-digital-finance)\n- [SNSF Grant Page](https://www.mysnf.ch/grants/grant.aspx?id=c8d8081e-6eee-4418-92bb-21dc2c89566a)\n- [EU CORDIS](https://cordis.europa.eu/project/id/101119635)\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.\n\n---\n\n*Funded by the European Union. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or Horizon Europe: Marie Sklodowska-Curie Actions.*\n",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "Narrative-Digital-Finance",
      "research": {
        "title": "SNSF Narrative Digital Finance: A tale of structural breaks, bubbles & market narratives - Research Project of the Swiss National Science Foundation",
        "abstract": "Large fluctuations, instabilities, trends and uncertainty of financial markets constitute a substantial challenge for asset management companies, pension funds and regulators. This project develops a comprehensive framework that utilizes advanced machine learning and NLP techniques to predict market outcomes, detect asset price bubbles, and identify structural breaks using diverse data sources.",
        "keywords": [],
        "authors": [
          {
            "name": "Cooperation between University of Twente  and Bern Business School",
            "affiliation": "Netherlands"
          },
          {
            "name": "| Name | Role | Affiliation |"
          },
          {
            "name": "|------|------|-------------|"
          },
          {
            "name": "| **Joerg Osterrieder** | Principal Investigator | Bern Business School, University of Twente |"
          },
          {
            "name": "| **Marius Jan Klein** | Team Member | Bern Business School |"
          },
          {
            "name": "| **Branka Hadji Misheva** | Team Member | Bern Business School |"
          },
          {
            "name": "| **Gabin Taibi** | Team Member | Bern Business School, University of Twente |"
          },
          {
            "name": "the European Union. Views and opinions expressed are however those of the author only and do not necessarily reflect those of the European Union or Horizon Europe: Marie Sklodowska-Curie Actions.",
            "affiliation": "s"
          }
        ]
      },
      "publications": [],
      "code": {
        "languages": [
          "HTML"
        ],
        "notebooks": [],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": false,
        "has_dockerfile": false,
        "has_environment_yml": false,
        "has_makefile": false,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:05:10.991098",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": [
        {
          "name": "data",
          "path": "data",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": ".gitkeep",
          "path": "data/.gitkeep",
          "format": "",
          "size_bytes": 17
        },
        {
          "name": "authors.json",
          "path": "data/authors.json",
          "format": ".json",
          "size_bytes": 3283
        },
        {
          "name": "publications.json",
          "path": "data/publications.json",
          "format": ".json",
          "size_bytes": 123715
        },
        {
          "name": "summary.json",
          "path": "data/summary.json",
          "format": ".json",
          "size_bytes": 415
        }
      ]
    }
  },
  {
    "name": "network-based-credit-risk-models",
    "full_name": "Digital-AI-Finance/network-based-credit-risk-models",
    "description": "Network-Based Credit Risk Models in P2P Lending Markets - SNSF Research Project",
    "url": "https://github.com/Digital-AI-Finance/network-based-credit-risk-models",
    "clone_url": "https://github.com/Digital-AI-Finance/network-based-credit-risk-models.git",
    "homepage": "",
    "language": "Python",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 11632,
    "default_branch": "main",
    "created_at": "2025-12-01T10:21:04+00:00",
    "updated_at": "2025-12-02T16:29:21+00:00",
    "pushed_at": "2025-12-02T16:29:16+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": true,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 3,
    "readme": "# Network-Based Credit Risk Models in P2P Lending Markets\n\n**SNSF Research Project** - Swiss National Science Foundation\n\n## Overview\n\nThis research project focuses on developing advanced, interpretable credit risk models tailored specifically to the needs of Peer-to-Peer (P2P) lending markets. The project addresses the unique challenges of P2P lending, such as higher information asymmetry, less regulation, and increased risk during economic downturns.\n\n## Team\n\n**Principal Investigator:** Joerg Osterrieder (Bern Business School, Switzerland / University of Twente, Netherlands)\n\n**Team Members:**\n- Lennart John Baals (Bern Business School / University of Twente)\n- Branka Hadji Misheva (Bern Business School)\n- Yiting Liu (Bern Business School / University of Twente)\n\n## Key Publications\n\n1. **Network centrality and credit risk: A comprehensive analysis of peer-to-peer lending dynamics**\n   Liu, Y., Baals, L. J., Osterrieder, J., & Hadji-Misheva, B. (2024). *Finance Research Letters*, 63, 105308.\n\n2. **Leveraging network topology for credit risk assessment in P2P lending: A comparative study under the lens of machine learning**\n   Liu, Y., Baals, L. J., Osterrieder, J., & Hadji-Misheva, B. (2024). *Expert Systems with Applications*, 252, 124100.\n\n## Collaborations\n\n- American University of Sharjah, UAE (Prof. Dr. Stephen Chan)\n- University of Manchester, UK (Dr. Yuanyuan Zhang)\n- Renmin University, China (Prof. Dr. Jeffrey Chu)\n- COST Action CA19130 - Fintech and Artificial Intelligence in Finance\n- MSCA Industrial Doctoral Network on Digital Finance\n\n## More Information\n\nSee the [Wiki](https://github.com/Digital-AI-Finance/network-based-credit-risk-models/wiki) for complete project information.\n\n**Source:** [digital-finance-msca.com](https://www.digital-finance-msca.com/network-based-credit-risk-models-snsf)\n",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "network-based-credit-risk-models",
      "research": {
        "title": "Network-Based Credit Risk Models in P2P Lending Markets - SNSF Research Project",
        "abstract": "This research project focuses on developing advanced, interpretable credit risk models tailored specifically to the needs of Peer-to-Peer (P2P) lending markets. The project addresses the unique challenges of P2P lending, such as higher information asymmetry, less regulation, and increased risk during economic downturns.",
        "keywords": [],
        "authors": [
          {
            "name": "Principal Investigator:** Joerg Osterrieder",
            "affiliation": "Bern Business School, Switzerland / University of Twente, Netherlands"
          },
          {
            "name": "Team Members:"
          },
          {
            "name": "Lennart John Baals",
            "affiliation": "Bern Business School / University of Twente"
          },
          {
            "name": "Branka Hadji Misheva",
            "affiliation": "Bern Business School"
          },
          {
            "name": "Yiting Liu",
            "affiliation": "Bern Business School / University of Twente"
          }
        ]
      },
      "publications": [],
      "code": {
        "languages": [
          "Python"
        ],
        "notebooks": [],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": false,
        "has_dockerfile": false,
        "has_environment_yml": false,
        "has_makefile": false,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:05:17.509824",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": [
        {
          "name": "news.json",
          "path": "_data/news.json",
          "format": ".json",
          "size_bytes": 1219
        },
        {
          "name": "publications.json",
          "path": "_data/publications.json",
          "format": ".json",
          "size_bytes": 30608
        },
        {
          "name": "snsf_project.json",
          "path": "_data/snsf_project.json",
          "format": ".json",
          "size_bytes": 7338
        },
        {
          "name": "snsf_project_complete.json",
          "path": "_data/snsf_project_complete.json",
          "format": ".json",
          "size_bytes": 17071
        },
        {
          "name": "team.json",
          "path": "_data/team.json",
          "format": ".json",
          "size_bytes": 2404
        }
      ]
    }
  },
  {
    "name": "msca-digital-finance",
    "full_name": "Digital-AI-Finance/msca-digital-finance",
    "description": "MSCA Digital Finance Website - Hugo static site migrated from Wix",
    "url": "https://github.com/Digital-AI-Finance/msca-digital-finance",
    "clone_url": "https://github.com/Digital-AI-Finance/msca-digital-finance.git",
    "homepage": "",
    "language": "HTML",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 302949,
    "default_branch": "main",
    "created_at": "2025-12-01T21:42:05+00:00",
    "updated_at": "2025-12-04T11:36:53+00:00",
    "pushed_at": "2025-12-04T11:36:49+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": true,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "# MSCA Digital Finance Website\n\nThis repository contains the Hugo static site for the MSCA Digital Finance Industrial Doctoral Network, migrated from the original Wix website at [digital-finance-msca.com](https://www.digital-finance-msca.com/).\n\n## About MSCA Digital Finance\n\nThe Industrial Doctoral Network in Digital Finance (DIGITAL) is an EU-funded Marie Sklodowska-Curie Action (Grant Agreement No. 101119635) that brings together:\n\n- 8 leading European universities\n- 3 major international corporations (Swedbank, Raiffeisen Bank, Deutsche Bank)\n- 3 SMEs\n- 3 research centres (Athena, EIT Digital, Fraunhofer)\n- 2 intergovernmental agencies (European Central Bank, Bank for International Settlements)\n\nThe network trains 17 PhD researchers in digital finance, focusing on AI, machine learning, and data science applications in the financial sector.\n\n## Project Structure\n\n```\nmsca-digital-finance/\n‚îú‚îÄ‚îÄ content/                 # Hugo content (markdown files)\n‚îÇ   ‚îú‚îÄ‚îÄ _index.md           # Homepage\n‚îÇ   ‚îú‚îÄ‚îÄ people/             # Team member profiles\n‚îÇ   ‚îú‚îÄ‚îÄ partners/           # Partner organizations\n‚îÇ   ‚îú‚îÄ‚îÄ blog/               # News and blog posts\n‚îÇ   ‚îú‚îÄ‚îÄ training-modules/   # Training module descriptions\n‚îÇ   ‚îú‚îÄ‚îÄ training-events/    # Training event information\n‚îÇ   ‚îî‚îÄ‚îÄ events/             # Event registrations\n‚îú‚îÄ‚îÄ static/\n‚îÇ   ‚îú‚îÄ‚îÄ css/                # Stylesheets\n‚îÇ   ‚îî‚îÄ‚îÄ images/             # Downloaded images\n‚îú‚îÄ‚îÄ layouts/                # Hugo templates\n‚îú‚îÄ‚îÄ scripts/                # Migration scripts\n‚îú‚îÄ‚îÄ data/                   # Scraped data and URLs\n‚îî‚îÄ‚îÄ .github/workflows/      # GitHub Actions for deployment\n```\n\n## Development\n\n### Prerequisites\n\n- [Hugo](https://gohugo.io/installation/) (extended version recommended)\n- Python 3.8+ (for migration scripts)\n\n### Local Development\n\n```bash\n# Clone the repository\ngit clone https://github.com/Digital-AI-Finance/msca-digital-finance.git\ncd msca-digital-finance\n\n# Start Hugo development server\nhugo server -D\n\n# Build for production\nhugo --minify\n```\n\n### Migration Scripts\n\nThe `scripts/` directory contains Python scripts used to migrate content from the original Wix site:\n\n1. `01_discover_urls.py` - Discovers all URLs from sitemaps\n2. `02_scrape_content.py` - Scrapes content using Playwright\n3. `03_download_images.py` - Downloads all images locally\n4. `04_setup_hugo.py` - Sets up Hugo configuration and templates\n5. `05_verify.py` - Verifies migration completeness\n\nTo run the migration scripts:\n\n```bash\ncd scripts\npip install -r requirements.txt\nplaywright install chromium\n\npython 01_discover_urls.py\npython 02_scrape_content.py\npython 03_download_images.py\npython 04_setup_hugo.py\npython 05_verify.py\n```\n\n## Deployment\n\nThe site is automatically deployed to GitHub Pages when changes are pushed to the `main` branch via GitHub Actions.\n\n**Live Site:** [https://digital-ai-finance.github.io/msca-digital-finance/](https://digital-ai-finance.github.io/msca-digital-finance/)\n\n## License\n\nContent copyright MSCA Digital Finance Consortium. Website template and migration scripts available under MIT License.\n\n## Contact\n\nFor questions about the MSCA Digital Finance project, please visit [Contact Us](https://digital-ai-finance.github.io/msca-digital-finance/contact-us/).\n\n---\n\n*This site was migrated from Wix using automated Python scripts and is hosted on GitHub Pages using Hugo.*\n",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "msca-digital-finance",
      "research": {
        "title": "MSCA Digital Finance Website - Hugo static site migrated from Wix",
        "abstract": "This repository contains the Hugo static site for the MSCA Digital Finance Industrial Doctoral Network, migrated from the original Wix website at [digital-finance-msca.com](https://www.digital-finance-msca.com/).",
        "keywords": [],
        "authors": []
      },
      "publications": [],
      "code": {
        "languages": [
          "HTML"
        ],
        "notebooks": [],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": true,
        "has_dockerfile": false,
        "has_environment_yml": false,
        "has_makefile": false,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:05:37.901368",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": [
        {
          "name": "data",
          "path": "data",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "all_discovered_urls.json",
          "path": "data/all_discovered_urls.json",
          "format": ".json",
          "size_bytes": 17355
        },
        {
          "name": "asset_extraction_results.json",
          "path": "data/asset_extraction_results.json",
          "format": ".json",
          "size_bytes": 254067
        },
        {
          "name": "cleanup_report.json",
          "path": "data/cleanup_report.json",
          "format": ".json",
          "size_bytes": 4243
        },
        {
          "name": "content_audit_report.json",
          "path": "data/content_audit_report.json",
          "format": ".json",
          "size_bytes": 809
        },
        {
          "name": "content_verification_report.json",
          "path": "data/content_verification_report.json",
          "format": ".json",
          "size_bytes": 4980
        },
        {
          "name": "deep_crawl_results.json",
          "path": "data/deep_crawl_results.json",
          "format": ".json",
          "size_bytes": 35943
        },
        {
          "name": "eu_cordis_20251202.json",
          "path": "data/eu_cordis_20251202.json",
          "format": ".json",
          "size_bytes": 1066
        },
        {
          "name": "eu_cordis_latest.json",
          "path": "data/eu_cordis_latest.json",
          "format": ".json",
          "size_bytes": 1066
        },
        {
          "name": "eu_project_data.json",
          "path": "data/eu_project_data.json",
          "format": ".json",
          "size_bytes": 5160
        },
        {
          "name": "eu_publications_complete.json",
          "path": "data/eu_publications_complete.json",
          "format": ".json",
          "size_bytes": 11456
        },
        {
          "name": "final_build_report.json",
          "path": "data/final_build_report.json",
          "format": ".json",
          "size_bytes": 228
        },
        {
          "name": "final_verification_report.json",
          "path": "data/final_verification_report.json",
          "format": ".json",
          "size_bytes": 191646
        },
        {
          "name": "image_check_report.json",
          "path": "data/image_check_report.json",
          "format": ".json",
          "size_bytes": 209
        },
        {
          "name": "image_fix_report.json",
          "path": "data/image_fix_report.json",
          "format": ".json",
          "size_bytes": 164
        },
        {
          "name": "image_mapping.json",
          "path": "data/image_mapping.json",
          "format": ".json",
          "size_bytes": 47986
        },
        {
          "name": "latest_sync_report.json",
          "path": "data/latest_sync_report.json",
          "format": ".json",
          "size_bytes": 509
        },
        {
          "name": "link_check_report.json",
          "path": "data/link_check_report.json",
          "format": ".json",
          "size_bytes": 3985
        },
        {
          "name": "link_fix_results.json",
          "path": "data/link_fix_results.json",
          "format": ".json",
          "size_bytes": 812058
        },
        {
          "name": "migration_report.html",
          "path": "data/migration_report.html",
          "format": ".html",
          "size_bytes": 29950
        },
        {
          "name": "missing_pages_results.json",
          "path": "data/missing_pages_results.json",
          "format": ".json",
          "size_bytes": 5927
        },
        {
          "name": "navigation_report.json",
          "path": "data/navigation_report.json",
          "format": ".json",
          "size_bytes": 141
        },
        {
          "name": "picture_audit_report.json",
          "path": "data/picture_audit_report.json",
          "format": ".json",
          "size_bytes": 6135
        },
        {
          "name": "retry_results.json",
          "path": "data/retry_results.json",
          "format": ".json",
          "size_bytes": 1523
        },
        {
          "name": "scrape_progress.json",
          "path": "data/scrape_progress.json",
          "format": ".json",
          "size_bytes": 32249
        },
        {
          "name": "site_audit_report.json",
          "path": "data/site_audit_report.json",
          "format": ".json",
          "size_bytes": 1563
        },
        {
          "name": "sync_history",
          "path": "data/sync_history",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "sync_20251202_145416.json",
          "path": "data/sync_history/sync_20251202_145416.json",
          "format": ".json",
          "size_bytes": 278
        },
        {
          "name": "sync_20251202_145546.json",
          "path": "data/sync_history/sync_20251202_145546.json",
          "format": ".json",
          "size_bytes": 2877
        },
        {
          "name": "sync_20251202_145603.json",
          "path": "data/sync_history/sync_20251202_145603.json",
          "format": ".json",
          "size_bytes": 278
        },
        {
          "name": "sync_20251202_145746.json",
          "path": "data/sync_history/sync_20251202_145746.json",
          "format": ".json",
          "size_bytes": 278
        },
        {
          "name": "sync_20251202_153256.json",
          "path": "data/sync_history/sync_20251202_153256.json",
          "format": ".json",
          "size_bytes": 509
        },
        {
          "name": "sync_20251202_160008.json",
          "path": "data/sync_history/sync_20251202_160008.json",
          "format": ".json",
          "size_bytes": 509
        },
        {
          "name": "sync_state.json",
          "path": "data/sync_state.json",
          "format": ".json",
          "size_bytes": 113032
        },
        {
          "name": "urls.json",
          "path": "data/urls.json",
          "format": ".json",
          "size_bytes": 94959
        },
        {
          "name": "verification_report.txt",
          "path": "data/verification_report.txt",
          "format": ".txt",
          "size_bytes": 543
        }
      ]
    }
  },
  {
    "name": "ai-in-finance",
    "full_name": "Digital-AI-Finance/ai-in-finance",
    "description": "AI in Finance - UT & ING Collaboration Website",
    "url": "https://github.com/Digital-AI-Finance/ai-in-finance",
    "clone_url": "https://github.com/Digital-AI-Finance/ai-in-finance.git",
    "homepage": "",
    "language": "SCSS",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 14876,
    "default_branch": "main",
    "created_at": "2025-12-01T22:57:40+00:00",
    "updated_at": "2025-12-04T11:48:26+00:00",
    "pushed_at": "2025-12-04T11:48:22+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": true,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "# AI in Finance Website\n\nThis is the Hugo-based website for the AI in Finance collaboration between University of Twente and ING.\n\n## Overview\n\nThe ING-UT AI in Finance collaboration focuses on advanced AI applications in:\n- Data handling\n- Risk management\n- Business operations within the finance sector\n\n## Local Development\n\n1. Install Hugo (extended version): https://gohugo.io/installation/\n\n2. Clone with submodules:\n   ```bash\n   git clone --recurse-submodules https://github.com/Digital-AI-Finance/ai-in-finance.git\n   ```\n\n3. Start development server:\n   ```bash\n   cd ai-in-finance\n   hugo server -D\n   ```\n\n4. Open http://localhost:1313\n\n## Deployment\n\nThe site is automatically deployed to GitHub Pages when pushing to the `main` branch.\n\nLive site: https://digital-ai-finance.github.io/ai-in-finance/\n\n## Team\n\n- **University of Twente**: 11 professors and researchers\n- **ING**: 22+ team members\n- **Students**: 9 MSc students, 1 PhD candidate\n\n## Links\n\n- [Original site](https://www.ai-in-finance.eu/)\n- [Digital Finance MSCA](https://www.digital-finance-msca.com/)\n- [KickStart AI](https://www.kickstartai.nl/)\n",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "ai-in-finance",
      "research": {
        "title": "AI in Finance - UT & ING Collaboration Website",
        "abstract": "The ING-UT AI in Finance collaboration focuses on advanced AI applications in:\n- Data handling\n- Risk management\n- Business operations within the finance sector",
        "keywords": [],
        "authors": [
          {
            "name": "University of Twente**: 11 professors and researchers"
          },
          {
            "name": "ING**: 22+ team members"
          },
          {
            "name": "Students**: 9 MSc students, 1 PhD candidate"
          }
        ]
      },
      "publications": [],
      "code": {
        "languages": [
          "SCSS"
        ],
        "notebooks": [],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": false,
        "has_dockerfile": false,
        "has_environment_yml": false,
        "has_makefile": false,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:05:43.361220",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": [
        {
          "name": "funding.json",
          "path": "_data/funding.json",
          "format": ".json",
          "size_bytes": 1299
        },
        {
          "name": "news.json",
          "path": "_data/news.json",
          "format": ".json",
          "size_bytes": 1454
        },
        {
          "name": "publications.json",
          "path": "_data/publications.json",
          "format": ".json",
          "size_bytes": 24034
        },
        {
          "name": "team.json",
          "path": "_data/team.json",
          "format": ".json",
          "size_bytes": 9956
        }
      ]
    }
  },
  {
    "name": "Fintech-and-Artificial-Intelligence-in-Finance",
    "full_name": "Digital-AI-Finance/Fintech-and-Artificial-Intelligence-in-Finance",
    "description": "Archive of COST Action CA19130 - FinAI: Fintech and Artificial Intelligence in Finance (2020-2024)",
    "url": "https://github.com/Digital-AI-Finance/Fintech-and-Artificial-Intelligence-in-Finance",
    "clone_url": "https://github.com/Digital-AI-Finance/Fintech-and-Artificial-Intelligence-in-Finance.git",
    "homepage": "",
    "language": "HTML",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 240781,
    "default_branch": "main",
    "created_at": "2025-12-02T07:11:54+00:00",
    "updated_at": "2025-12-05T12:42:52+00:00",
    "pushed_at": "2025-12-05T12:42:48+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": false,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "# COST Action CA19130 - Fintech and Artificial Intelligence in Finance (FinAI)\n\n![Files](https://img.shields.io/badge/Files-724+-blue) ![Size](https://img.shields.io/badge/Size-763%20MB-green) ![Countries](https://img.shields.io/badge/Countries-51-orange) ![Members](https://img.shields.io/badge/Members-413+-red)\n\n**Complete Archive of COST Action CA19130: \"Fintech and Artificial Intelligence in Finance - Towards a transparent financial industry\"**\n\n## Browse the Archive Online\n\nView the complete archive website: [https://digital-ai-finance.github.io/Fintech-and-Artificial-Intelligence-in-Finance/](https://digital-ai-finance.github.io/Fintech-and-Artificial-Intelligence-in-Finance/)\n\nThe site provides easy navigation through all working groups, publications, country reports, and deliverables.\n\n---\n\n## Table of Contents\n\n- [Quick Start](#quick-start)\n- [Action Overview](#action-overview)\n- [Leadership](#leadership)\n- [Action Description](#action-description)\n- [Working Groups](#working-groups)\n- [Repository Contents](#repository-contents)\n- [Country Reports](#country-reports-2023-2024)\n- [Key Documents](#key-documents)\n- [Participating Countries](#participating-countries-51)\n- [Management Committee Members](#management-committee-members-sample)\n- [Primary Websites](#primary-websites)\n- [Academic Publications](#academic-publications)\n- [Related Projects](#related-projects)\n- [Using the Scraper](#using-the-scraper)\n- [License](#license)\n- [Acknowledgments](#acknowledgments)\n- [Documentation Index](#documentation-index)\n\n---\n\n## Quick Start\n\n### Navigating the Archive\n\nThis archive contains the complete historical record of COST Action CA19130 (2020-2024). Here's how to navigate:\n\n1. **For Country Reports**: Navigate to `downloads/google_drive_robust/Country_Updates/` or see [COUNTRY_UPDATES.md](docs/COUNTRY_UPDATES.md)\n2. **For Academic Publications**: See [PUBLICATIONS.md](docs/PUBLICATIONS.md) or check `downloads/ssrn_papers/`\n3. **For Official Deliverables**: See [DELIVERABLES.md](docs/DELIVERABLES.md) or check `downloads/google_drive_robust/Progress_Report/`\n4. **For Grant Information**: See [INNOVATORS_GRANT.md](docs/INNOVATORS_GRANT.md) or check `downloads/google_drive_robust/COST_Innovators_Grant/`\n5. **For Member Directory**: See [MEMBERS.md](docs/MEMBERS.md) for complete leadership and MC member lists\n\n### Quick Links to Major Sections\n\n- **Country Reports (2023-2024)**: 28+ reports from 24+ countries\n- **COST Innovators Grant**: Application materials and evaluation reports\n- **Progress Reports**: Mid-term and final action assessments\n- **Working Group Outputs**: Research deliverables from WG1, WG2, WG3\n- **Conference Materials**: Events and presentations archive\n\n---\n\n## Action Overview\n\n| Field | Value |\n|-------|-------|\n| **Action Number** | CA19130 |\n| **Acronym** | FinAI |\n| **Full Title** | Fintech and Artificial Intelligence in Finance - Towards a transparent financial industry |\n| **CSO Approval** | 31/03/2020 |\n| **Start Date** | 14/09/2020 |\n| **End Date** | 13/09/2024 |\n| **Duration** | 4 years |\n| **COST Countries** | 51 |\n| **Researchers** | 413+ members |\n\n## Leadership\n\n| Role | Name | Institution |\n|------|------|-------------|\n| **Action Chair** | Prof. Jorg Osterrieder | josterri@googlemail.com |\n| **Action Vice-Chair** | Prof. Valerio Poti | University College Dublin |\n| **Grant Holder Scientific Representative** | Prof. Branka Hadji Misheva | Switzerland |\n| **Science Communication Coordinator** | Dr. Ioana Coita | Romania |\n| **Grant Awarding Coordinator** | Prof. Codruta Mare | Romania |\n| **WG1 Leader** | Prof. Wolfgang Hardle | Humboldt University Berlin |\n| **WG2 Leader** | Prof. Petre Lameski | North Macedonia |\n| **WG3 Leader** | Prof. Peter Schwendner | ZHAW Switzerland |\n\n## Action Description\n\nThe financial sector is the largest user of digital technologies and a major driver in the digital transformation of the economy. Financial technology (FinTech) aims to both compete with and support the established financial industry in the delivery of financial services. Globally, more than $100 billion of investments have been made into FinTech companies and Artificial Intelligence (AI) since 2010, and continue growing substantially.\n\nIn early 2018, the European Commission unveiled (a) their action plan for a more competitive and innovative financial market, and (b) an initiative on AI with the aim to harness the opportunities presented by technology-enabled innovations. Europe should become a global hub for FinTech, with the economy being able to benefit from the European Single Market.\n\n### Key Objectives\n\n1. **Improve transparency of AI-supported processes** in the Fintech space\n2. **Address the disparity** between the proliferation in AI models within the financial industry for risk assessment and decision-making, and the limited insight the public has in its consequences by developing policy papers and methods to increase transparency\n3. **Develop methods to scrutinize the quality of products**, especially rule-based \"smart beta\" ones, across the asset management, banking and insurance industries\n\n### Action Keywords\n\nArtificial Intelligence - Fintech - Finance - Transparency - Financial Markets\n\n---\n\n## Working Groups\n\n### WG1: Transparency in FinTech\n**Leader:** Prof. Wolfgang Hardle (Humboldt University Berlin)\n**Co-Leaders:** Dr. Ioana Coita, Prof. Daniel Traian Pele\n\nFocus areas:\n- Machine Learning in Finance\n- Blockchain Analytics\n- Big Data Mining\n- Cryptocurrency Research\n- NLP in Finance\n\n### WG2: Transparent versus Black Box Decision-Support Models\n**Leader:** Prof. Petre Lameski (North Macedonia)\n**Co-Leader:** Dr. Kristina Sutiene\n\nFocus areas:\n- Explainable AI (XAI) in Finance\n- Credit Scoring Models\n- Risk Assessment\n- Algorithmic Trading\n- Model Interpretability\n\n### WG3: Transparency into Investment Product Performance for Clients\n**Leader:** Prof. Peter Schwendner (ZHAW Switzerland)\n\nFocus areas:\n- Investment Product Analysis\n- Smart Beta Strategies\n- Asset Management\n- Banking Products\n- Insurance Products\n\n---\n\n## Repository Contents\n\n**Total Files:** 724+ files\n**Total Size:** ~763 MB\n\n### Directory Structure\n\n```\nFintech-and-Artificial-Intelligence-in-Finance/\n|\n|-- downloads/\n|   |-- google_drive_robust/     # Primary archive (718 MB, 529 files)\n|   |   |-- Country_Updates/     # National reports 2023-2024\n|   |   |-- Country_Updates_2023/\n|   |   |-- Country_Updates_2024/\n|   |   |-- COST_Innovators_Grant/\n|   |   |-- CIG_1st_Round/\n|   |   |-- CIG_2nd_Round_Hearings/\n|   |   |-- Progress_Report/\n|   |   |-- Final_Report/\n|   |   |-- GP4_Progress_Report/\n|   |   |-- EU_Consultations/\n|   |   |-- 2024_COST_Application/\n|   |   |-- 2024_Proposal/\n|   |   |-- Evaluation_2023/\n|   |   |-- Evaluation_Criteria_2024/\n|   |   |-- Gantt_Chart/\n|   |   |-- Proposals_2023/\n|   |   |-- Template/\n|   |   |-- Archive/\n|   |   |-- Action_Chair/\n|   |   |-- Action_Chair_Main/\n|   |   `-- COST_GPs_Annual_Reports/\n|   |\n|   |-- google_drive/            # Additional materials (37 MB)\n|   |-- google_drive_authenticated/\n|   |-- wiki_fin_ai_eu/          # Wiki content (40 files)\n|   |-- fin_ai_eu/               # Main website (18 files)\n|   |-- cost_eu/                 # COST.eu content\n|   |-- conference_fin_ai_eu/    # Conference materials\n|   |-- external_conferences/    # External conference content\n|   |-- ssrn_papers/             # SSRN publications\n|   |-- academia_papers/         # Academia.edu papers\n|   |-- researchgate/            # ResearchGate content\n|   |-- springer/                # Springer publications\n|   |-- digital_finance_msca/    # MSCA project info\n|   `-- ai_in_finance_eu/        # AI in Finance portal\n|\n|-- docs/\n|   |-- COUNTRY_UPDATES.md       # Country reports summary\n|   |-- PUBLICATIONS.md          # Academic publications\n|   |-- DELIVERABLES.md          # COST deliverables\n|   |-- INNOVATORS_GRANT.md      # CIG documentation\n|   `-- MEMBERS.md               # Full membership list\n|\n|-- scripts/\n|   |-- scrape_cost_ca19130.py   # Web scraper\n|   |-- download_gdrive_oauth.py # Google Drive OAuth\n|   |-- download_gdrive_robust.py\n|   `-- requirements.txt\n|\n`-- README.md\n```\n\n---\n\n## Country Reports (2023-2024)\n\nReports from participating countries:\n\n| Country | Year | Type |\n|---------|------|------|\n| Austria | 2023 | PDF, PPTX |\n| Bulgaria | 2023 | PDF |\n| Croatia | 2023 | PPTX |\n| Cyprus | 2023 | PPTX |\n| Czechia | 2023 | PDF |\n| Denmark | 2023 | PPTX |\n| Estonia | 2023 | PDF, PPTX |\n| Finland | 2023 | PPTX |\n| France | 2023 | PPTX |\n| Germany | 2023 | PDF |\n| Greece | 2023 | PPTX |\n| Hungary | 2023 | PPTX |\n| Ireland | 2023 | PPTX |\n| Italy | 2023 | PPTX |\n| Kosovo | 2023 | PDF, PPTX |\n| Latvia | 2023 | PDF |\n| Lithuania | 2023 | PPTX |\n| North Macedonia | 2023 | PDF |\n| Poland | 2023 | PPTX |\n| Portugal | 2023 | PPTX |\n| Romania | 2023 | DOCX |\n| Serbia | 2023 | PPTX |\n| Slovakia | 2023-2024 | PPTX |\n| Turkey | 2023 | PPTX |\n\n**For complete details**: See [docs/COUNTRY_UPDATES.md](docs/COUNTRY_UPDATES.md)\n\n---\n\n## Key Documents\n\n### Official COST Documents\n- [Memorandum of Understanding (MoU)](https://e-services.cost.eu/files/domain_files/CA/Action_CA19130/mou/CA19130-e.pdf)\n- [Second Progress Report](https://e-services.cost.eu/files/domain_files/CA/Action_CA19130/second_progress_report/second_progress_report-CA19130.pdf)\n\n### Deliverables\n- Stress Test Designs for AI/ML Models\n- Policy Papers on AI Transparency\n- Research Publications\n\n**For complete details**: See [docs/DELIVERABLES.md](docs/DELIVERABLES.md)\n\n### COST Innovators Grant (CIG)\n- CIG Application 2024.01.24\n- CIG Evaluation Report\n- Letters of Support\n- Topic Proposals\n\n**For complete details**: See [docs/INNOVATORS_GRANT.md](docs/INNOVATORS_GRANT.md)\n\n---\n\n## Participating Countries (51)\n\n### COST Full Members\nAlbania, Austria, Belgium, Bosnia and Herzegovina, Bulgaria, Croatia, Cyprus, Czech Republic, Denmark, Estonia, Finland, France, Germany, Greece, Hungary, Iceland, Ireland, Israel, Italy, Latvia, Lithuania, Luxembourg, Malta, Moldova, Montenegro, Netherlands, North Macedonia, Norway, Poland, Portugal, Romania, Serbia, Slovakia, Slovenia, Spain, Sweden, Switzerland, Turkey, United Kingdom\n\n### COST Near Neighbor Countries\nAlgeria, Kosovo\n\n### International Partner Countries\nNew Zealand, Nigeria, Singapore, Taiwan, United Arab Emirates, United States, Vietnam\n\n---\n\n## Management Committee Members (Sample)\n\n| Country | MC Members |\n|---------|------------|\n| Germany | Prof. Wolfgang Hardle, Dr. Alla Petukhina |\n| Switzerland | Prof. Branka Hadji Misheva, Prof. Peter Schwendner |\n| Ireland | Prof. Don Bredin, Prof. Valerio Poti |\n| Romania | Prof. Codruta Mare, Prof. Daniel Traian Pele |\n| Italy | Prof. Maria Iannario, Prof. Claudia Tarantola |\n| Poland | Prof. Barbara Bedowska-Sojka, Prof. Piotr Wojcik |\n| Turkey | Prof. Enis Kayis, Prof. Belma Ozturkkal |\n| UK | Prof. Tomaso Aste, Prof. Ania Zalewska |\n\n**For complete directory**: See [docs/MEMBERS.md](docs/MEMBERS.md) or `downloads/cost_eu/main_page.md`\n\n---\n\n## Primary Websites\n\n| Website | URL | Description |\n|---------|-----|-------------|\n| COST EU Official | https://www.cost.eu/actions/CA19130/ | Official action page |\n| Main Website | https://fin-ai.eu/ | Project main site |\n| Wiki | https://wiki.fin-ai.eu/ | Documentation wiki |\n| Conference | https://conference.fin-ai.eu/ | Conference portal |\n| AI in Finance | https://www.ai-in-finance.eu/ | Research portal |\n\n---\n\n## Academic Publications\n\n### SSRN Papers\n1. **Exploring Research Visibility of the FinAI COST Action Members: a Bibliometric Analysis of Topics** (Abstract ID: 4616662)\n   - URL: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4616662\n\n2. **Mitigating Digital Asset Risks** - 30+ authors from COST Action CA19130 (Abstract ID: 4594467)\n   - URL: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4594467\n\n### Springer Publications\n- Financial Technology: 5th International Conference, ICFT 2024\n- Proceedings of International Conference on AI and Financial Innovation: AIFI 2025\n\n### Academic Resources\n- **Academia.edu**: https://www.academia.edu/92374154/\n- **ResearchGate Lab**: https://www.researchgate.net/lab/COST-Fintech-and-Artificial-Intelligence-in-Finance-FinAI-CA19130-Joerg-Osterrieder\n\n**For complete list**: See [docs/PUBLICATIONS.md](docs/PUBLICATIONS.md)\n\n---\n\n## Related Projects\n\n| Project | URL |\n|---------|-----|\n| MSCA Digital Finance | https://www.digital-finance-msca.com/ |\n| BFH Project | https://www.bfh.ch/en/research/research-projects/2022-993-860-061/ |\n\n---\n\n## Using the Scraper\n\n```bash\n# Install dependencies\npip install -r requirements.txt\n\n# Run the main scraper\npython scrape_cost_ca19130.py\n\n# For Google Drive OAuth download\npython download_gdrive_oauth.py\n```\n\nThe scraper handles:\n- SSL certificate issues via curl subprocess\n- Rate limiting (2 second delay between requests)\n- Automatic retry on failures (3 attempts)\n- HTML to Markdown conversion\n- PDF and media file downloads\n- Google Drive folder downloads\n\n---\n\n## License\n\nThis archive is for research and educational purposes. All original content remains the property of their respective owners.\n\n---\n\n## Acknowledgments\n\nCOST (European Cooperation in Science and Technology) is funded by the Horizon 2020 Framework Programme of the European Union.\n\n**COST Association**\nAvenue du Boulevard - Bolwerklaan 21\n1210 Brussels, Belgium\n\n---\n\n## Documentation Index\n\nComprehensive documentation files available in the `docs/` directory:\n\n### [COUNTRY_UPDATES.md](docs/COUNTRY_UPDATES.md)\nComplete list of country update reports submitted by participating countries (2023-2024). Includes:\n- 28+ reports from 24+ countries\n- Central, Northern, Southern, and Western Europe coverage\n- Balkans and international partner reports\n- Report topics and statistics\n- File locations and formats\n\n### [PUBLICATIONS.md](docs/PUBLICATIONS.md)\nAcademic publications, papers, and research outputs from the action. Includes:\n- Official COST documents (MoU, Progress Reports)\n- SSRN publications (2+ papers)\n- Springer conference proceedings (2+ volumes)\n- Research deliverables from all working groups\n- Academic resources (Academia.edu, ResearchGate)\n- Publication statistics and locations\n\n### [DELIVERABLES.md](docs/DELIVERABLES.md)\nOfficial deliverables produced during the action (2020-2024). Includes:\n- Official action documents (MoU, Progress Reports)\n- Working group deliverables (WG1, WG2, WG3)\n- Technical deliverables (software, tools, datasets)\n- Policy deliverables (papers, position statements)\n- Training and capacity building materials\n- Administrative documents and templates\n\n### [INNOVATORS_GRANT.md](docs/INNOVATORS_GRANT.md)\nCOST Innovators Grant (CIG) activities and documentation. Includes:\n- CIG application details (January 2024)\n- Application materials and templates\n- Evaluation process (1st and 2nd rounds)\n- CIG topics and innovation focus areas\n- Letters of support and branding materials\n- Eligibility requirements and key contacts\n\n### [MEMBERS.md](docs/MEMBERS.md)\nComplete list of Management Committee members and participants. Includes:\n- Leadership team (8 core roles)\n- Working group leaders and co-leaders\n- Additional leadership roles (15+ positions)\n- Management Committee members by country (70+ members)\n- COST staff contacts\n- Working group membership statistics (413+ total members)\n\n---\n\n*Archive created: 2025-12-02*\n*Last updated: 2025-12-04*\n",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "Fintech-and-Artificial-Intelligence-in-Finance",
      "research": {
        "title": "Archive of COST Action CA19130 - FinAI: Fintech and Artificial Intelligence in Finance (2020-2024)",
        "abstract": "![Files](https://img.shields.io/badge/Files-724+-blue) ![Size](https://img.shields.io/badge/Size-763%20MB-green) ![Countries](https://img.shields.io/badge/Countries-51-orange) ![Members](https://img.shields.io/badge/Members-413+-red)",
        "keywords": [],
        "authors": [
          {
            "name": "technology-enabled innovations. Europe should become a global hub for FinTech, with the economy being able to benefit from the European Single Market."
          },
          {
            "name": "developing policy papers and methods to increase transparency"
          },
          {
            "name": "the Horizon 2020 Framework Programme of the European Union."
          },
          {
            "name": "participating countries . Includes:",
            "affiliation": "2023-2024"
          },
          {
            "name": "country",
            "affiliation": "70+ members"
          }
        ]
      },
      "publications": [],
      "code": {
        "languages": [
          "HTML"
        ],
        "notebooks": [],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": true,
        "has_dockerfile": false,
        "has_environment_yml": false,
        "has_makefile": false,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:06:08.824319",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": [
        {
          "name": "deep_crawl_report.json",
          "path": "deep_crawl_report.json",
          "format": ".json",
          "size_bytes": 5621
        },
        {
          "name": "deep_crawl_v2_report.json",
          "path": "deep_crawl_v2_report.json",
          "format": ".json",
          "size_bytes": 4793
        },
        {
          "name": "deep_crawl_verification.json",
          "path": "deep_crawl_verification.json",
          "format": ".json",
          "size_bytes": 33090
        },
        {
          "name": "deliverables_inventory.json",
          "path": "deliverables_inventory.json",
          "format": ".json",
          "size_bytes": 109488
        },
        {
          "name": "publication_catalog.json",
          "path": "docs/publication_catalog.json",
          "format": ".json",
          "size_bytes": 78629
        },
        {
          "name": "download_progress.json",
          "path": "download_progress.json",
          "format": ".json",
          "size_bytes": 665
        },
        {
          "name": "CIG_FILE_INVENTORY.json",
          "path": "downloads/docs/CIG_FILE_INVENTORY.json",
          "format": ".json",
          "size_bytes": 69596
        },
        {
          "name": "gdrive_auth_download_report.json",
          "path": "gdrive_auth_download_report.json",
          "format": ".json",
          "size_bytes": 11208
        },
        {
          "name": "gdrive_download_report.json",
          "path": "gdrive_download_report.json",
          "format": ".json",
          "size_bytes": 25589
        },
        {
          "name": "gdrive_robust_download_report.json",
          "path": "gdrive_robust_download_report.json",
          "format": ".json",
          "size_bytes": 3716
        },
        {
          "name": "datasets",
          "path": "hugo/content/resources/datasets",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "data",
          "path": "hugo/static/data",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "countries.json",
          "path": "hugo/static/data/countries.json",
          "format": ".json",
          "size_bytes": 4533
        },
        {
          "name": "timeline.json",
          "path": "hugo/static/data/timeline.json",
          "format": ".json",
          "size_bytes": 1202
        },
        {
          "name": "members.csv",
          "path": "members/members.csv",
          "format": ".csv",
          "size_bytes": 23021
        },
        {
          "name": "members.json",
          "path": "members/members.json",
          "format": ".json",
          "size_bytes": 45431
        },
        {
          "name": "members_extract.json",
          "path": "members_extract.json",
          "format": ".json",
          "size_bytes": 1434
        },
        {
          "name": "urls_discovered.json",
          "path": "urls_discovered.json",
          "format": ".json",
          "size_bytes": 8435
        },
        {
          "name": "verification_report.json",
          "path": "verification_report.json",
          "format": ".json",
          "size_bytes": 8351
        }
      ]
    }
  },
  {
    "name": "book-next-word-prediction-site",
    "full_name": "Digital-AI-Finance/book-next-word-prediction-site",
    "description": "GitHub Pages site for Predicting the Next Word book",
    "url": "https://github.com/Digital-AI-Finance/book-next-word-prediction-site",
    "clone_url": "https://github.com/Digital-AI-Finance/book-next-word-prediction-site.git",
    "homepage": "",
    "language": "CSS",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 21,
    "default_branch": "main",
    "created_at": "2025-12-02T07:15:07+00:00",
    "updated_at": "2025-12-02T08:11:19+00:00",
    "pushed_at": "2025-12-02T08:11:15+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": true,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "No README available",
    "latest_release": null
  },
  {
    "name": "Systematic-Literature-Reviews-with-Artificial-Intelligence",
    "full_name": "Digital-AI-Finance/Systematic-Literature-Reviews-with-Artificial-Intelligence",
    "description": "Resources and tools for conducting Systematic Literature Reviews using Artificial Intelligence",
    "url": "https://github.com/Digital-AI-Finance/Systematic-Literature-Reviews-with-Artificial-Intelligence",
    "clone_url": "https://github.com/Digital-AI-Finance/Systematic-Literature-Reviews-with-Artificial-Intelligence.git",
    "homepage": "",
    "language": "Python",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 4638,
    "default_branch": "main",
    "created_at": "2025-12-02T07:52:39+00:00",
    "updated_at": "2025-12-04T06:51:37+00:00",
    "pushed_at": "2025-12-04T06:51:34+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": false,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "# Systematic Literature Reviews with Artificial Intelligence\n\nA comprehensive collection of resources, tools, and research for conducting Systematic Literature Reviews (SLRs) using Artificial Intelligence and Large Language Models.\n\n---\n\n## Table of Contents\n\n1. [Overview](#overview)\n2. [AI Tools for Systematic Reviews](#ai-tools-for-systematic-reviews)\n3. [Key Research Papers](#key-research-papers)\n4. [Methodological Guidelines](#methodological-guidelines)\n5. [Performance Benchmarks](#performance-benchmarks)\n6. [Getting Started](#getting-started)\n7. [Resources in This Repository](#resources-in-this-repository)\n\n---\n\n## Overview\n\nSystematic reviews constitute a critical foundation for evidence-based decision-making across disciplines. However, the labor-intensive nature of traditional SLRs - requiring weeks to months of manual work - has driven significant interest in AI-assisted automation.\n\n**Key Statistics:**\n- AI screening tools can reduce workload by 40-95%\n- otto-SR reproduced 12 Cochrane reviews in 2 days (equivalent to ~12 work-years manually)\n- GPT-4 achieves median accuracy >85% for PICO element extraction\n\n---\n\n## AI Tools for Systematic Reviews\n\n### Open Source Tools\n\n| Tool | Description | Key Features | Link |\n|------|-------------|--------------|------|\n| **ASReview** | Active learning for systematic reviews | Open-source, 95% workload reduction, Python-based | [asreview.nl](https://asreview.nl/) |\n| **RobotReviewer** | ML system for RCT assessment | Free, web-based, bias assessment | [robotreviewer.net](https://www.robotreviewer.net/) |\n| **Colandr** | Open-source screening tool | Free, collaborative | [colandrapp.com](https://www.colandrapp.com/) |\n| **FAST2** | Active learning screening | Open source | [GitHub](https://github.com/OHNLP/FAST2) |\n\n### Commercial/Freemium Tools\n\n| Tool | Description | Pricing | Link |\n|------|-------------|---------|------|\n| **Rayyan** | AI-powered review management | Free tier available | [rayyan.ai](https://www.rayyan.ai/) |\n| **Elicit** | AI research assistant | Free: basic / Pro: $42/mo | [elicit.com](https://elicit.com/) |\n| **Covidence** | Cochrane-recommended tool | Free for Cochrane reviews | [covidence.org](https://www.covidence.org/) |\n| **DistillerSR** | Enterprise review software | Subscription-based | [distillersr.com](https://www.distillersr.com/) |\n| **Laser AI** | Living systematic reviews | Commercial | [laser.ai](https://www.laser.ai/) |\n| **otto-SR** | End-to-end LLM workflow | Web platform | [ottosr.com](https://ottosr.com/) |\n| **EPPI-Reviewer** | Comprehensive review tool | Subscription | [eppi.ioe.ac.uk](https://eppi.ioe.ac.uk/) |\n\n### Specialized LLM Applications\n\n| Tool/Method | Application | Model |\n|-------------|-------------|-------|\n| **Systematic Review Extractor Pro** | Data extraction | Custom GPT |\n| **otto-SR Screening Agent** | Abstract/full-text screening | GPT-4.1 |\n| **otto-SR Extraction Agent** | Data extraction | o3-mini-high |\n\n---\n\n## Key Research Papers\n\n### Foundational Papers\n\n1. **ASReview Framework** (2021)\n   - van de Schoot, R. et al. \"An open source machine learning framework for efficient and transparent systematic reviews\"\n   - *Nature Machine Intelligence* 3, 125-133\n   - DOI: [10.1038/s42256-020-00287-7](https://doi.org/10.1038/s42256-020-00287-7)\n\n2. **Rayyan Original Paper** (2016)\n   - Ouzzani, M. et al. \"Rayyan - a web and mobile app for systematic reviews\"\n   - *Systematic Reviews* 5, 210\n   - DOI: [10.1186/s13643-016-0384-4](https://doi.org/10.1186/s13643-016-0384-4)\n\n### Recent LLM Research (2024-2025)\n\n3. **otto-SR: Automation of Systematic Reviews with LLMs** (2025)\n   - Demonstrated 96.7% sensitivity, 97.9% specificity in screening\n   - [Manuscript PDF](papers/otto-SR_manuscript.pdf) | [medRxiv](https://www.medrxiv.org/content/10.1101/2025.06.13.25329541v1)\n\n4. **LLMs for Systematic Reviews: Scoping Review** (2025)\n   - \"Large language models for conducting systematic reviews: on the rise, but not yet ready for use\"\n   - *Journal of Clinical Epidemiology*\n   - [ScienceDirect](https://www.sciencedirect.com/science/article/pii/S0895435625000794)\n\n5. **GPT-4 Evaluation for SLR** (2024)\n   - Khraisha, Q. et al. \"Can large language models replace humans in systematic reviews?\"\n   - *Research Synthesis Methods*\n   - DOI: [10.1002/jrsm.1715](https://doi.org/10.1002/jrsm.1715)\n\n6. **LLM-Assisted SLR System** (2025)\n   - \"Enhancing systematic literature reviews with generative AI\"\n   - *JAMIA* 32(4), 616\n   - [Oxford Academic](https://academic.oup.com/jamia/article/32/4/616/8045049)\n\n### Methodology & Guidelines\n\n7. **PRISMA-AI Guidelines** (2023)\n   - \"PRISMA AI reporting guidelines for systematic reviews and meta-analyses on AI in healthcare\"\n   - *Nature Medicine*\n   - DOI: [10.1038/s41591-022-02139-w](https://doi.org/10.1038/s41591-022-02139-w)\n\n8. **Practical Guide to ML in Research Synthesis** (2019)\n   - \"Toward systematic review automation: a practical guide\"\n   - *Systematic Reviews*\n   - DOI: [10.1186/s13643-019-1074-9](https://doi.org/10.1186/s13643-019-1074-9)\n\n---\n\n## Methodological Guidelines\n\n### PRISMA-AI Framework\n\nThe PRISMA-AI extension provides standardized reporting for AI-related systematic reviews:\n- Search strategy documentation\n- Quality assessment with AI-specific criteria\n- Transparent result reporting\n- Technical reproducibility requirements\n\n### LLM Integration Guidelines\n\nWhen integrating LLMs into systematic reviews:\n\n1. **Screening Phase**\n   - Use zero-shot or few-shot classification\n   - Define clear inclusion/exclusion criteria in prompts\n   - Maintain human oversight for borderline cases\n\n2. **Data Extraction**\n   - Use structured prompts (RISEN framework)\n   - Validate extracted data against source documents\n   - Document prompt versions for reproducibility\n\n3. **Quality Assurance**\n   - Dual verification (AI + human) recommended\n   - Report sensitivity and specificity metrics\n   - Document AI model versions and parameters\n\n---\n\n## Performance Benchmarks\n\n### Screening Accuracy\n\n| Tool/Method | Sensitivity | Specificity | Notes |\n|-------------|-------------|-------------|-------|\n| otto-SR | 96.7% | 97.9% | GPT-4.1 based |\n| Human dual review | 81.7% | 98.1% | Traditional approach |\n| Rayyan AI | 97-99% | 19-58% | At <2.5 threshold |\n| ASReview | Variable | Variable | Depends on dataset |\n\n### Data Extraction\n\n| Model | Precision | Recall | Notes |\n|-------|-----------|--------|-------|\n| GPT-based (pooled) | 83.0% | 86.0% | Mean across studies |\n| BERT-based | Lower | Lower | Compared to GPT |\n| otto-SR extraction | 93.1% accuracy | - | o3-mini-high |\n\n### Time Savings\n\n| Stage | Traditional | AI-Assisted | Reduction |\n|-------|-------------|-------------|-----------|\n| Screening | 8-12 weeks | 2-3 weeks | ~75% |\n| Data extraction | 10-16 weeks | 3-5 weeks | ~70% |\n| Per-paper extraction | 36 min | 27 sec + 13 min review | ~60% |\n\n---\n\n## Getting Started\n\n### For Beginners\n\n1. **Start with Rayyan** - Free tier, user-friendly interface\n2. **Try ASReview** - Open source, well-documented\n3. **Read the PRISMA guidelines** - Understand methodological requirements\n\n### For Advanced Users\n\n1. **Explore otto-SR** - State-of-the-art LLM automation\n2. **Build custom GPT extractors** - Use RISEN framework\n3. **Combine tools** - ASReview for screening + ChatGPT for extraction\n\n### Python Implementation\n\n```python\n# Install ASReview\npip install asreview\n\n# Basic usage\nfrom asreview import ASReviewProject\n\n# See ASReview documentation: https://asreview.readthedocs.io/\n```\n\n---\n\n## Resources in This Repository\n\n### Papers Directory (`/papers`)\n\n| File | Description |\n|------|-------------|\n| `otto-SR_manuscript.pdf` | Full otto-SR methodology paper |\n| `otto-SR_full_paper.pdf` | medRxiv preprint |\n| `LLM_systematic_reviews_scoping.pdf` | Scoping review of LLMs in SLRs |\n| `ASReview_paper_info.txt` | Citation info for ASReview paper |\n\n### Scripts\n\n| File | Description |\n|------|-------------|\n| `create_repo.py` | GitHub repository creation script |\n| `download_resources.py` | Resource download automation |\n\n---\n\n## Key GitHub Repositories\n\n- [asreview/asreview](https://github.com/asreview/asreview) - Active learning for systematic reviews\n- [asreview/synergy-dataset](https://github.com/asreview/synergy-dataset) - ML dataset for study selection\n- [asreview/paper-asreview](https://github.com/asreview/paper-asreview) - Scripts for ASReview paper\n- [systematic-reviews topic](https://github.com/topics/systematic-reviews) - GitHub topic for SLR tools\n\n---\n\n## Additional Resources\n\n### Library Guides\n\n- [King's College London - AI in Evidence Synthesis](https://libguides.kcl.ac.uk/systematicreview/ai)\n- [Purdue University - AI Tools for Systematic Review](https://guides.lib.purdue.edu/c.php?g=1371380&p=10619604)\n- [Harvard Library - Systematic Reviews Software](https://guides.library.harvard.edu/meta-analysis/software)\n- [Lancaster University - Systematic Reviews Tools](https://lancaster.libguides.com/SystematicReviews/tools)\n\n### Tutorials & Blogs\n\n- [Anara - AI for Literature Review Guide 2025](https://anara.com/blog/ai-for-literature-review)\n- [ASReview Documentation](https://asreview.readthedocs.io/)\n\n---\n\n## Citation\n\nIf you use resources from this repository, please cite the original sources appropriately.\n\n---\n\n## Contributing\n\nContributions are welcome! Please submit issues or pull requests for:\n- New tools or resources\n- Updated benchmarks\n- Bug fixes or corrections\n\n---\n\n## License\n\nThis repository is for educational and research purposes. Individual papers and tools may have their own licenses.\n\n---\n\n*Last updated: December 2025*\n",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "Systematic-Literature-Reviews-with-Artificial-Intelligence",
      "research": {
        "title": "Resources and tools for conducting Systematic Literature Reviews using Artificial Intelligence",
        "abstract": "Systematic reviews constitute a critical foundation for evidence-based decision-making across disciplines. However, the labor-intensive nature of traditional SLRs - requiring weeks to months of manual work - has driven significant interest in AI-assisted automation.\n\nKey Statistics:\n- AI screening tools can reduce workload by 40-95%\n- otto-SR reproduced 12 Cochrane reviews in 2 days (equivalent to ~12 work-years manually)\n- GPT-4 achieves median accuracy >85% for PICO element extraction\n\n---",
        "keywords": [],
        "authors": [
          {
            "name": "40-95%"
          }
        ]
      },
      "publications": [
        {
          "type": "journal-article",
          "doi": "10.1038/s42256-020-00287-7",
          "url": "https://doi.org/10.1038/s42256-020-00287-7",
          "title": "An open source machine learning framework for efficient and transparent systematic reviews",
          "abstract": "<jats:title>Abstract</jats:title><jats:p>To help researchers conduct a systematic review or meta-analysis as efficiently and transparently as possible, we designed a tool to accelerate the step of screening titles and abstracts. For many tasks‚Äîincluding but not limited to systematic reviews and meta-analyses‚Äîthe scientific literature needs to be checked systematically. Scholars and practitioners currently screen thousands of studies by hand to determine which studies to include in their review or meta-analysis. This is error prone and inefficient because of extremely imbalanced data: only a fraction of the screened studies is relevant. The future of systematic reviewing will be an interaction with machine learning algorithms to deal with the enormous increase of available text. We therefore developed an open source machine learning-aided pipeline applying active learning: ASReview. We demonstrate by means of simulation studies that active learning can yield far more efficient reviewing than manual reviewing while providing high quality. Furthermore, we describe the options of the free and open source research software and present the results from user experience tests. We invite the community to contribute to open source projects such as our own that provide measurable and reproducible improvements over current practice.</jats:p>",
          "published": "2021-02-01",
          "year": 2021,
          "authors": [
            {
              "name": "Rens van de Schoot"
            },
            {
              "name": "Jonathan de Bruin"
            },
            {
              "name": "Raoul Schram"
            },
            {
              "name": "Parisa Zahedi"
            },
            {
              "name": "Jan de Boer"
            },
            {
              "name": "Felix Weijdema"
            },
            {
              "name": "Bianca Kramer"
            },
            {
              "name": "Martijn Huijts"
            },
            {
              "name": "Maarten Hoogerwerf"
            },
            {
              "name": "Gerbrich Ferdinands"
            },
            {
              "name": "Albert Harkema"
            },
            {
              "name": "Joukje Willemsen"
            },
            {
              "name": "Yongchao Ma"
            },
            {
              "name": "Qixiang Fang"
            },
            {
              "name": "Sybren Hindriks"
            },
            {
              "name": "Lars Tummers"
            },
            {
              "name": "Daniel L. Oberski"
            }
          ],
          "venue": "Nature Machine Intelligence",
          "publisher": "Springer Science and Business Media LLC",
          "citation_count": 712,
          "reference_count": 63
        },
        {
          "type": "journal-article",
          "doi": "10.1038/s41591-022-02139-w",
          "url": "https://doi.org/10.1038/s41591-022-02139-w",
          "title": "PRISMA AI reporting guidelines for systematic reviews and meta-analyses on AI in healthcare",
          "abstract": "",
          "published": "2023-01-01",
          "year": 2023,
          "authors": [
            {
              "name": "Giovanni E. Cacciamani"
            },
            {
              "name": "Timothy N. Chu"
            },
            {
              "name": "Daniel I. Sanford"
            },
            {
              "name": "Andre Abreu"
            },
            {
              "name": "Vinay Duddalwar"
            },
            {
              "name": "Assad Oberai"
            },
            {
              "name": "C.-C. Jay Kuo"
            },
            {
              "name": "Xiaoxuan Liu"
            },
            {
              "name": "Alastair K. Denniston"
            },
            {
              "name": "Baptiste Vasey"
            },
            {
              "name": "Peter McCulloch"
            },
            {
              "name": "Robert F. Wolff"
            },
            {
              "name": "Sue Mallett"
            },
            {
              "name": "John Mongan"
            },
            {
              "name": "Charles E. Kahn"
            },
            {
              "name": "Viknesh Sounderajah"
            },
            {
              "name": "Ara Darzi"
            },
            {
              "name": "Philipp Dahm"
            },
            {
              "name": "Karel G. M. Moons"
            },
            {
              "name": "Eric Topol"
            },
            {
              "name": "Gary S. Collins"
            },
            {
              "name": "David Moher"
            },
            {
              "name": "Inderbir S. Gill"
            },
            {
              "name": "Andrew J. Hung"
            }
          ],
          "venue": "Nature Medicine",
          "publisher": "Springer Science and Business Media LLC",
          "citation_count": 92,
          "reference_count": 10
        },
        {
          "type": "journal",
          "doi": "10.1038/s41591-022-02139-w)",
          "url": "https://doi.org/10.1038/s41591-022-02139-w)"
        },
        {
          "type": "journal-article",
          "doi": "10.1002/jrsm.1715",
          "url": "https://doi.org/10.1002/jrsm.1715",
          "title": "Can large language models replace humans in systematic reviews? Evaluating <scp>GPT</scp>‚Äê4's efficacy in screening and extracting data from peer‚Äêreviewed and grey literature in multiple languages",
          "abstract": "<jats:title>Abstract</jats:title><jats:p>Systematic reviews are vital for guiding practice, research and policy, although they are often slow and labour‚Äêintensive. Large language models (LLMs) could speed up and automate systematic reviews, but their performance in such tasks has yet to be comprehensively evaluated against humans, and no study has tested Generative Pre‚ÄêTrained Transformer (GPT)‚Äê4, the biggest LLM so far. This pre‚Äêregistered study uses a ‚Äúhuman‚Äêout‚Äêof‚Äêthe‚Äêloop‚Äù approach to evaluate GPT‚Äê4's capability in title/abstract screening, full‚Äêtext review and data extraction across various literature types and languages. Although GPT‚Äê4 had accuracy on par with human performance in some tasks, results were skewed by chance agreement and dataset imbalance. Adjusting for these caused performance scores to drop across all stages: for data extraction, performance was moderate, and for screening, it ranged from none in highly balanced literature datasets (~1:1) to moderate in those datasets where the ratio of inclusion to exclusion in studies was imbalanced (~1:3). When screening full‚Äêtext literature using highly reliable prompts, GPT‚Äê4's performance was more robust, reaching ‚Äúhuman‚Äêlike‚Äù levels. Although our findings indicate that, currently, substantial caution should be exercised if LLMs are being used to conduct systematic reviews, they also offer preliminary evidence that, for certain review tasks delivered under specific conditions, LLMs can rival human performance.</jats:p>",
          "published": "2024-03-14",
          "year": 2024,
          "authors": [
            {
              "name": "Qusai Khraisha",
              "affiliation": "Trinity Centre for Global Health Trinity College Dublin  Dublin Ireland"
            },
            {
              "name": "Sophie Put",
              "affiliation": "Department of Education York University  York UK"
            },
            {
              "name": "Johanna Kappenberg",
              "affiliation": "School of Psychology Trinity College Dublin  Dublin Ireland"
            },
            {
              "name": "Azza Warraitch",
              "affiliation": "Trinity Centre for Global Health Trinity College Dublin  Dublin Ireland"
            },
            {
              "name": "Kristin Hadfield",
              "affiliation": "Trinity Centre for Global Health Trinity College Dublin  Dublin Ireland"
            }
          ],
          "venue": "Research Synthesis Methods",
          "publisher": "Wiley",
          "citation_count": 133,
          "reference_count": 42
        },
        {
          "type": "journal",
          "doi": "10.1186/s13643-016-0384-4)",
          "url": "https://doi.org/10.1186/s13643-016-0384-4)"
        },
        {
          "type": "journal-article",
          "doi": "10.1186/s13643-016-0384-4",
          "url": "https://doi.org/10.1186/s13643-016-0384-4",
          "title": "Rayyan‚Äîa web and mobile app for systematic reviews",
          "abstract": "",
          "published": "2016-12-01",
          "year": 2016,
          "authors": [
            {
              "name": "Mourad Ouzzani"
            },
            {
              "name": "Hossam Hammady"
            },
            {
              "name": "Zbys Fedorowicz"
            },
            {
              "name": "Ahmed Elmagarmid"
            }
          ],
          "venue": "Systematic Reviews",
          "publisher": "Springer Science and Business Media LLC",
          "citation_count": 16780,
          "reference_count": 19
        },
        {
          "type": "journal",
          "doi": "10.1101/2025.06.13.25329541v1)",
          "url": "https://doi.org/10.1101/2025.06.13.25329541v1)"
        },
        {
          "type": "journal",
          "doi": "10.1186/s13643-019-1074-9)",
          "url": "https://doi.org/10.1186/s13643-019-1074-9)"
        },
        {
          "type": "journal",
          "doi": "10.1002/jrsm.1715)",
          "url": "https://doi.org/10.1002/jrsm.1715)"
        },
        {
          "type": "journal-article",
          "doi": "10.1186/s13643-019-1074-9",
          "url": "https://doi.org/10.1186/s13643-019-1074-9",
          "title": "Toward systematic review automation: a practical guide to using machine learning tools in research synthesis",
          "abstract": "",
          "published": "2019-07-11",
          "year": 2019,
          "authors": [
            {
              "name": "Iain J. Marshall"
            },
            {
              "name": "Byron C. Wallace"
            }
          ],
          "venue": "Systematic Reviews",
          "publisher": "Springer Science and Business Media LLC",
          "citation_count": 393,
          "reference_count": 40
        },
        {
          "type": "journal",
          "doi": "10.1038/s42256-020-00287-7)",
          "url": "https://doi.org/10.1038/s42256-020-00287-7)"
        }
      ],
      "code": {
        "languages": [
          "Python"
        ],
        "notebooks": [],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": false,
        "has_dockerfile": false,
        "has_environment_yml": false,
        "has_makefile": false,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:06:18.168932",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": [
        {
          "name": "data",
          "path": "data",
          "format": "",
          "size_bytes": 0
        },
        {
          "name": "benchmarks.json",
          "path": "data/benchmarks.json",
          "format": ".json",
          "size_bytes": 1460
        },
        {
          "name": "papers.json",
          "path": "data/papers.json",
          "format": ".json",
          "size_bytes": 2849
        },
        {
          "name": "tools.json",
          "path": "data/tools.json",
          "format": ".json",
          "size_bytes": 2529
        }
      ]
    }
  },
  {
    "name": "European-Financial-Data-Space",
    "full_name": "Digital-AI-Finance/European-Financial-Data-Space",
    "description": "European Financial Data Space - MSCA Digital research on EFDS, ESAP, ESEF, and FIDA",
    "url": "https://github.com/Digital-AI-Finance/European-Financial-Data-Space",
    "clone_url": "https://github.com/Digital-AI-Finance/European-Financial-Data-Space.git",
    "homepage": "",
    "language": "HTML",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 376,
    "default_branch": "main",
    "created_at": "2025-12-03T19:38:21+00:00",
    "updated_at": "2025-12-04T12:18:41+00:00",
    "pushed_at": "2025-12-04T12:18:38+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": false,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "# European Financial Data Space Status\n\nResearch documentation on the European Financial Data Space (EFDS), covering ESAP, ESEF, and FIDA regulatory frameworks.\n\n## Overview\n\nThis repository contains a comprehensive status report on the European Financial Data Space initiative, including:\n\n- **ESAP** (European Single Access Point) - Centralized EU financial data portal\n- **ESEF** (European Single Electronic Format) - iXBRL reporting standard\n- **FIDA** (Financial Data Access) - Open finance framework proposal\n- **ESRS** (European Sustainability Reporting Standards) integration\n\n## Authors\n\n- **Coordinator:** Codruta Mare\n- **Authors:** Siang-Li Jheng, Fulvio Raddi, Karolina Weyna\n\n## View Online\n\nThis document is available as a GitHub Pages site with:\n- Single-page scroll format\n- Interactive table of contents\n- Executive summary\n- Glossary of acronyms\n- Country-by-country authority table\n\n## Key Dates\n\n| Milestone | Date |\n|-----------|------|\n| ESAP Regulation adopted | 2023 |\n| ESAP portal minimum functionality | December 2025 |\n| ESAP full operationalization | July 2027 |\n\n## Acknowledgments\n\nThis work has been conducted under the Marie Sklodowska-Curie Actions under the European Union's Horizon Europe Research and Innovation program for the Industrial Doctoral Network on Digital Finance (DIGITAL), Project No 101119635.\n\n## License\n\nThis is a research document. Please cite appropriately if using in academic work.\n",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "European-Financial-Data-Space",
      "research": {
        "title": "European Financial Data Space - MSCA Digital research on EFDS, ESAP, ESEF, and FIDA",
        "abstract": "This repository contains a comprehensive status report on the European Financial Data Space initiative, including:\n\n- ESAP (European Single Access Point) - Centralized EU financial data portal\n- ESEF (European Single Electronic Format) - iXBRL reporting standard\n- FIDA (Financial Data Access) - Open finance framework proposal\n- ESRS (European Sustainability Reporting Standards) integration",
        "keywords": [],
        "authors": [
          {
            "name": "Coordinator:** Codruta Mare"
          },
          {
            "name": "Authors:** Siang-Li Jheng, Fulvio Raddi, Karolina Weyna"
          }
        ]
      },
      "publications": [],
      "code": {
        "languages": [
          "HTML"
        ],
        "notebooks": [],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": false,
        "has_dockerfile": false,
        "has_environment_yml": false,
        "has_makefile": false,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:06:20.766027",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": [
        {
          "name": "policy_recommendations_analysis.json",
          "path": "policy_recommendations_analysis.json",
          "format": ".json",
          "size_bytes": 6235
        }
      ]
    }
  },
  {
    "name": "ML_Design_Thinking_16",
    "full_name": "Digital-AI-Finance/ML_Design_Thinking_16",
    "description": "Machine Learning for Smarter Innovation - BSc course bridging ML/AI with design thinking",
    "url": "https://github.com/Digital-AI-Finance/ML_Design_Thinking_16",
    "clone_url": "https://github.com/Digital-AI-Finance/ML_Design_Thinking_16.git",
    "homepage": "",
    "language": "Jupyter Notebook",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 233134,
    "default_branch": "main",
    "created_at": "2025-12-03T22:35:05+00:00",
    "updated_at": "2025-12-04T11:54:14+00:00",
    "pushed_at": "2025-12-04T11:54:10+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": false,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "No README available",
    "latest_release": null
  },
  {
    "name": "joerg-osterrieder",
    "full_name": "Digital-AI-Finance/joerg-osterrieder",
    "description": "CV and Academic Profile - Prof. Dr. Joerg R. Osterrieder",
    "url": "https://github.com/Digital-AI-Finance/joerg-osterrieder",
    "clone_url": "https://github.com/Digital-AI-Finance/joerg-osterrieder.git",
    "homepage": "",
    "language": "HTML",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 5437,
    "default_branch": "main",
    "created_at": "2025-12-04T07:37:45+00:00",
    "updated_at": "2025-12-04T11:45:00+00:00",
    "pushed_at": "2025-12-04T11:44:57+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": false,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "No README available",
    "latest_release": null
  },
  {
    "name": "hugo-research-template",
    "full_name": "Digital-AI-Finance/hugo-research-template",
    "description": "Hugo template for research project websites with GitHub Pages",
    "url": "https://github.com/Digital-AI-Finance/hugo-research-template",
    "clone_url": "https://github.com/Digital-AI-Finance/hugo-research-template.git",
    "homepage": "",
    "language": "HTML",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 11,
    "default_branch": "main",
    "created_at": "2025-12-04T08:23:34+00:00",
    "updated_at": "2025-12-04T08:31:04+00:00",
    "pushed_at": "2025-12-04T08:31:01+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": true,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "# Hugo Research Project Website Template\n\nA clean, professional Hugo template for research project websites with a fixed left sidebar navigation.\n\n## Features\n\n- Fixed left sidebar navigation (140px)\n- Responsive design (sidebar hides below 900px)\n- Multiple section types pre-styled\n- Easy color customization via CSS variables\n- GitHub Pages compatible (with GitHub Actions)\n\n## Quick Start\n\n1. Copy this entire folder to your new project\n2. Update `hugo.toml` with your project details\n3. Modify `layouts/index.html` with your content\n4. Run `hugo server` for local development\n5. Push to GitHub and configure GitHub Actions\n\n## Local Development\n\n```bash\n# Install Hugo (https://gohugo.io/installation/)\n# Windows: winget install Hugo.Hugo.Extended\n# Mac: brew install hugo\n\n# Serve locally with live reload\nhugo server -D\n\n# Build for production\nhugo --minify\n```\n\nThe site is served at `http://localhost:1313/` by default.\n\n## File Structure\n\n```\nhugo-research-template/\n‚îú‚îÄ‚îÄ hugo.toml              # Hugo configuration\n‚îú‚îÄ‚îÄ layouts/\n‚îÇ   ‚îú‚îÄ‚îÄ _default/\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ baseof.html    # Base layout with sidebar\n‚îÇ   ‚îî‚îÄ‚îÄ index.html         # Homepage content\n‚îú‚îÄ‚îÄ static/\n‚îÇ   ‚îî‚îÄ‚îÄ css/\n‚îÇ       ‚îî‚îÄ‚îÄ style.css      # All styles\n‚îú‚îÄ‚îÄ content/\n‚îÇ   ‚îî‚îÄ‚îÄ _index.md          # Homepage metadata\n‚îî‚îÄ‚îÄ README.md              # This file\n```\n\n## Configuration (hugo.toml)\n\nUpdate these values:\n\n```toml\nbaseURL = \"https://yourusername.github.io/your-repo-name/\"\ntitle = \"Your Project Title\"\n\n[params]\n  description = \"Brief description for SEO\"\n  badge1 = \"Research Project\"\n  badge2 = \"2024-2027\"\n  author = \"Dr. Your Name\"\n  email = \"email@university.edu\"\n  institution = \"Department Name\"\n  university = \"University Name\"\n  location = \"City, Country\"\n```\n\n## GitHub Pages Deployment\n\nCreate `.github/workflows/hugo.yml`:\n\n```yaml\nname: Deploy Hugo site to Pages\n\non:\n  push:\n    branches: [\"main\"]\n  workflow_dispatch:\n\npermissions:\n  contents: read\n  pages: write\n  id-token: write\n\nconcurrency:\n  group: \"pages\"\n  cancel-in-progress: false\n\ndefaults:\n  run:\n    shell: bash\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    env:\n      HUGO_VERSION: 0.128.0\n    steps:\n      - name: Install Hugo CLI\n        run: |\n          wget -O ${{ runner.temp }}/hugo.deb https://github.com/gohugoio/hugo/releases/download/v${HUGO_VERSION}/hugo_extended_${HUGO_VERSION}_linux-amd64.deb \\\n          && sudo dpkg -i ${{ runner.temp }}/hugo.deb\n      - name: Checkout\n        uses: actions/checkout@v4\n      - name: Build with Hugo\n        run: hugo --minify\n      - name: Upload artifact\n        uses: actions/upload-pages-artifact@v3\n        with:\n          path: ./public\n\n  deploy:\n    environment:\n      name: github-pages\n      url: ${{ steps.deployment.outputs.page_url }}\n    runs-on: ubuntu-latest\n    needs: build\n    steps:\n      - name: Deploy to GitHub Pages\n        id: deployment\n        uses: actions/deploy-pages@v4\n```\n\n## Customization\n\n### Colors (static/css/style.css)\n\nEdit the CSS variables at the top of `style.css`:\n\n```css\n:root {\n    --primary-color: #2d3748;      /* Dark gray - headers, sidebar */\n    --primary-light: #4a5568;      /* Lighter gray - hover states */\n    --accent-color: #805ad5;       /* Purple - accents, badges */\n    --text-color: #2d3748;         /* Dark gray - body text */\n}\n```\n\n### Navigation Links (layouts/_default/baseof.html)\n\nAdd or remove navigation links in the sidebar section.\n\n## Dependencies\n\nThe template uses these external resources (loaded via CDN):\n\n- Google Fonts: Inter\n- Font Awesome 6.4.0 (icons)\n\n## License\n\nFree to use for any research project.\n",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "hugo-research-template",
      "research": {
        "title": "Hugo template for research project websites with GitHub Pages",
        "abstract": "A clean, professional Hugo template for research project websites with a fixed left sidebar navigation.",
        "keywords": [],
        "authors": [
          {
            "name": "default."
          }
        ]
      },
      "publications": [],
      "code": {
        "languages": [
          "HTML"
        ],
        "notebooks": [],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": false,
        "has_dockerfile": false,
        "has_environment_yml": false,
        "has_makefile": false,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:06:28.099294",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": []
    }
  },
  {
    "name": "2025-shanghai-workshop",
    "full_name": "Digital-AI-Finance/2025-shanghai-workshop",
    "description": "23rd China International Talent Exchange Conference - Data Science, Fintech and Digital Economy - October 2025",
    "url": "https://github.com/Digital-AI-Finance/2025-shanghai-workshop",
    "clone_url": "https://github.com/Digital-AI-Finance/2025-shanghai-workshop.git",
    "homepage": "",
    "language": "HTML",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 2143,
    "default_branch": "main",
    "created_at": "2025-12-04T08:24:50+00:00",
    "updated_at": "2025-12-04T12:11:45+00:00",
    "pushed_at": "2025-12-04T12:11:42+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": false,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "No README available",
    "latest_release": null
  },
  {
    "name": "Research-Page-Template-Test",
    "full_name": "Digital-AI-Finance/Research-Page-Template-Test",
    "description": "Test page generated using the Research Project Page Template",
    "url": "https://github.com/Digital-AI-Finance/Research-Page-Template-Test",
    "clone_url": "https://github.com/Digital-AI-Finance/Research-Page-Template-Test.git",
    "homepage": "",
    "language": "HTML",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 6,
    "default_branch": "main",
    "created_at": "2025-12-04T10:46:51+00:00",
    "updated_at": "2025-12-04T10:47:49+00:00",
    "pushed_at": "2025-12-04T10:47:45+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": true,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "No README available",
    "latest_release": null
  },
  {
    "name": "research-page-generator",
    "full_name": "Digital-AI-Finance/research-page-generator",
    "description": "Toolkit for generating professional GitHub Pages sites for research projects",
    "url": "https://github.com/Digital-AI-Finance/research-page-generator",
    "clone_url": "https://github.com/Digital-AI-Finance/research-page-generator.git",
    "homepage": "",
    "language": "SCSS",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 41,
    "default_branch": "main",
    "created_at": "2025-12-04T11:11:35+00:00",
    "updated_at": "2025-12-04T11:11:51+00:00",
    "pushed_at": "2025-12-04T11:13:57+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": false,
    "has_pages": false,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "private",
    "contributors_count": 1,
    "readme": "# Research Project Page Generator\n\nA complete toolkit for generating professional GitHub Pages sites for research projects. Based on the [Network-Based Credit Risk Models](https://digital-ai-finance.github.io/network-based-credit-risk-models/) project.\n\n## Features\n\n- **Professional Design**: Navy/gold color scheme with responsive layout\n- **Dark Mode**: User-toggleable dark/light theme with localStorage persistence\n- **Mobile-First**: Hamburger menu and responsive grid layouts\n- **Publications**: Auto-fetched from OpenAlex.org with BibTeX export\n- **Search**: Full-text search using Lunr.js\n- **Analytics**: Publications chart (Chart.js) and co-authorship network (D3.js)\n- **SEO Ready**: Schema.org structured data, meta tags, sitemap, RSS feed\n- **Sections**: Home, Team, Research, Publications, Analytics, Resources, News, Events, Collaborations, Funding, Contact\n\n## Quick Start\n\n### Option 1: Use with Claude\n\n1. Copy `PROMPT.md` content\n2. Start a new conversation with Claude\n3. Paste the prompt and provide your project details\n4. Claude will generate a complete site customized for your project\n\n### Option 2: Manual Setup\n\n1. Copy the `template/` folder to your new repository\n2. Replace all `{{PLACEHOLDER}}` values in the files\n3. Update `_data/*.json` files with your content\n4. Push to GitHub and enable Pages\n\n## File Structure\n\n```\nresearch-page-generator/\n+-- PROMPT.md                    # Claude prompt for site generation\n+-- README.md                    # This file\n+-- template/\n    +-- _config.yml              # Jekyll configuration\n    +-- index.md                 # Main page with all sections\n    +-- feed.xml                 # RSS feed (create if needed)\n    +-- _data/\n    |   +-- team.json            # Team member profiles\n    |   +-- publications.json    # Publication list\n    |   +-- news.json            # News/updates\n    |   +-- funding.json         # Funding sources\n    +-- assets/\n    |   +-- css/\n    |   |   +-- style.scss       # All styles with dark mode\n    |   +-- js/\n    |   |   +-- main.js          # Core functionality\n    |   |   +-- visualizations.js# Charts and graphs\n    |   +-- images/\n    |       +-- logos/           # Institution logos\n    +-- scripts/\n        +-- fetch_publications.py # OpenAlex publication fetcher\n        +-- verify_site.py       # Site verification tool\n```\n\n## Configuration Guide\n\n### 1. Jekyll Config (`_config.yml`)\n\nUpdate these key settings:\n\n```yaml\ntitle: \"Your Project Title\"\ndescription: \"Your project description\"\nbaseurl: \"/your-repo-name\"\nurl: \"https://your-org.github.io\"\nauthor: \"Principal Investigator Name\"\n```\n\n### 2. Team Data (`_data/team.json`)\n\n```json\n[\n  {\n    \"name\": \"Prof. Dr. Name\",\n    \"role\": \"Principal Investigator\",\n    \"institution\": \"University Name\",\n    \"bio\": \"Research focus description\",\n    \"image\": \"images/photo.jpg\",\n    \"orcid\": \"0000-0000-0000-0000\",\n    \"google_scholar\": \"https://scholar.google.com/...\",\n    \"linkedin\": \"https://linkedin.com/in/...\",\n    \"website\": \"https://...\"\n  }\n]\n```\n\n### 3. Publications (`_data/publications.json`)\n\nAuto-fetch using the Python script:\n\n```bash\n# 1. Edit scripts/fetch_publications.py\n# 2. Update TEAM_MEMBERS and CONTACT_EMAIL\n# 3. Run:\npython scripts/fetch_publications.py\n```\n\nOr manually add entries:\n\n```json\n[\n  {\n    \"title\": \"Publication Title\",\n    \"authors\": \"Author1, A., Author2, B.\",\n    \"journal\": \"Journal Name\",\n    \"year\": 2024,\n    \"doi\": \"10.xxxx/xxxxx\",\n    \"citations\": 10,\n    \"open_access\": true,\n    \"abstract\": \"Optional abstract text\"\n  }\n]\n```\n\n### 4. News (`_data/news.json`)\n\n```json\n[\n  {\n    \"date\": \"2024-12-01\",\n    \"title\": \"News Title\",\n    \"description\": \"News description text.\"\n  }\n]\n```\n\n### 5. Funding (`_data/funding.json`)\n\n```json\n[\n  {\n    \"title\": \"Grant Title\",\n    \"funder\": \"Funding Agency\",\n    \"amount\": \"100,000 CHF\",\n    \"grant_number\": \"12345\",\n    \"period\": \"2024-01 - 2027-12\",\n    \"institution\": \"Host Institution\",\n    \"team\": \"PI Name (PI); Researcher Names\"\n  }\n]\n```\n\n## Customization\n\n### Colors\n\nEdit CSS variables in `assets/css/style.scss`:\n\n```scss\n:root {\n  --primary-color: #1a365d;      // Main brand color\n  --primary-dark: #0f2942;        // Darker shade\n  --accent-color: #c9a227;        // Highlight color\n  --accent-light: #f4e4bc;        // Light accent\n}\n```\n\n### Publication Filters\n\nEdit topic keywords in `assets/js/main.js`:\n\n```javascript\nconst topicKeywords = {\n  'topic1': ['keyword1', 'keyword2'],\n  'topic2': ['keyword3', 'keyword4'],\n  // Add your research topics\n};\n```\n\n### Network Graph\n\nEdit nodes and links in `assets/js/visualizations.js`:\n\n```javascript\nconst nodes = [\n  { id: 'PI', name: 'Principal Investigator', group: 1 },\n  { id: 'R1', name: 'Researcher 1', group: 1 },\n  // Add team members\n];\n\nconst links = [\n  { source: 'PI', target: 'R1', value: 5 },\n  // Add collaboration connections\n];\n```\n\n### Contact Form\n\nThe template uses [Formspree](https://formspree.io/) for contact forms:\n\n1. Create a free Formspree account\n2. Get your form endpoint\n3. Update the form action in `index.md`:\n\n```html\n<form action=\"https://formspree.io/f/YOUR_ID\" method=\"POST\">\n```\n\n## Deployment\n\n### GitHub Pages\n\n1. Push your repository to GitHub\n2. Go to Settings > Pages\n3. Select \"Deploy from a branch\"\n4. Choose `main` branch, `/ (root)` folder\n5. Save and wait for deployment\n\n### Local Development\n\n```bash\n# Install Jekyll\ngem install bundler jekyll\n\n# Install dependencies\nbundle install\n\n# Serve locally\nbundle exec jekyll serve\n\n# Open http://localhost:4000/your-repo-name/\n```\n\n## Python Scripts\n\n### fetch_publications.py\n\nFetches publications from OpenAlex.org:\n\n```bash\n# Install dependencies\npip install requests\n\n# Configure team members in the script\n# Run\npython scripts/fetch_publications.py\n```\n\n### verify_site.py\n\nComprehensive site verification:\n\n```bash\n# Install dependencies\npip install requests playwright\nplaywright install chromium\n\n# Configure SITE_URL in the script\n# Run\npython scripts/verify_site.py\n```\n\n## Placeholders Reference\n\nKey placeholders to replace in `index.md`:\n\n| Placeholder | Description |\n|-------------|-------------|\n| `{{PROJECT_TITLE}}` | Full project title |\n| `{{PROJECT_DESCRIPTION}}` | SEO description |\n| `{{GITHUB_ORG}}` | GitHub organization name |\n| `{{REPO_NAME}}` | Repository name |\n| `{{PI_NAME}}` | Principal investigator name |\n| `{{PI_INSTITUTION}}` | PI's institution |\n| `{{PROJECT_FUNDER}}` | Main funding agency |\n| `{{GRANT_NUMBER}}` | Grant/project number |\n| `{{GRANT_AMOUNT}}` | Total funding amount |\n| `{{START_DATE}}` | Project start date |\n| `{{END_DATE}}` | Project end date |\n\n## License\n\nMIT License - Feel free to use for any research project.\n\n## Credits\n\nBased on the SNSF-funded \"Network-Based Credit Risk Models in P2P Lending Markets\" project by Prof. Dr. Joerg Osterrieder at Bern University of Applied Sciences.\n",
    "latest_release": null,
    "research_metadata": {
      "repo_name": "research-page-generator",
      "research": {
        "title": "Toolkit for generating professional GitHub Pages sites for research projects",
        "abstract": "A complete toolkit for generating professional GitHub Pages sites for research projects. Based on the [Network-Based Credit Risk Models](https://digital-ai-finance.github.io/network-based-credit-risk-models/) project.",
        "keywords": [],
        "authors": [
          {
            "name": "Prof. Dr. Joerg Osterrieder at Bern University of Applied Sciences."
          }
        ]
      },
      "publications": [],
      "code": {
        "languages": [
          "SCSS"
        ],
        "notebooks": [],
        "dependencies": {}
      },
      "reproducibility": {
        "has_requirements": false,
        "has_dockerfile": false,
        "has_environment_yml": false,
        "has_makefile": false,
        "replication_status": "not_attempted"
      },
      "citations": {
        "cited_by": [],
        "cites": [],
        "citation_count": 0
      },
      "meta": {
        "extracted_at": "2025-12-06T03:06:36.980894",
        "extraction_version": "1.0",
        "extraction_method": "readme_parse"
      },
      "datasets": [
        {
          "name": "funding.json",
          "path": "sample/_data/funding.json",
          "format": ".json",
          "size_bytes": 1249
        },
        {
          "name": "news.json",
          "path": "sample/_data/news.json",
          "format": ".json",
          "size_bytes": 1687
        },
        {
          "name": "publications.json",
          "path": "sample/_data/publications.json",
          "format": ".json",
          "size_bytes": 5252
        },
        {
          "name": "team.json",
          "path": "sample/_data/team.json",
          "format": ".json",
          "size_bytes": 2146
        },
        {
          "name": "funding.json",
          "path": "template/_data/funding.json",
          "format": ".json",
          "size_bytes": 532
        },
        {
          "name": "news.json",
          "path": "template/_data/news.json",
          "format": ".json",
          "size_bytes": 514
        },
        {
          "name": "publications.json",
          "path": "template/_data/publications.json",
          "format": ".json",
          "size_bytes": 625
        },
        {
          "name": "team.json",
          "path": "template/_data/team.json",
          "format": ".json",
          "size_bytes": 1003
        }
      ]
    }
  },
  {
    "name": "crypto-market-ai-sample",
    "full_name": "Digital-AI-Finance/crypto-market-ai-sample",
    "description": "Sample research project page: AI-Powered Cryptocurrency Market Analysis",
    "url": "https://github.com/Digital-AI-Finance/crypto-market-ai-sample",
    "clone_url": "https://github.com/Digital-AI-Finance/crypto-market-ai-sample.git",
    "homepage": "",
    "language": "SCSS",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 24,
    "default_branch": "main",
    "created_at": "2025-12-04T12:01:46+00:00",
    "updated_at": "2025-12-05T12:35:35+00:00",
    "pushed_at": "2025-12-05T12:35:31+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": true,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "No README available",
    "latest_release": null
  },
  {
    "name": "climate-ai-sample",
    "full_name": "Digital-AI-Finance/climate-ai-sample",
    "description": "Sample research project page: AI for Climate Prediction and Extreme Weather Forecasting",
    "url": "https://github.com/Digital-AI-Finance/climate-ai-sample",
    "clone_url": "https://github.com/Digital-AI-Finance/climate-ai-sample.git",
    "homepage": "",
    "language": "SCSS",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 24,
    "default_branch": "main",
    "created_at": "2025-12-04T12:03:59+00:00",
    "updated_at": "2025-12-05T12:35:56+00:00",
    "pushed_at": "2025-12-05T12:35:52+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": true,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "No README available",
    "latest_release": null
  },
  {
    "name": "medical-ai-sample",
    "full_name": "Digital-AI-Finance/medical-ai-sample",
    "description": "Sample research project page: AI for Medical Imaging and Early Disease Detection",
    "url": "https://github.com/Digital-AI-Finance/medical-ai-sample",
    "clone_url": "https://github.com/Digital-AI-Finance/medical-ai-sample.git",
    "homepage": "",
    "language": "SCSS",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "watchers": 0,
    "open_issues": 0,
    "size": 24,
    "default_branch": "main",
    "created_at": "2025-12-04T12:05:41+00:00",
    "updated_at": "2025-12-05T12:36:07+00:00",
    "pushed_at": "2025-12-05T12:36:03+00:00",
    "license": "No License",
    "has_issues": true,
    "has_wiki": true,
    "has_pages": true,
    "has_downloads": true,
    "archived": false,
    "disabled": false,
    "is_template": false,
    "visibility": "public",
    "contributors_count": 1,
    "readme": "No README available",
    "latest_release": null
  }
]