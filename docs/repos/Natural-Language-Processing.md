# Natural-Language-Processing

NLP Course 2025: From N-grams to Transformers - Complete 12-week curriculum with discovery-based pedagogy

[View on GitHub](https://github.com/Digital-AI-Finance/Natural-Language-Processing){ .md-button .md-button--primary }


---





## Information

| Property | Value |
|----------|-------|
| Language | Jupyter Notebook |
| Stars | 0 |
| Forks | 0 |
| Watchers | 0 |
| Open Issues | 0 |
| License | No License |
| Created | 2025-11-22 |
| Last Updated | 2025-12-04 |
| Last Push | 2025-12-04 |
| Contributors | 1 |
| Default Branch | main |
| Visibility | public |




## Notebooks

This repository contains 35 notebook(s):

| Notebook | Language | Type |
|----------|----------|------|

| [llm_summarization_lab](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/summarization_module/lab/llm_summarization_lab.ipynb) | PYTHON | jupyter |

| [week01_ngrams_lab](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week01_foundations/lab/week01_ngrams_lab.ipynb) | PYTHON | jupyter |

| [week02_word_embeddings_lab](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week02_neural_lm/lab/week02_word_embeddings_lab.ipynb) | PYTHON | jupyter |

| [week03_rnn_lab](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week03_rnn/lab/week03_rnn_lab.ipynb) | PYTHON | jupyter |

| [week03_rnn_lab_enhanced](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week03_rnn/lab/week03_rnn_lab_enhanced.ipynb) | PYTHON | jupyter |

| [week04_part1_basic_seq2seq](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week04_seq2seq/lab/week04_part1_basic_seq2seq.ipynb) | PYTHON | jupyter |

| [week04_part2_attention](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week04_seq2seq/lab/week04_part2_attention.ipynb) | PYTHON | jupyter |

| [week04_part3_advanced](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week04_seq2seq/lab/week04_part3_advanced.ipynb) | PYTHON | jupyter |

| [week04_seq2seq_lab](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week04_seq2seq/lab/week04_seq2seq_lab.ipynb) | PYTHON | jupyter |

| [week04_seq2seq_lab_enhanced](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week04_seq2seq/lab/week04_seq2seq_lab_enhanced.ipynb) | PYTHON | jupyter |

| [week05_transformer_lab](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week05_transformers/lab/week05_transformer_lab.ipynb) | PYTHON | jupyter |

| [week06_bert_finetuning](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week06_pretrained/lab/week06_bert_finetuning.ipynb) | PYTHON | jupyter |

| [week06_pretrained_feature_extraction](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week06_pretrained/lab/week06_pretrained_feature_extraction.ipynb) | PYTHON | jupyter |

| [week07_advanced_transformers_lab](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week07_advanced/lab/week07_advanced_transformers_lab.ipynb) | PYTHON | jupyter |

| [week08_tokenization_lab](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week08_tokenization/lab/week08_tokenization_lab.ipynb) | PYTHON | jupyter |

| [week09_decoding_lab](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week09_decoding/lab/week09_decoding_lab.ipynb) | PYTHON | jupyter |

| [week09_decoding_simplified](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week09_decoding/lab/week09_decoding_simplified.ipynb) | PYTHON | jupyter |

| [week10_finetuning_lab](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week10_finetuning/lab/week10_finetuning_lab.ipynb) | PYTHON | jupyter |

| [week11_efficiency_lab](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week11_efficiency/lab/week11_efficiency_lab.ipynb) | PYTHON | jupyter |

| [week12_ethics_lab](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week12_ethics/lab/week12_ethics_lab.ipynb) | PYTHON | jupyter |

| [demo_agent_multistep](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/demos/demo_agent_multistep.ipynb) | PYTHON | jupyter |

| [demo_rag_simple](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/demos/demo_rag_simple.ipynb) | PYTHON | jupyter |

| [demo_reasoning_compare](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/demos/demo_reasoning_compare.ipynb) | PYTHON | jupyter |

| [discovery_notebook](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/embeddings/handouts/discovery_notebook.ipynb) | PYTHON | jupyter |

| [word_embeddings_3d_msc](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/embeddings/word_embeddings_3d_msc.ipynb) | PYTHON | jupyter |

| [ngrams_Alice_in_Wonderland](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/exercises/ngrams_Alice_in_Wonderland.ipynb) | PYTHON | jupyter |

| [shakespeare_sonnets_simple_bsc](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/exercises/shakespeare/shakespeare_sonnets_simple_bsc.ipynb) | PYTHON | jupyter |

| [1_simple_ngrams](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/notebooks/visualizations/1_simple_ngrams.ipynb) | PYTHON | jupyter |

| [2_word_embeddings](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/notebooks/visualizations/2_word_embeddings.ipynb) | PYTHON | jupyter |

| [3_simple_neural_net](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/notebooks/visualizations/3_simple_neural_net.ipynb) | PYTHON | jupyter |

| [4_compare_NLP_methods](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/notebooks/visualizations/4_compare_NLP_methods.ipynb) | PYTHON | jupyter |

| [5_Tokens Journey Through a Transformer](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/notebooks/visualizations/5_Tokens Journey Through a Transformer.ipynb) | PYTHON | jupyter |

| [6_Transformers in 3D A Visual Journey](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/notebooks/visualizations/6_Transformers in 3D A Visual Journey.ipynb) | PYTHON | jupyter |

| [7_Transformers_in_3d_simplified](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/notebooks/visualizations/7_Transformers_in_3d_simplified.ipynb) | PYTHON | jupyter |

| [8_How_Transformers_Learn_Training_in_3D](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/notebooks/visualizations/8_How_Transformers_Learn_Training_in_3D.ipynb) | PYTHON | jupyter |




## Datasets

This repository includes 6 dataset(s):

| Dataset | Format | Size |
|---------|--------|------|

| [package-lock.json](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week09_decoding/learning-app/package-lock.json) | .json | 149.59 KB |

| [package.json](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week09_decoding/learning-app/package.json) | .json | 0.9 KB |

| [package-lock.json](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week09_decoding/react-app/package-lock.json) | .json | 162.65 KB |

| [package.json](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/NLP_slides/week09_decoding/react-app/package.json) | .json | 0.95 KB |

| [search.json](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/docs/search.json) | .json | 6.16 KB |

| [notebook_test_results.json](https://github.com/Digital-AI-Finance/Natural-Language-Processing/blob/main/notebook_test_results.json) | .json | 9.48 KB |




## Reproducibility


This repository includes reproducibility tools:


- Python requirements.txt



- Conda environment.yml


- Makefile for automation










## Status





- Issues: Enabled
- Wiki: Disabled
- Pages: Enabled

## README

# NLP Course 2025: From N-grams to Transformers

<p align="center">
  <a href="https://quantlet.de">
    <img src="logo/quantlet.png" alt="QuantLet Logo" width="120"/>
  </a>
</p>

<p align="center">
  <strong>QuantLet-Compatible Course Materials</strong>
</p>

![Course Status](https://img.shields.io/badge/weeks-12%2F12%20complete-brightgreen)
![Framework](https://img.shields.io/badge/framework-100%25%20applied-success)
![Labs](https://img.shields.io/badge/labs-12%20notebooks-blue)
![Charts](https://img.shields.io/badge/charts-168%20visualizations-purple)
![License](https://img.shields.io/badge/license-MIT-orange)

> A comprehensive Natural Language Processing course covering statistical foundations through modern transformer architectures. Build ChatGPT from scratch!

## Quick Start (3 Steps)

```bash
# 1. Clone the repository
git clone https://github.com/josterri/2025_NLP_Lectures.git
cd 2025_NLP_Lectures

# 2. Install dependencies
pip install -r requirements.txt

# 3. Start learning!
jupyter lab NLP_slides/week02_neural_lm/lab/week02_word_embeddings_lab.ipynb
```

## What You'll Learn

This course takes you from foundational statistical methods to state-of-the-art neural architectures:

- **Weeks 1-2:** Statistical language models and word embeddings (Word2Vec, GloVe)
- **Weeks 3-4:** Sequential models (RNN/LSTM) and sequence-to-sequence with attention
- **Weeks 5-7:** Transformers, BERT, GPT, and advanced architectures
- **Weeks 8-10:** Tokenization, decoding strategies, and fine-tuning
- **Weeks 11-12:** Efficiency optimization and ethical AI deployment

By the end, you'll build a working transformer from scratch and understand the architecture behind ChatGPT and Claude.

## Course Structure

### Core Materials (12 Weeks)
Each week includes:
- **Presentation:** LaTeX/Beamer slides with optimal readability
- **Lab Notebook:** Interactive Jupyter notebook with hands-on exercises
- **Handouts:** Pre-class discovery exercises and post-class technical practice

### Supplementary Modules
- **Neural Network Primer:** Zero pre-knowledge intro to neural networks
- **LSTM Primer:** Comprehensive deep dive into LSTM architecture (32 slides)
- **Embeddings Module:** Standalone word embedding module with 3D visualizations

### Total Content
- 60+ presentations (including versions and supplements)
- 12 interactive lab notebooks
- 40+ handout documents
- 100+ Python-generated figures
- 8 progressive visualization notebooks

## Prerequisites

- **Required:**
  - Python 3.8 or higher
  - Basic linear algebra (vectors, matrices)
  - Basic probability theory
  - Comfortable with Python programming

- **Helpful but not required:**
  - PyTorch experience
  - Understanding of backpropagation
  - Machine learning fundamentals

**New to neural networks?** Start with our Neural Network Primer module before Week 2.

## Installation

### Option 1: pip (Recommended)
```bash
pip install -r requirements.txt
```

### Option 2: conda
```bash
conda env create -f environment.yml
conda activate nlp2025
```

### GPU Support
For GPU acceleration (recommended for Weeks 5+):
```bash
# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

See [INSTALLATION.md](INSTALLATION.md) for detailed setup instructions and troubleshooting.

## Course Navigation

### Week-by-Week Guide
Full navigation with topics, prerequisites, and learning objectives: [COURSE_INDEX.md](COURSE_INDEX.md)

### Week Highlights

| Week | Topic | Key Concepts | Lab |
|------|-------|--------------|-----|
| 1 | Foundations | N-grams, perplexity, statistical LM | - |
| 2 | Word Embeddings | Word2Vec, GloVe, neural LM | Implement embeddings |
| 3 | RNN/LSTM | Sequential models, BPTT | Build LSTM from scratch |
| 4 | Seq2Seq | Attention mechanism, translation | Machine translation |
| 5 | Transformers | Self-attention, multi-head | Build transformer |
| 6 | Pre-trained | BERT, GPT, transfer learning | Fine-tune BERT |
| 7 | Advanced | T5, GPT-3, scaling laws | Experiment with GPT |
| 8 | Tokenization | BPE, WordPiece, SentencePiece | Implement tokenizer |
| 9 | Decoding | Beam, sampling, nucleus, **contrastive** | Compare 6 methods |
| 10 | Fine-tuning | LoRA, prompt engineering | Adapt models |
| 11 | Efficiency | Quantization, distillation | Optimize models |
| 12 | Ethics | Bias, fairness, safety | Measure bias |

## Quantlet Charts

All Python-generated visualizations follow the [Quantlet](https://quantlet.de) standard format with:
- Numbered folders (`01_chart_name/`, `02_chart_name/`, etc.)
- Self-contained Python scripts
- Standard `metainfo.txt` with description, keywords, and usage

### Final Lecture Charts
See [FinalLecture/](FinalLecture/) for 8 Quantlet-formatted visualizations covering:
- Vector database architecture
- HNSW nearest neighbor search
- RAG conditional probabilities
- Hybrid search flow

## Project Structure

```
├── FinalLecture/               # Quantlet-formatted charts (Final Lecture)
├── logo/                       # Quantlet branding
├── NLP_slides/
│   ├── week01_foundations/      # Week 1: Statistical LM
│   ├── week02_neural_lm/        # Week 2: Word embeddings
│   ├── week03_rnn/              # Week 3: RNN/LSTM/GRU
│   ├── ...                      # Weeks 4-12
│   ├── nn_primer/               # Neural network primer
│   ├── lstm_primer/             # LSTM deep dive
│   └── common/                  # Shared templates and utils
├── embeddings/                  # Standalone embeddings module
├── exercises/                   # Additional practice
├── figures/                     # Shared visualizations
├── requirements.txt             # Python dependencies
├── environment.yml              # Conda environment
└── COURSE_INDEX.md              # Full course navigation
```

## Key Learning Milestones

- ✅ **After Week 2:** Understand and implement word embeddings
- ✅ **After Week 3:** Build RNN and LSTM from scratch
- ✅ **After Week 5:** Comprehend transformer architecture completely
- ✅ **After Week 6:** Fine-tune pre-trained models (BERT, GPT)
- ✅ **After Week 9:** Control text generation quality and diversity
- ✅ **After Week 12:** Deploy models responsibly with ethical considerations

## Usage Examples

### Run a Lab Notebook
```bash
# Start Jupyter Lab
jupyter lab

# Navigate to a week's lab folder
cd NLP_slides/week05_transformers/lab
jupyter notebook week05_transformer_lab.ipynb
```

### Compile a Presentation
```bash
cd NLP_slides/week02_neural_lm/presentations
pdflatex week02_neural_lm.tex
```

### Generate Figures
```bash
cd NLP_slides/week05_transformers/python
python generate_week05_optimal_charts.py
```

## Testing the Course

Test all lab notebooks for execution:
```bash
python test_notebooks.py
```

This validates that all 12 lab notebooks execute correctly in your environment.

## Course Delivery Options

### Standard 12-Week Semester
- One week per topic
- Weekly labs and assignments
- Suitable for undergraduate/graduate courses

### Intensive 8-Week Course
- Combine Weeks 1-2, skip some advanced topics
- Accelerated pace for bootcamps
- Focus on core transformer concepts

### Self-Paced Learning
- Progress at your own speed
- Complete prerequisite modules first
- Focus on labs and hands-on practice

## Documentation

- **[COURSE_INDEX.md](COURSE_INDEX.md)** - Complete week-by-week navigation
- **[INSTALLATION.md](INSTALLATION.md)** - Detailed setup instructions
- **[CLAUDE.md](CLAUDE.md)** - Development guide and conventions
- **[status.md](status.md)** - Project status and completion tracking
- **[changelog.md](changelog.md)** - Change history

## Support and Resources

- **Issues:** Report problems at [GitHub Issues](https://github.com/josterri/2025_NLP_Lectures/issues)
- **Prerequisites:** Check the Neural Network Primer if you're new to deep learning
- **GPU Requirements:** Most labs work on CPU; Weeks 5+ benefit from GPU

## Contributing

Contributions are welcome! Areas for contribution:
- Additional exercises and examples
- Translations to other languages
- MSc-level challenge problems
- Bug fixes and improvements

## License

This course is released under the MIT License. See LICENSE for details.

## Acknowledgments

Course materials developed with pedagogical focus on:
- Discovery-based learning
- Concrete-to-abstract progression
- Hands-on implementation
- Real-world applications

Built with LaTeX/Beamer, Python, PyTorch, and Jupyter.

## Citation

If you use these materials in your course or research, please cite:

```bibtex
@misc{nlp2025course,
  title={NLP Course 2025: From N-grams to Transformers},
  author={Joerg Osterrieder},
  year={2025},
  url={https://github.com/josterri/2025_NLP_Lectures}
}
```

---

**Ready to start?** Check [INSTALLATION.md](INSTALLATION.md) for setup, then dive into Week 2's word embeddings lab!

**Questions?** See [COURSE_INDEX.md](COURSE_INDEX.md) for complete navigation and prerequisites.